{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Lecture Notes \u00b6 Lecture notes for some subjects of the Master's in Advanced Mathematics and Mathematical Engineering. Author \u00b6 Maria Guasch-Morgades","title":"Lecture Notes"},{"location":"#lecture-notes","text":"Lecture notes for some subjects of the Master's in Advanced Mathematics and Mathematical Engineering.","title":"Lecture Notes"},{"location":"#author","text":"Maria Guasch-Morgades","title":"Author"},{"location":"Mathematical%20models%20in%20biology/%28book%29%20Mathematicals%20Models%20in%20Biology%20-%20An%20introduction/","text":"Mathematicals Models in Biology - An introduction \u00b6 Preface \u00b6 Chapters summary 1. Dynamic Modeling with Difference Equations: Concepts of dynamic modeling through one-variable difference equations. Notions of equilibria, linearization and stability 2. Linear Models of Structured Populations : Matrix algebra and eigenvector analysis (two-variable linear models) 3. Nonlinear models of interactions : Models of interacting populations 4. Modeling molecular evolution : Introduction to probability to model molecular evolution 5. Constructing Phylogenetic Trees : Algorithmic flavor (uses distance formulas from 4) 6. Genetics : Genetics applications (extended probability) 7. Infectious disease modeling : Tratement of infectious disease models 8. Curve fitting and biological modeling Full index 1. Dynamic Modeling with Difference Equations 1. The Malthusian Model 2. Nonlinear Models 3. Analyzing Nonlinear Models 4. Variations on the Logistic Model 5. Comments on Discrete and Continuous Models 2. Linear Models of Structured Populations 1. Linear Models and Matrix Algebra 2. Projection Matrices for Structured Models 3. Eigenvectors and Eigenvalues 4. Computing Eigenvectors and Eigenvalues 3. Nonlinear Models of Interactions 1. A Simple Predator\u2013Prey Model 2. Equilibria of Multipopulation Models 3. Linearization and Stability 4. Positive and Negative Interactions 4. Modeling Molecular Evolution 1. Background on DNA 2. An Introduction to Probability 3. Conditional Probabilities 4. Matrix Models of Base Substitution 5. Phylogenetic Distances 5. Constructing Phylogenetic Trees 1. Phylogenetic Trees 2. Tree Construction: Distance Methods \u2013 Basics 3. Tree Construction: Distance Methods \u2013 Neighbor Joining 4. Tree Construction: Maximum Parsimony 5. Other Methods 6. Applications and Further Reading 6. Genetics 1. Mendelian Genetics 2. Probability Distributions in Genetics 3. Linkage 4. Gene Frequency in Populations 7. Infectious Disease Modeling 1. Elementary Epidemic Models 2. Threshold Values and Critical Parameters 3. Variations on a Theme 4. Multiple Populations and Differentiated Infectivity 8. Curve Fitting and Biological Modeling 1. Fitting Curves to Data 2. The Method of Least Squares 3. Polynomial Curve Fitting A. Basic Analysis of Numerical Data 1. The Meaning of a Measurement 2. Understanding Variable Data \u2013 Histograms and Distributions 3. Mean, Median, and Mode 4. The Spread of Data 5. Populations and Samples 6. Practice 4. Modeling Molecular Evolution \u00b6 Natural selection - Fundamental mechanism through which evolutions occurs. It acts to reduce variability. - Needs some underlying genetic variability. Evolution Many of the molecular changes are belived to be selectively neutral, and so are passed on to further descendents and preserved. The DNA within a gene may continue to mutate from generatijon to generatijon and acumulate differences. Similarities between species will hint at common ancestors, while differences point to the evolutionary divergence. Questions - Can we reconstruct evolutionary relationships between several modern species by comparing the DNA sequences of their verisons of a certain gene? - What do we might mean by \"degree of similarity\"? Goal - Study molecular evolution - Define phylogenetic distance : measure of sequence similarity 1. Background on DNA \u00b6 DNA molecule forms a double helix, a twisted ladder-like structure. Each point corresponds to a molecular subunit, called nucleotides or bases : - A denine: purine . - G uanine: purine . - C yosine: pyrimidne . - T hymine: pyrimidne Molecule Complement A T G C DNA has a directional sense. Sections of DNA form genes, with the encoded instruction to manufacture propeins (through RNA). Triplets of consecutives bases form codons and each specifies a particular amino acid for the protein chain. Certain codons signal the end of the protein sequence. There's some redundancy in the genetic code: \\(4^{3}= 64\\) different codons, only 20 aminoacids and one stop command. N many codons, the third base has no affect on the paricular aminoacid the codon specifies. Not all genes encode proteins via mRNA, some genes encode for the production of other types of RNA, which are the \"final products\". About 97% of human DNA is believed to be noncoding ( junkDNA ). Mutations \u00b6 Most common: - Base substitution : replacement of one base for another at a certain site n the sequence. - Transition : Interchanges a purine with a purine or a pyrimidine with a pyrimidine. This are more frequent. - Transversion : Interchange of classes - Delation of a base or consecutive one - Insertion of a base Let's focus only on base substitution. Example \u00b6 (Assuming sequences have been aligned) \\( \\(\\begin{aligned} S_{0} &= AC\\mathbf{C}TGC\\mathbf{C}CTA...\\\\ S_{1} &= AC\\mathbf{G}TGC\\mathbf{A}CTA...\\\\ S_{2} &= AC\\mathbf{G}TGC\\mathbf{G}CTA... \\end{aligned}\\) \\) From \\(S_0\\) to \\(S_2\\) , we notice only one base substitution, but we know \\(S_1\\) , which shows a change from \\(G \\rightarrow A \\rightarrow G\\) , which has been hidden due to a back mutation . We can also have consecutive substitutions, which are counted only once when looking at \\(S_2\\) directly. A simple ratio of mutations per site obtained rom comparing the first and last equences may give too low an estimate of the amount of mutations that acutally occured. 2. An introduction to probability \u00b6","title":"Mathematicals Models in Biology - An introduction"},{"location":"Mathematical%20models%20in%20biology/%28book%29%20Mathematicals%20Models%20in%20Biology%20-%20An%20introduction/#mathematicals-models-in-biology-an-introduction","text":"","title":"Mathematicals Models in Biology - An introduction"},{"location":"Mathematical%20models%20in%20biology/%28book%29%20Mathematicals%20Models%20in%20Biology%20-%20An%20introduction/#preface","text":"Chapters summary 1. Dynamic Modeling with Difference Equations: Concepts of dynamic modeling through one-variable difference equations. Notions of equilibria, linearization and stability 2. Linear Models of Structured Populations : Matrix algebra and eigenvector analysis (two-variable linear models) 3. Nonlinear models of interactions : Models of interacting populations 4. Modeling molecular evolution : Introduction to probability to model molecular evolution 5. Constructing Phylogenetic Trees : Algorithmic flavor (uses distance formulas from 4) 6. Genetics : Genetics applications (extended probability) 7. Infectious disease modeling : Tratement of infectious disease models 8. Curve fitting and biological modeling Full index 1. Dynamic Modeling with Difference Equations 1. The Malthusian Model 2. Nonlinear Models 3. Analyzing Nonlinear Models 4. Variations on the Logistic Model 5. Comments on Discrete and Continuous Models 2. Linear Models of Structured Populations 1. Linear Models and Matrix Algebra 2. Projection Matrices for Structured Models 3. Eigenvectors and Eigenvalues 4. Computing Eigenvectors and Eigenvalues 3. Nonlinear Models of Interactions 1. A Simple Predator\u2013Prey Model 2. Equilibria of Multipopulation Models 3. Linearization and Stability 4. Positive and Negative Interactions 4. Modeling Molecular Evolution 1. Background on DNA 2. An Introduction to Probability 3. Conditional Probabilities 4. Matrix Models of Base Substitution 5. Phylogenetic Distances 5. Constructing Phylogenetic Trees 1. Phylogenetic Trees 2. Tree Construction: Distance Methods \u2013 Basics 3. Tree Construction: Distance Methods \u2013 Neighbor Joining 4. Tree Construction: Maximum Parsimony 5. Other Methods 6. Applications and Further Reading 6. Genetics 1. Mendelian Genetics 2. Probability Distributions in Genetics 3. Linkage 4. Gene Frequency in Populations 7. Infectious Disease Modeling 1. Elementary Epidemic Models 2. Threshold Values and Critical Parameters 3. Variations on a Theme 4. Multiple Populations and Differentiated Infectivity 8. Curve Fitting and Biological Modeling 1. Fitting Curves to Data 2. The Method of Least Squares 3. Polynomial Curve Fitting A. Basic Analysis of Numerical Data 1. The Meaning of a Measurement 2. Understanding Variable Data \u2013 Histograms and Distributions 3. Mean, Median, and Mode 4. The Spread of Data 5. Populations and Samples 6. Practice","title":"Preface"},{"location":"Mathematical%20models%20in%20biology/%28book%29%20Mathematicals%20Models%20in%20Biology%20-%20An%20introduction/#4-modeling-molecular-evolution","text":"Natural selection - Fundamental mechanism through which evolutions occurs. It acts to reduce variability. - Needs some underlying genetic variability. Evolution Many of the molecular changes are belived to be selectively neutral, and so are passed on to further descendents and preserved. The DNA within a gene may continue to mutate from generatijon to generatijon and acumulate differences. Similarities between species will hint at common ancestors, while differences point to the evolutionary divergence. Questions - Can we reconstruct evolutionary relationships between several modern species by comparing the DNA sequences of their verisons of a certain gene? - What do we might mean by \"degree of similarity\"? Goal - Study molecular evolution - Define phylogenetic distance : measure of sequence similarity","title":"4. Modeling Molecular Evolution"},{"location":"Mathematical%20models%20in%20biology/%28book%29%20Mathematicals%20Models%20in%20Biology%20-%20An%20introduction/#1-background-on-dna","text":"DNA molecule forms a double helix, a twisted ladder-like structure. Each point corresponds to a molecular subunit, called nucleotides or bases : - A denine: purine . - G uanine: purine . - C yosine: pyrimidne . - T hymine: pyrimidne Molecule Complement A T G C DNA has a directional sense. Sections of DNA form genes, with the encoded instruction to manufacture propeins (through RNA). Triplets of consecutives bases form codons and each specifies a particular amino acid for the protein chain. Certain codons signal the end of the protein sequence. There's some redundancy in the genetic code: \\(4^{3}= 64\\) different codons, only 20 aminoacids and one stop command. N many codons, the third base has no affect on the paricular aminoacid the codon specifies. Not all genes encode proteins via mRNA, some genes encode for the production of other types of RNA, which are the \"final products\". About 97% of human DNA is believed to be noncoding ( junkDNA ).","title":"1. Background on DNA"},{"location":"Mathematical%20models%20in%20biology/%28book%29%20Mathematicals%20Models%20in%20Biology%20-%20An%20introduction/#mutations","text":"Most common: - Base substitution : replacement of one base for another at a certain site n the sequence. - Transition : Interchanges a purine with a purine or a pyrimidine with a pyrimidine. This are more frequent. - Transversion : Interchange of classes - Delation of a base or consecutive one - Insertion of a base Let's focus only on base substitution.","title":"Mutations"},{"location":"Mathematical%20models%20in%20biology/%28book%29%20Mathematicals%20Models%20in%20Biology%20-%20An%20introduction/#example","text":"(Assuming sequences have been aligned) \\( \\(\\begin{aligned} S_{0} &= AC\\mathbf{C}TGC\\mathbf{C}CTA...\\\\ S_{1} &= AC\\mathbf{G}TGC\\mathbf{A}CTA...\\\\ S_{2} &= AC\\mathbf{G}TGC\\mathbf{G}CTA... \\end{aligned}\\) \\) From \\(S_0\\) to \\(S_2\\) , we notice only one base substitution, but we know \\(S_1\\) , which shows a change from \\(G \\rightarrow A \\rightarrow G\\) , which has been hidden due to a back mutation . We can also have consecutive substitutions, which are counted only once when looking at \\(S_2\\) directly. A simple ratio of mutations per site obtained rom comparing the first and last equences may give too low an estimate of the amount of mutations that acutally occured.","title":"Example"},{"location":"Mathematical%20models%20in%20biology/%28book%29%20Mathematicals%20Models%20in%20Biology%20-%20An%20introduction/#2-an-introduction-to-probability","text":"","title":"2. An introduction to probability"},{"location":"Mathematical%20models%20in%20biology/Evolutionary%20models%20lessons/","text":"Evolutionary models \u00b6 Markov matrices \u00b6 Square matrix with real entries \\(\\geq 0\\) whose rows sum to 1 is called a Markov matrix (or stochastic matrix). Equivalent to seighg that the vector \\((1, \\dots 1)^T\\) is an eigenvector of eigenvalue 1. In general our Markov matrices will be \\(4 \\times 4\\) (4 nucletoids). Theorem (Perron-Frobenius) \u00b6 Claim \u00b6 ... 1. 2. 3. [[convex sum]] of Markov matrices Is \\(\\Delta_n\\) a Algebraic group ? In other words, is the inverse of matrix A also a Markov matrix? The answer is no . Per #Theorem Perron-Frobenius , all eigenvalues are upper bounded by 1. Consider $ \\( \\(\\begin{pmatrix} \\frac{1}{n} & \\dots / \\frac{1}{n} \\\\\\\\ \\frac{1}{n} & \\dots / \\frac{1}{n} \\end{pmatrix}\\) \\) The determinant is 0 and it's not invertible (Proof in book in the slides). Conditional probabilities \u00b6 A is independent of B: \\(A \\perp \\!\\!\\! \\perp B\\) . ... Let's say A is independent of B given C: \\(A \\perp \\!\\!\\! \\perp B \\:|\\: C\\) . Two dependent events that become independent once we assume a third event. Discrete-time probabilistic processes \u00b6 In all this teory, time is considered discrete. Discrete-time Markov processes \u00b6 A discrete-time probabilistic process is called a Markov process if future is independent of past given the present. I don't care about what happened yesterday: like in the getting sick or the weather example. It's not the case for the \"self-avoiding random walk\". We can attach a matrix \\(M(n)\\) to run the transition from one state to the other. It's a Markov matrix. We call them Markov matrices . Evolutionary models \u00b6 Models : Simplifications of reality. Models are appropiate or not depending on what we want to study. More complex models does not mean better models. Substitution models : What happens at each step. We have a sequence of genes that is transmited and we want to understand the rule of how the substitution may act. Nucleotide substitution models \u00b6 Assumption: Biological hereditary information encoded in the DNA Code of 4 nucleotides: A, G, C, T Double helix structure: pairing A-T, G-C The order or sequence determines the information available for building proteins (every 3 is an aminoacid) It can make copies of itself. Let's assume a sequence of nucleotides \\(S=TCAACTTGATC\\) changing through an evolutionary process \\( \\(S_{0} = S \\rightarrow S_{1}\\rightarrow S_{2}\\dots\\) \\) Possible changes: - Substitutions (error in duplication):We will focus in this one. - Insertions - Deletions We won't include insertions or deletions. Not realistic but it's the typical way of modeling. Homogeneous Markov models \u00b6 Each site follows a discrete-time probabilistic process. We only care if each position is an A, C, G, T Hypothesis - iid hypothesis : Each site of the sequence evolves independently of the other sites and with the same probabilities. It's the same probability of changing in the end of the sequence that in the beginning (homogeneity across sites). Not realistic : Also because of the genetic code, substitutions in the 3rd nucletoid of the codon does not always matter. - The process does not depend on the past given the present (Markov hypothesis) - From generation to generation the same system of probabilities are ruling. This is not realistic if you consider a big amount of time. It's called homogeneity across lineages. HOmogeneous Markov model is determined by 1. Transition matrix 2. Ancestral distribution \\(S_0\\) . How are we estimating them? - Estimation : With the relative frequency of changes - Ancestral distribution : Count the relative frequencies of \\(S_0\\) . If \\(S_0\\) is not available, we take \\(p_0\\) as the left eigenvector of eigenvalue 1 of M. Jukes-Cantor Model (JC) \u00b6 Simplest model 1. Distribution of the ancestral sequence is uniform: \\(p^{0}= (\\frac{1}{4}, \\frac{1}{4}, \\frac{1}{4}, \\frac{1}{4})\\) 2. Probability of obserivng a subsitution is the same (no matter the nucletoid) Kimura models \u00b6 Biological motivation Nucletoids have two groups according to biochemical structure. A substitution within the same type (within purines or within pyrimidines) is a transition, which is more likely. From purine to pyrimidine or the other way around, it's called transitions. 3 parameter kimura model 1. Uniform ancestral distribution 2. Particular structure of the matrix. 4 parameters but an extra restriction of the sum of probabilities = 1. From a mathematical point of view, the 3-parameter model is very nice to work with. Strand symmetric model (SSM) \u00b6 Exploits the strand symmetric model. 6 free parameters (a,b,c and d fixed py the som to 1 and so on) QUESTIONS: BUT WE ONLY LOOK INTO ONE STRAND OR NOT? General Markov model (GMM) \u00b6 No assumptions. Most general model Stationary process \u00b6 Definition \u00b6 The nucletoid distribution as m goes to infinity goes to a stationary distribution \\(\\pi = (\\pi_{A}, \\pi_{C}, \\pi_{G}, \\pi_{T})\\) . In case the process is homogenous and the transition matrix M has only positive entries, because of #Theorem Perron-Frobenius , we can garantee the limit exists. And the vector is the left eigenvector of M with eigenvalue 1. This is related to dynamical systems Time reversibility \u00b6 Time reversible if - Stationary - For any pair of nucleotides we need, the probability of getting X times the probability of X changing to Y must be equal to the probability of getting Y times the probability of changing Y to X. The idea of this is for \\(S_{0}= AGCAT\\) and \\(S_{1} = ATCGA\\) , we do not really know if \\(S_0\\) changed to \\(S_1\\) or the other way around. Or, in other words, we don't know if one species evolved into another one or the other way around. Biology vs Maths \u00b6 Time reversibility is assume in almost every paper in evolutionary. It implies \"time has no direction\". [[Felsenstein]] big guys in the field. It can be shown that most of the models assuming this condition, needs to be multiplicative close. That means, If \\(S_{0}\\rightarrow S_{1}\\) is governed by \\(M_1\\) and it's a JC model and then \\(S_{1}\\rightarrow S_{2}\\) is governed by \\(M_2\\) , then \\(S_{0}\\rightarrow S_{2}\\) is governed by \\(M_{1} \\cdot M_2\\) and should be also a JC model. Projects Recap \u00b6 Jesus S\u00e1nchez Slide 8: In GM, since the matrix is not necessarily symmetric, there's no time reversability. Continous models \u00b6 Slide 10: Instead of transition probability, we consider the matrix Q, where the entries correspond to the speed of substitutions between nucleotides. You can imagine it like 4 pools (A, C, G, T) and you can imagine the Q matrix as the quantity of water going from one pool to another (and in the diagonal is the water remaining.) This approach is much more strictive, because there's gonna be positive determinant for M and M will be non-singular (due to thefact that arises from \\(e^{Qt}\\) ). Time reversability \u00b6 \\(Q = RD(\\pi)\\) where \\(\\pi\\) is the ancient distribution or the equilibrium distribution. Felsenstein Hierarchy \u00b6 You have an organitzation of different substitution models according to how many parameters are there. Mutation rate \u00b6 Since the \"time\" is difficult to estimate, it's usually measured in terms of \"number of expected subtitutions per site\" when comparing sequencies. Under the continous-time approach, there's the mutation rate \\(\\mu_{\\pi, Q}\\) quantity, which is just an estimation of the number of expected substitutions per site from the rate matrix. You can normalize Q so you have one substitution per unit of time. To compare Q's is better to take the normalized versions. Evolutionary distances \u00b6 It's important that they are symmetric (as in JC or the KM - see slides). In the log-det distance we have to define if it's from \\(S_0\\) to \\(S_1\\) . It's not symmetric in general. You can get a very different value if you apply the formula in one direction or in the opposite one. If it's time reversible, the formula is symmetric. Remark For continous time substitutions processes: \\( \\(-\\frac{1}{4}\\log{\\det{M(t)}} = -tr(D(\\pi)Q)t\\) \\) Phylogenetic trees \u00b6 Cherry : is a couple of neighbouring leaves. Remember: in a phylogenetic tree you have labels in the leaves. To have the same tree, you need a graph isomorphism and you need to preserve the labeling. It's unfeasible to list all the tree topologies when you have a big number of leaves (even 20 is big enough). You can use Buneman's theory to define asymmetric difference between trees . Hidden Markov Models \u00b6 We assume certain independence (slide 30) of processes. Remark : HMM and phylogenetic trees look very similar. You could try to use this idea of the forward algorithm to evaluate the probability of nucleotide pattern at the leaves: it's the Felsenstein algorith . So this is the same as the forward algorithm but applied to phylogenetic tree. And, at the same time, that was the same as the Horner's rule.","title":"Evolutionary models"},{"location":"Mathematical%20models%20in%20biology/Evolutionary%20models%20lessons/#evolutionary-models","text":"","title":"Evolutionary models"},{"location":"Mathematical%20models%20in%20biology/Evolutionary%20models%20lessons/#markov-matrices","text":"Square matrix with real entries \\(\\geq 0\\) whose rows sum to 1 is called a Markov matrix (or stochastic matrix). Equivalent to seighg that the vector \\((1, \\dots 1)^T\\) is an eigenvector of eigenvalue 1. In general our Markov matrices will be \\(4 \\times 4\\) (4 nucletoids).","title":"Markov matrices"},{"location":"Mathematical%20models%20in%20biology/Evolutionary%20models%20lessons/#theorem-perron-frobenius","text":"","title":"Theorem (Perron-Frobenius)"},{"location":"Mathematical%20models%20in%20biology/Evolutionary%20models%20lessons/#claim","text":"... 1. 2. 3. [[convex sum]] of Markov matrices Is \\(\\Delta_n\\) a Algebraic group ? In other words, is the inverse of matrix A also a Markov matrix? The answer is no . Per #Theorem Perron-Frobenius , all eigenvalues are upper bounded by 1. Consider $ \\( \\(\\begin{pmatrix} \\frac{1}{n} & \\dots / \\frac{1}{n} \\\\\\\\ \\frac{1}{n} & \\dots / \\frac{1}{n} \\end{pmatrix}\\) \\) The determinant is 0 and it's not invertible (Proof in book in the slides).","title":"Claim"},{"location":"Mathematical%20models%20in%20biology/Evolutionary%20models%20lessons/#conditional-probabilities","text":"A is independent of B: \\(A \\perp \\!\\!\\! \\perp B\\) . ... Let's say A is independent of B given C: \\(A \\perp \\!\\!\\! \\perp B \\:|\\: C\\) . Two dependent events that become independent once we assume a third event.","title":"Conditional probabilities"},{"location":"Mathematical%20models%20in%20biology/Evolutionary%20models%20lessons/#discrete-time-probabilistic-processes","text":"In all this teory, time is considered discrete.","title":"Discrete-time probabilistic processes"},{"location":"Mathematical%20models%20in%20biology/Evolutionary%20models%20lessons/#discrete-time-markov-processes","text":"A discrete-time probabilistic process is called a Markov process if future is independent of past given the present. I don't care about what happened yesterday: like in the getting sick or the weather example. It's not the case for the \"self-avoiding random walk\". We can attach a matrix \\(M(n)\\) to run the transition from one state to the other. It's a Markov matrix. We call them Markov matrices .","title":"Discrete-time Markov processes"},{"location":"Mathematical%20models%20in%20biology/Evolutionary%20models%20lessons/#evolutionary-models_1","text":"Models : Simplifications of reality. Models are appropiate or not depending on what we want to study. More complex models does not mean better models. Substitution models : What happens at each step. We have a sequence of genes that is transmited and we want to understand the rule of how the substitution may act.","title":"Evolutionary models"},{"location":"Mathematical%20models%20in%20biology/Evolutionary%20models%20lessons/#nucleotide-substitution-models","text":"Assumption: Biological hereditary information encoded in the DNA Code of 4 nucleotides: A, G, C, T Double helix structure: pairing A-T, G-C The order or sequence determines the information available for building proteins (every 3 is an aminoacid) It can make copies of itself. Let's assume a sequence of nucleotides \\(S=TCAACTTGATC\\) changing through an evolutionary process \\( \\(S_{0} = S \\rightarrow S_{1}\\rightarrow S_{2}\\dots\\) \\) Possible changes: - Substitutions (error in duplication):We will focus in this one. - Insertions - Deletions We won't include insertions or deletions. Not realistic but it's the typical way of modeling.","title":"Nucleotide substitution models"},{"location":"Mathematical%20models%20in%20biology/Evolutionary%20models%20lessons/#homogeneous-markov-models","text":"Each site follows a discrete-time probabilistic process. We only care if each position is an A, C, G, T Hypothesis - iid hypothesis : Each site of the sequence evolves independently of the other sites and with the same probabilities. It's the same probability of changing in the end of the sequence that in the beginning (homogeneity across sites). Not realistic : Also because of the genetic code, substitutions in the 3rd nucletoid of the codon does not always matter. - The process does not depend on the past given the present (Markov hypothesis) - From generation to generation the same system of probabilities are ruling. This is not realistic if you consider a big amount of time. It's called homogeneity across lineages. HOmogeneous Markov model is determined by 1. Transition matrix 2. Ancestral distribution \\(S_0\\) . How are we estimating them? - Estimation : With the relative frequency of changes - Ancestral distribution : Count the relative frequencies of \\(S_0\\) . If \\(S_0\\) is not available, we take \\(p_0\\) as the left eigenvector of eigenvalue 1 of M.","title":"Homogeneous Markov models"},{"location":"Mathematical%20models%20in%20biology/Evolutionary%20models%20lessons/#jukes-cantor-model-jc","text":"Simplest model 1. Distribution of the ancestral sequence is uniform: \\(p^{0}= (\\frac{1}{4}, \\frac{1}{4}, \\frac{1}{4}, \\frac{1}{4})\\) 2. Probability of obserivng a subsitution is the same (no matter the nucletoid)","title":"Jukes-Cantor Model (JC)"},{"location":"Mathematical%20models%20in%20biology/Evolutionary%20models%20lessons/#kimura-models","text":"Biological motivation Nucletoids have two groups according to biochemical structure. A substitution within the same type (within purines or within pyrimidines) is a transition, which is more likely. From purine to pyrimidine or the other way around, it's called transitions. 3 parameter kimura model 1. Uniform ancestral distribution 2. Particular structure of the matrix. 4 parameters but an extra restriction of the sum of probabilities = 1. From a mathematical point of view, the 3-parameter model is very nice to work with.","title":"Kimura models"},{"location":"Mathematical%20models%20in%20biology/Evolutionary%20models%20lessons/#strand-symmetric-model-ssm","text":"Exploits the strand symmetric model. 6 free parameters (a,b,c and d fixed py the som to 1 and so on) QUESTIONS: BUT WE ONLY LOOK INTO ONE STRAND OR NOT?","title":"Strand symmetric model (SSM)"},{"location":"Mathematical%20models%20in%20biology/Evolutionary%20models%20lessons/#general-markov-model-gmm","text":"No assumptions. Most general model","title":"General Markov model (GMM)"},{"location":"Mathematical%20models%20in%20biology/Evolutionary%20models%20lessons/#stationary-process","text":"","title":"Stationary process"},{"location":"Mathematical%20models%20in%20biology/Evolutionary%20models%20lessons/#definition","text":"The nucletoid distribution as m goes to infinity goes to a stationary distribution \\(\\pi = (\\pi_{A}, \\pi_{C}, \\pi_{G}, \\pi_{T})\\) . In case the process is homogenous and the transition matrix M has only positive entries, because of #Theorem Perron-Frobenius , we can garantee the limit exists. And the vector is the left eigenvector of M with eigenvalue 1. This is related to dynamical systems","title":"Definition"},{"location":"Mathematical%20models%20in%20biology/Evolutionary%20models%20lessons/#time-reversibility","text":"Time reversible if - Stationary - For any pair of nucleotides we need, the probability of getting X times the probability of X changing to Y must be equal to the probability of getting Y times the probability of changing Y to X. The idea of this is for \\(S_{0}= AGCAT\\) and \\(S_{1} = ATCGA\\) , we do not really know if \\(S_0\\) changed to \\(S_1\\) or the other way around. Or, in other words, we don't know if one species evolved into another one or the other way around.","title":"Time reversibility"},{"location":"Mathematical%20models%20in%20biology/Evolutionary%20models%20lessons/#biology-vs-maths","text":"Time reversibility is assume in almost every paper in evolutionary. It implies \"time has no direction\". [[Felsenstein]] big guys in the field. It can be shown that most of the models assuming this condition, needs to be multiplicative close. That means, If \\(S_{0}\\rightarrow S_{1}\\) is governed by \\(M_1\\) and it's a JC model and then \\(S_{1}\\rightarrow S_{2}\\) is governed by \\(M_2\\) , then \\(S_{0}\\rightarrow S_{2}\\) is governed by \\(M_{1} \\cdot M_2\\) and should be also a JC model. Projects","title":"Biology vs Maths"},{"location":"Mathematical%20models%20in%20biology/Evolutionary%20models%20lessons/#recap","text":"Jesus S\u00e1nchez Slide 8: In GM, since the matrix is not necessarily symmetric, there's no time reversability.","title":"Recap"},{"location":"Mathematical%20models%20in%20biology/Evolutionary%20models%20lessons/#continous-models","text":"Slide 10: Instead of transition probability, we consider the matrix Q, where the entries correspond to the speed of substitutions between nucleotides. You can imagine it like 4 pools (A, C, G, T) and you can imagine the Q matrix as the quantity of water going from one pool to another (and in the diagonal is the water remaining.) This approach is much more strictive, because there's gonna be positive determinant for M and M will be non-singular (due to thefact that arises from \\(e^{Qt}\\) ).","title":"Continous models"},{"location":"Mathematical%20models%20in%20biology/Evolutionary%20models%20lessons/#time-reversability","text":"\\(Q = RD(\\pi)\\) where \\(\\pi\\) is the ancient distribution or the equilibrium distribution.","title":"Time reversability"},{"location":"Mathematical%20models%20in%20biology/Evolutionary%20models%20lessons/#felsenstein-hierarchy","text":"You have an organitzation of different substitution models according to how many parameters are there.","title":"Felsenstein Hierarchy"},{"location":"Mathematical%20models%20in%20biology/Evolutionary%20models%20lessons/#mutation-rate","text":"Since the \"time\" is difficult to estimate, it's usually measured in terms of \"number of expected subtitutions per site\" when comparing sequencies. Under the continous-time approach, there's the mutation rate \\(\\mu_{\\pi, Q}\\) quantity, which is just an estimation of the number of expected substitutions per site from the rate matrix. You can normalize Q so you have one substitution per unit of time. To compare Q's is better to take the normalized versions.","title":"Mutation rate"},{"location":"Mathematical%20models%20in%20biology/Evolutionary%20models%20lessons/#evolutionary-distances","text":"It's important that they are symmetric (as in JC or the KM - see slides). In the log-det distance we have to define if it's from \\(S_0\\) to \\(S_1\\) . It's not symmetric in general. You can get a very different value if you apply the formula in one direction or in the opposite one. If it's time reversible, the formula is symmetric. Remark For continous time substitutions processes: \\( \\(-\\frac{1}{4}\\log{\\det{M(t)}} = -tr(D(\\pi)Q)t\\) \\)","title":"Evolutionary distances"},{"location":"Mathematical%20models%20in%20biology/Evolutionary%20models%20lessons/#phylogenetic-trees","text":"Cherry : is a couple of neighbouring leaves. Remember: in a phylogenetic tree you have labels in the leaves. To have the same tree, you need a graph isomorphism and you need to preserve the labeling. It's unfeasible to list all the tree topologies when you have a big number of leaves (even 20 is big enough). You can use Buneman's theory to define asymmetric difference between trees .","title":"Phylogenetic trees"},{"location":"Mathematical%20models%20in%20biology/Evolutionary%20models%20lessons/#hidden-markov-models","text":"We assume certain independence (slide 30) of processes. Remark : HMM and phylogenetic trees look very similar. You could try to use this idea of the forward algorithm to evaluate the probability of nucleotide pattern at the leaves: it's the Felsenstein algorith . So this is the same as the forward algorithm but applied to phylogenetic tree. And, at the same time, that was the same as the Horner's rule.","title":"Hidden Markov Models"},{"location":"Mathematical%20models%20in%20biology/Genomics/","text":"Genomics \u00b6 Hidden Markov Models \u00b6 Forward-Viterbi pdf in atenea Decoding HMM \u00b6 Decoding means to find out the sequences of \\(y\\) that maximizes the probability of my observation. The Viterbi algorithm \u00b6 The Viterbi algorithm provides a sequence ( path ) that has the highest probability of generating the observed samples. Example \u00b6 Forward-Viterbi algorithm example I'm looking for a minimum since I'm taking the negative logarithm of a probability. \\(v_0(3)\\) is the maximum probability (minimum -log) of having A B B and a 0 as the hidden state above the last value. You take \\(e^{-whatever}\\) to obtain the maximum probability of obtaining the observed sequences. To know the path, we can check the underlined quantities in the algorithm, which will tell us the path. You start by the end. You know that \\(y_{3}= 0\\) . Then you go back and saw tha tthe minimum was observed for \\(y_{2} = 0\\) . And you keep going back. Tropical arithmetics \u00b6 We define two new operations - Tropical sum \\(x \\oplus y \\equiv min(x,y)\\) - Tropical product \\(x \\odot y \\equiv x + y\\) You can express the Viterbi algorithm in this new tropical arithmetics. In the end, what we have is the same as the tropicalization of the forward algorithm. The Horner rule applied to the tropical polynomial is the Viterbi algorithm. So, both the Viterbi algorithm and the Forward algorithm have the same structure. Example \u00b6 In Atenea, we have a [[SAGE maths]] workseet on HMM where you can play with HMM, apply the Viterbi algorithm, etc. Date: 24-10-2022 We will see two problems that can be seen as applications of the Hidden Markov Models. Sequence alignment \u00b6 Pairwise sequence alignment \u00b6 Assume we have two sequences of nucleotoids, $$\\begin{cases} \\sigma_{1}= \\sigma_{1}^{1}, \\sigma_{2}^{1} \\dots \\sigma_{n}^{1} \\ \\sigma_{2}= \\sigma_{1}^{2}, \\sigma_{2}^{2} \\dots \\sigma_{m}^{1} \\end{cases} $$ which may have different lengths \\(m\\) and \\(n\\) and we want to find a way to alignment. We will see three ways that are different but equivalent. Method 1 \u00b6 We define new pair of sequences, with alphabet Method 2 \u00b6 Homology : We had a nucleotoid in both sequences (no matter if they are the same or not). Deletions Insertions You need to impose two conditions on the new sequence formed by \\(H, D, I\\) (eq. 1) Method 3 \u00b6 Three different types of edges: - Horizontal edges correspond to insertions - Vertical vedges correspond to deletions - Diagonal edges are labeled by the homologies All possible paths correspond to all possible alignemnts Example \u00b6 (Picture / slides) Scoring scheme \u00b6 We are looking for good alignments. We need to define what optimal means in a mathematical way. For that, we need a scoring scheme Each weight in \\(\\{w\\}_{i,j}\\) corresponds to the weight from going from \\(i\\) to \\(j\\) . The \\(4 \\times 4\\) matrix, corresponds to the homology case. From biological point of view, once we open the possibility to insertion, an insertion right next to it should not be as penalized. So, all insertions that are consecutive, should all be penalized once. That's the reason why we may want to use \\(w'\\) . The wieghts in \\(w'\\) correspond to the weight \\(w'_{H, H}\\) for a homology after a homology. In a way, it's the penalty for changing the \"edit mode\" in the alignment (passing from homology to insertions). Given two sequences, we can define a score for the alignment. Def: Global alignment problem with scores \u00b6 Fin the global alignment \\(h\\) with minimal weight among all alignments of \\(\\sigma_1\\) and \\(\\sigma_2\\) . This is equivalent to assigning weights to the edges of the aligment graph \\(G_{n,m}\\) . With this notation, the previous problem is equivalent to finding the shortest path from node \\((0,0)\\) to node \\((n, m)\\) . Needleman\u2013Wunsch algorithm \u00b6 Example \u00b6 In the paper. Pair Hidden Markov Model \u00b6 This is essentially the Viterby algorithm. We can consider a Hidden Markov model: - hidden states \\(\\Sigma' = \\{H, I, D\\}\\) - observable states \\(\\{A, C, G, T, \\_ \\} ^{2}\\) For the emission matrix, we whould have something super big ( \\(25 \\times 3\\) ), but there are combinations that do not make sense (check notes) \\(\\Delta_{16}\\) : Probability simplex of dimension 16 (the sixten points must sum 1) In the end, the Needlman-Wunsch is just an application of the Viterbi algorithm in a conveninent Hidden Markov model (the one defined in the previous slide). CpG islands and gene finding \u00b6 CpG sites are just two nucleotides, which are consecutive and appear in this particular order (C and G). There are regions that have alot of this pairings. We have particular regions in the geneome which can suffer this methylated process and when this happens the transition rate for \\(C\\rightarrow T\\) mutation is very high. this is why we don't see those many of these paris. Consequences : In genes, if we observe \\(C \\rightarrow T\\) , it may be highly detrimental, meaning that the organism may no longer succeed. CpG islands \u00b6 Consider the following trnasition matrices \\(\\theta\\) , where \\(\\theta_{AA}\\) means that an A follows an A. The transition matrices for CpG and non CpG are estimated from data. If the log is positive, it means it's a CpG island, if not, it's not. Gene finding \u00b6 Assume we already know the sequence it's a gene. We want to identify exons and introns. As hidden states: - \\(E_1\\) the site is the first element of a codon in an exon - \\(E_2\\) the site is the second element of a codon in an exon - \\(E_3\\) the site is the last element of a codon in an exon We can translate the state space diagram into transition matrix: \\( \\(\\begin{pmatrix}E_1 & 0 & 1 & 0 & 0 \\\\ E_2 & 0 & 0 & 1 & 0 \\\\ E_3 & * & 0 & 0 & * \\\\ I & * & 0 & 0 & *\\end{pmatrix}\\) \\) If we want to decode exons and introns, we'd apply the Viterbi algorithm to this HMM.","title":"Genomics"},{"location":"Mathematical%20models%20in%20biology/Genomics/#genomics","text":"","title":"Genomics"},{"location":"Mathematical%20models%20in%20biology/Genomics/#hidden-markov-models","text":"Forward-Viterbi pdf in atenea","title":"Hidden Markov Models"},{"location":"Mathematical%20models%20in%20biology/Genomics/#decoding-hmm","text":"Decoding means to find out the sequences of \\(y\\) that maximizes the probability of my observation.","title":"Decoding HMM"},{"location":"Mathematical%20models%20in%20biology/Genomics/#the-viterbi-algorithm","text":"The Viterbi algorithm provides a sequence ( path ) that has the highest probability of generating the observed samples.","title":"The Viterbi algorithm"},{"location":"Mathematical%20models%20in%20biology/Genomics/#example","text":"Forward-Viterbi algorithm example I'm looking for a minimum since I'm taking the negative logarithm of a probability. \\(v_0(3)\\) is the maximum probability (minimum -log) of having A B B and a 0 as the hidden state above the last value. You take \\(e^{-whatever}\\) to obtain the maximum probability of obtaining the observed sequences. To know the path, we can check the underlined quantities in the algorithm, which will tell us the path. You start by the end. You know that \\(y_{3}= 0\\) . Then you go back and saw tha tthe minimum was observed for \\(y_{2} = 0\\) . And you keep going back.","title":"Example"},{"location":"Mathematical%20models%20in%20biology/Genomics/#tropical-arithmetics","text":"We define two new operations - Tropical sum \\(x \\oplus y \\equiv min(x,y)\\) - Tropical product \\(x \\odot y \\equiv x + y\\) You can express the Viterbi algorithm in this new tropical arithmetics. In the end, what we have is the same as the tropicalization of the forward algorithm. The Horner rule applied to the tropical polynomial is the Viterbi algorithm. So, both the Viterbi algorithm and the Forward algorithm have the same structure.","title":"Tropical arithmetics"},{"location":"Mathematical%20models%20in%20biology/Genomics/#example_1","text":"In Atenea, we have a [[SAGE maths]] workseet on HMM where you can play with HMM, apply the Viterbi algorithm, etc. Date: 24-10-2022 We will see two problems that can be seen as applications of the Hidden Markov Models.","title":"Example"},{"location":"Mathematical%20models%20in%20biology/Genomics/#sequence-alignment","text":"","title":"Sequence alignment"},{"location":"Mathematical%20models%20in%20biology/Genomics/#pairwise-sequence-alignment","text":"Assume we have two sequences of nucleotoids, $$\\begin{cases} \\sigma_{1}= \\sigma_{1}^{1}, \\sigma_{2}^{1} \\dots \\sigma_{n}^{1} \\ \\sigma_{2}= \\sigma_{1}^{2}, \\sigma_{2}^{2} \\dots \\sigma_{m}^{1} \\end{cases} $$ which may have different lengths \\(m\\) and \\(n\\) and we want to find a way to alignment. We will see three ways that are different but equivalent.","title":"Pairwise sequence alignment"},{"location":"Mathematical%20models%20in%20biology/Genomics/#method-1","text":"We define new pair of sequences, with alphabet","title":"Method 1"},{"location":"Mathematical%20models%20in%20biology/Genomics/#method-2","text":"Homology : We had a nucleotoid in both sequences (no matter if they are the same or not). Deletions Insertions You need to impose two conditions on the new sequence formed by \\(H, D, I\\) (eq. 1)","title":"Method 2"},{"location":"Mathematical%20models%20in%20biology/Genomics/#method-3","text":"Three different types of edges: - Horizontal edges correspond to insertions - Vertical vedges correspond to deletions - Diagonal edges are labeled by the homologies All possible paths correspond to all possible alignemnts","title":"Method 3"},{"location":"Mathematical%20models%20in%20biology/Genomics/#example_2","text":"(Picture / slides)","title":"Example"},{"location":"Mathematical%20models%20in%20biology/Genomics/#scoring-scheme","text":"We are looking for good alignments. We need to define what optimal means in a mathematical way. For that, we need a scoring scheme Each weight in \\(\\{w\\}_{i,j}\\) corresponds to the weight from going from \\(i\\) to \\(j\\) . The \\(4 \\times 4\\) matrix, corresponds to the homology case. From biological point of view, once we open the possibility to insertion, an insertion right next to it should not be as penalized. So, all insertions that are consecutive, should all be penalized once. That's the reason why we may want to use \\(w'\\) . The wieghts in \\(w'\\) correspond to the weight \\(w'_{H, H}\\) for a homology after a homology. In a way, it's the penalty for changing the \"edit mode\" in the alignment (passing from homology to insertions). Given two sequences, we can define a score for the alignment.","title":"Scoring scheme"},{"location":"Mathematical%20models%20in%20biology/Genomics/#def-global-alignment-problem-with-scores","text":"Fin the global alignment \\(h\\) with minimal weight among all alignments of \\(\\sigma_1\\) and \\(\\sigma_2\\) . This is equivalent to assigning weights to the edges of the aligment graph \\(G_{n,m}\\) . With this notation, the previous problem is equivalent to finding the shortest path from node \\((0,0)\\) to node \\((n, m)\\) .","title":"Def: Global alignment problem with scores"},{"location":"Mathematical%20models%20in%20biology/Genomics/#needlemanwunsch-algorithm","text":"","title":"Needleman\u2013Wunsch algorithm"},{"location":"Mathematical%20models%20in%20biology/Genomics/#example_3","text":"In the paper.","title":"Example"},{"location":"Mathematical%20models%20in%20biology/Genomics/#pair-hidden-markov-model","text":"This is essentially the Viterby algorithm. We can consider a Hidden Markov model: - hidden states \\(\\Sigma' = \\{H, I, D\\}\\) - observable states \\(\\{A, C, G, T, \\_ \\} ^{2}\\) For the emission matrix, we whould have something super big ( \\(25 \\times 3\\) ), but there are combinations that do not make sense (check notes) \\(\\Delta_{16}\\) : Probability simplex of dimension 16 (the sixten points must sum 1) In the end, the Needlman-Wunsch is just an application of the Viterbi algorithm in a conveninent Hidden Markov model (the one defined in the previous slide).","title":"Pair Hidden Markov Model"},{"location":"Mathematical%20models%20in%20biology/Genomics/#cpg-islands-and-gene-finding","text":"CpG sites are just two nucleotides, which are consecutive and appear in this particular order (C and G). There are regions that have alot of this pairings. We have particular regions in the geneome which can suffer this methylated process and when this happens the transition rate for \\(C\\rightarrow T\\) mutation is very high. this is why we don't see those many of these paris. Consequences : In genes, if we observe \\(C \\rightarrow T\\) , it may be highly detrimental, meaning that the organism may no longer succeed.","title":"CpG islands and gene finding"},{"location":"Mathematical%20models%20in%20biology/Genomics/#cpg-islands","text":"Consider the following trnasition matrices \\(\\theta\\) , where \\(\\theta_{AA}\\) means that an A follows an A. The transition matrices for CpG and non CpG are estimated from data. If the log is positive, it means it's a CpG island, if not, it's not.","title":"CpG islands"},{"location":"Mathematical%20models%20in%20biology/Genomics/#gene-finding","text":"Assume we already know the sequence it's a gene. We want to identify exons and introns. As hidden states: - \\(E_1\\) the site is the first element of a codon in an exon - \\(E_2\\) the site is the second element of a codon in an exon - \\(E_3\\) the site is the last element of a codon in an exon We can translate the state space diagram into transition matrix: \\( \\(\\begin{pmatrix}E_1 & 0 & 1 & 0 & 0 \\\\ E_2 & 0 & 0 & 1 & 0 \\\\ E_3 & * & 0 & 0 & * \\\\ I & * & 0 & 0 & *\\end{pmatrix}\\) \\) If we want to decode exons and introns, we'd apply the Viterbi algorithm to this HMM.","title":"Gene finding"},{"location":"Mathematical%20models%20in%20biology/Introduction%20class%20%20to%20MMiB/","text":"Introduction \u00b6 Teaching staff Jes\u00fas Fern\u00e1ndez (cabell moreno, ulleres, barba) Gemma Huguet Adri\u00e1n Ponce-\u00c1lvarez (cabell negre) Course structure - Phylogenetics (JF, 2 blocks, 40%) - September / October - Populations dynamics (GH, 1 block, 20%) - November - Neuroscience (GH, APA) - November / December Teaching methodology By block, - Introductory sessions: basics in the topic and statement of short-projects ( groups of 3 ) - Short-projects: - work in class and outside - present in front of class with slides (to be delivered and shared with students) - most important thing: communicate what the project about and the main results (not as specific as an exam) - report to be submitted (later on) - wrap -up session: reveiw the basics, including things presented in the mini-projects. Each block takes around 6 sessions . Software and programming - Matlab, python, C... - free to use the software you are more comfortable. - XPP/AUTO: specific for neuroscience. Teaching resources Everything in ATENEA. We can ask for books to the professors (they cannot put them in Atenea) Qualification system - 50%: Based on the mini-projects (by weight) - 20%: Individual evaulation depending on participation: - Implication in the projects, questions to classmates, etc. - Final exam (January 9th): - 5 questions per block that will be provided at the end of each block (5 x 5 = 25 questions in the pool). You can prepare them in advance. - For the exam, they will choose 5 questions. - The material will include the topics introduced in class and also from the reports of our classmates. Brief CVs \u00b6 Jes\u00fas Fern\u00e1ndez: PhD in Algebraic Geometry. Phylogenetic Gemma Huguet: PhD in Dynamical Systems (theory) and now more applied things (NYU postdoc in comp neuroscience) Adri\u00e1n Ponce: Physics and PhD in neuroscience. Theoretical neuroscience: neural networks using dyanmical systems, statistical physics Intro to the blocks \u00b6 Populations dynamics \u00b6 Mathematical models to understand the dynamics of populations and predict their evolutions (ecology, epidemoiology) Mathematical aspects \u00b6 Types of modeling - Models of differential equations - Discrete dynamical systems Methodological aspects - Dynamical Systems techniques - Computational tools Concepts - Equilibrium points - Periodoc orbits - Bifurcations of low codimension - Data fitting Computational neuroscience \u00b6 Basics \u00b6 We work with neurons and synapsis: spans large temporal and spatial scales Spatial scales of the brain - Whole brain - Cortical area - Local network or column - Neuron - Sub-cellular compartements - Molecules Blocks We will have to courses on neuroscience: - Single cell neurophysiology - Neuronal networks: firing rate models and mean field equations - working memory, decision making and perceptual bistability","title":"Introduction"},{"location":"Mathematical%20models%20in%20biology/Introduction%20class%20%20to%20MMiB/#introduction","text":"Teaching staff Jes\u00fas Fern\u00e1ndez (cabell moreno, ulleres, barba) Gemma Huguet Adri\u00e1n Ponce-\u00c1lvarez (cabell negre) Course structure - Phylogenetics (JF, 2 blocks, 40%) - September / October - Populations dynamics (GH, 1 block, 20%) - November - Neuroscience (GH, APA) - November / December Teaching methodology By block, - Introductory sessions: basics in the topic and statement of short-projects ( groups of 3 ) - Short-projects: - work in class and outside - present in front of class with slides (to be delivered and shared with students) - most important thing: communicate what the project about and the main results (not as specific as an exam) - report to be submitted (later on) - wrap -up session: reveiw the basics, including things presented in the mini-projects. Each block takes around 6 sessions . Software and programming - Matlab, python, C... - free to use the software you are more comfortable. - XPP/AUTO: specific for neuroscience. Teaching resources Everything in ATENEA. We can ask for books to the professors (they cannot put them in Atenea) Qualification system - 50%: Based on the mini-projects (by weight) - 20%: Individual evaulation depending on participation: - Implication in the projects, questions to classmates, etc. - Final exam (January 9th): - 5 questions per block that will be provided at the end of each block (5 x 5 = 25 questions in the pool). You can prepare them in advance. - For the exam, they will choose 5 questions. - The material will include the topics introduced in class and also from the reports of our classmates.","title":"Introduction"},{"location":"Mathematical%20models%20in%20biology/Introduction%20class%20%20to%20MMiB/#brief-cvs","text":"Jes\u00fas Fern\u00e1ndez: PhD in Algebraic Geometry. Phylogenetic Gemma Huguet: PhD in Dynamical Systems (theory) and now more applied things (NYU postdoc in comp neuroscience) Adri\u00e1n Ponce: Physics and PhD in neuroscience. Theoretical neuroscience: neural networks using dyanmical systems, statistical physics","title":"Brief CVs"},{"location":"Mathematical%20models%20in%20biology/Introduction%20class%20%20to%20MMiB/#intro-to-the-blocks","text":"","title":"Intro to the blocks"},{"location":"Mathematical%20models%20in%20biology/Introduction%20class%20%20to%20MMiB/#populations-dynamics","text":"Mathematical models to understand the dynamics of populations and predict their evolutions (ecology, epidemoiology)","title":"Populations dynamics"},{"location":"Mathematical%20models%20in%20biology/Introduction%20class%20%20to%20MMiB/#mathematical-aspects","text":"Types of modeling - Models of differential equations - Discrete dynamical systems Methodological aspects - Dynamical Systems techniques - Computational tools Concepts - Equilibrium points - Periodoc orbits - Bifurcations of low codimension - Data fitting","title":"Mathematical aspects"},{"location":"Mathematical%20models%20in%20biology/Introduction%20class%20%20to%20MMiB/#computational-neuroscience","text":"","title":"Computational neuroscience"},{"location":"Mathematical%20models%20in%20biology/Introduction%20class%20%20to%20MMiB/#basics","text":"We work with neurons and synapsis: spans large temporal and spatial scales Spatial scales of the brain - Whole brain - Cortical area - Local network or column - Neuron - Sub-cellular compartements - Molecules Blocks We will have to courses on neuroscience: - Single cell neurophysiology - Neuronal networks: firing rate models and mean field equations - working memory, decision making and perceptual bistability","title":"Basics"},{"location":"Mathematical%20models%20in%20biology/Outline%20-%20Mathematical%20models%20in%20Biology/","text":"Mathematical models in Biology \u00b6 Outline \u00b6 Introduction Phylogenetics and Genomics - Jes\u00fas Fern\u00e1ndez Population dynamics and epidemiology - Gemma Huguet Population dynamics Projects Computational neuroscience","title":"Mathematical models in Biology"},{"location":"Mathematical%20models%20in%20biology/Outline%20-%20Mathematical%20models%20in%20Biology/#mathematical-models-in-biology","text":"","title":"Mathematical models in Biology"},{"location":"Mathematical%20models%20in%20biology/Outline%20-%20Mathematical%20models%20in%20Biology/#outline","text":"Introduction Phylogenetics and Genomics - Jes\u00fas Fern\u00e1ndez Population dynamics and epidemiology - Gemma Huguet Population dynamics Projects Computational neuroscience","title":"Outline"},{"location":"Mathematical%20models%20in%20biology/Phylogenetic%20reconstruction%20lesson/","text":"Phylogenetic reconstruction \u00b6 Introduction \u00b6 Now it's considered there's less than 20,000 genes. For some time, people considered that the number of genes was related to how complex an organism was, but now it's not clear anymore. The bond from A to T is weaker (2 hydrogen bonds) than the one from C to G (3 bonds). mRNA has a copy of a particular region of the DNA and it's able to leave the nucleus. Gene structure \u00b6 You have a given number of regions called exons, which contain the necessary information to build the aminoacids. Between exons, we have the introns. At the beginning of the gene, we may have this particular codon: ATG. There are other possibilities. And in the end we have 3 possibilities of codons to mark its stop. Sometimes we have a TATA box where the RNA starts to stick to begin the translation processes. Why it's TATA? Because these are the weaker links, it's easier to separate the strands. Two steps: 1. Transcription. Molecule consisting only of the exons, introns and the beggining and end block (pre-mRNA). 2. Splicing: we remove the introns, this is what will be transofrm into aminoacids (mRNA). Multiple sequence alignment \u00b6 We would like to arrange the sequences in a special way so to know the sequences that correspond to each other. Tree \u00b6 Once we have them aligned, we want a method to obtain a tree. Distance-based methods \u00b6 Based ona system of distances d computed from DNA sequences - Jukes-Cantor distance or log-det - Reduce the sequences to a symmetric table - d is not a metric. Character-based methods \u00b6 They try to use all the information contained in the sequences. Quartet based methods \u00b6 First to contruct trees with 4 leaves (quartets) and then joined them together. For all the processes, we will get a geneand we can go to a website, get alignments of primates and apply it to get a phylogenetic tree. Outline \u00b6 Quartet-based methods is typically based on maximum likelihood, but we could use also as input the branch-length of the interior edge provided by neighbor-joining. Note: the professor uses quartet to refer to topology. Distance based methods \u00b6 The tree metric ( \\(d_T\\) ). UPGMA \u00b6 Among the ones we've seen is the fastest. Molecular clock is okay(ish) if all the species are close. UPGMA is a method that is okay when you are analyzing species that are very similar. They are not using all the possible information on the sequences. It also happens with neighbor-joining. Neighbor-Joining Algortihm \u00b6 It's one of the important methods for phylogenetic reconstruction. Outgroup: you add a species that is very different from the others and then the node that connects this to the rest of the group is a root. Character-based methods \u00b6 Attempt to use all the information in the sequences. Maximum parsimony \u00b6 The number of non-informative sites is very small compared to the total number of sites. For some trees, MP is not consistent. For this, MP is no longer very used. Maximum likelihood \u00b6 The notation of the professor is slightly different. It tries to estimate the tree and parameters that maximize the likelihood. It's an optimization problem, which is a big problem in mathematics. There's some numerical methods, like the Newton's method. Probably the most used method to apply ML is the expectation maximization . The problem is that the method may not converge. Usually, the tree topology is provided by neighbor-joining and the other parameters are found in ML. Quartet-based methods \u00b6 Long branch-attraction : When you have long branches (same tree that gives problems to MP and UPGMA), it tends to put the long branches together. This is because, as we saw in Proj 2, the maximum number of observable substitutions when comparing two sequences tend to \\(3/4\\) . So, the distance between 1 and 3 (the two long branches) cannot grow forever. The problem of quartet based methods is that they don't know how to use the weights. There's still not a clear way on how to use the weights to build a tree.","title":"Phylogenetic reconstruction"},{"location":"Mathematical%20models%20in%20biology/Phylogenetic%20reconstruction%20lesson/#phylogenetic-reconstruction","text":"","title":"Phylogenetic reconstruction"},{"location":"Mathematical%20models%20in%20biology/Phylogenetic%20reconstruction%20lesson/#introduction","text":"Now it's considered there's less than 20,000 genes. For some time, people considered that the number of genes was related to how complex an organism was, but now it's not clear anymore. The bond from A to T is weaker (2 hydrogen bonds) than the one from C to G (3 bonds). mRNA has a copy of a particular region of the DNA and it's able to leave the nucleus.","title":"Introduction"},{"location":"Mathematical%20models%20in%20biology/Phylogenetic%20reconstruction%20lesson/#gene-structure","text":"You have a given number of regions called exons, which contain the necessary information to build the aminoacids. Between exons, we have the introns. At the beginning of the gene, we may have this particular codon: ATG. There are other possibilities. And in the end we have 3 possibilities of codons to mark its stop. Sometimes we have a TATA box where the RNA starts to stick to begin the translation processes. Why it's TATA? Because these are the weaker links, it's easier to separate the strands. Two steps: 1. Transcription. Molecule consisting only of the exons, introns and the beggining and end block (pre-mRNA). 2. Splicing: we remove the introns, this is what will be transofrm into aminoacids (mRNA).","title":"Gene structure"},{"location":"Mathematical%20models%20in%20biology/Phylogenetic%20reconstruction%20lesson/#multiple-sequence-alignment","text":"We would like to arrange the sequences in a special way so to know the sequences that correspond to each other.","title":"Multiple sequence alignment"},{"location":"Mathematical%20models%20in%20biology/Phylogenetic%20reconstruction%20lesson/#tree","text":"Once we have them aligned, we want a method to obtain a tree.","title":"Tree"},{"location":"Mathematical%20models%20in%20biology/Phylogenetic%20reconstruction%20lesson/#distance-based-methods","text":"Based ona system of distances d computed from DNA sequences - Jukes-Cantor distance or log-det - Reduce the sequences to a symmetric table - d is not a metric.","title":"Distance-based methods"},{"location":"Mathematical%20models%20in%20biology/Phylogenetic%20reconstruction%20lesson/#character-based-methods","text":"They try to use all the information contained in the sequences.","title":"Character-based methods"},{"location":"Mathematical%20models%20in%20biology/Phylogenetic%20reconstruction%20lesson/#quartet-based-methods","text":"First to contruct trees with 4 leaves (quartets) and then joined them together. For all the processes, we will get a geneand we can go to a website, get alignments of primates and apply it to get a phylogenetic tree.","title":"Quartet based methods"},{"location":"Mathematical%20models%20in%20biology/Phylogenetic%20reconstruction%20lesson/#outline","text":"Quartet-based methods is typically based on maximum likelihood, but we could use also as input the branch-length of the interior edge provided by neighbor-joining. Note: the professor uses quartet to refer to topology.","title":"Outline"},{"location":"Mathematical%20models%20in%20biology/Phylogenetic%20reconstruction%20lesson/#distance-based-methods_1","text":"The tree metric ( \\(d_T\\) ).","title":"Distance based methods"},{"location":"Mathematical%20models%20in%20biology/Phylogenetic%20reconstruction%20lesson/#upgma","text":"Among the ones we've seen is the fastest. Molecular clock is okay(ish) if all the species are close. UPGMA is a method that is okay when you are analyzing species that are very similar. They are not using all the possible information on the sequences. It also happens with neighbor-joining.","title":"UPGMA"},{"location":"Mathematical%20models%20in%20biology/Phylogenetic%20reconstruction%20lesson/#neighbor-joining-algortihm","text":"It's one of the important methods for phylogenetic reconstruction. Outgroup: you add a species that is very different from the others and then the node that connects this to the rest of the group is a root.","title":"Neighbor-Joining Algortihm"},{"location":"Mathematical%20models%20in%20biology/Phylogenetic%20reconstruction%20lesson/#character-based-methods_1","text":"Attempt to use all the information in the sequences.","title":"Character-based methods"},{"location":"Mathematical%20models%20in%20biology/Phylogenetic%20reconstruction%20lesson/#maximum-parsimony","text":"The number of non-informative sites is very small compared to the total number of sites. For some trees, MP is not consistent. For this, MP is no longer very used.","title":"Maximum parsimony"},{"location":"Mathematical%20models%20in%20biology/Phylogenetic%20reconstruction%20lesson/#maximum-likelihood","text":"The notation of the professor is slightly different. It tries to estimate the tree and parameters that maximize the likelihood. It's an optimization problem, which is a big problem in mathematics. There's some numerical methods, like the Newton's method. Probably the most used method to apply ML is the expectation maximization . The problem is that the method may not converge. Usually, the tree topology is provided by neighbor-joining and the other parameters are found in ML.","title":"Maximum likelihood"},{"location":"Mathematical%20models%20in%20biology/Phylogenetic%20reconstruction%20lesson/#quartet-based-methods_1","text":"Long branch-attraction : When you have long branches (same tree that gives problems to MP and UPGMA), it tends to put the long branches together. This is because, as we saw in Proj 2, the maximum number of observable substitutions when comparing two sequences tend to \\(3/4\\) . So, the distance between 1 and 3 (the two long branches) cannot grow forever. The problem of quartet based methods is that they don't know how to use the weights. There's still not a clear way on how to use the weights to build a tree.","title":"Quartet-based methods"},{"location":"Mathematical%20models%20in%20biology/Phylogenetics%20and%20Genomics%20-%20Outline/","text":"Phylogenetics and Genomics \u00b6 Outline \u00b6 Plan \u00b6 Evolutionary models : Understand evolution: define and design substitution models and evolutionary models on trees (this mainly for the project). 2 weeks Projects Phylogenetic reconstruction Up to 4 methods to see the phylogenetic tree. Different methods do not necessarily give the same tree and we don't know the right tree. We can only guess. 2 weeks Projects Genomics : How to find the best alignment and how to find genes in all the genome. 2 weeks Introduction \u00b6 Phylogenetic tree: - Where does the virus comes from. - It describes the evolutionary history. - How can we obtain this tree? From the genetic information in the DNA molecule. DNA - Most of the DNA is sequences of DNA with no known meaning (junk DNA). We don't know if they're responsible of some function or not. - Different species have different number of chromosomes and different length. Geneomics: Gene prediction \u00b6 How to find the genes inside the genome? Using maths we can propose statisical models to detect possible genes and compare them to other species. The problem is that different species have different number of chromosomes and different lengths. This leads to the problem of multiple sequence alignment Multiple sequence alignment \u00b6 Define a criterium to define which sequences are close to another: minimize the number of differences. Evoutionary models \u00b6 Matrix where each element corresponds to the probability of a nucleitaid A changes to C, for instance. Phylogenetic reconstruction \u00b6 We will see different methods to find trees that lead to certain species. Blocks \u00b6 1st block - Evolutionary models - Phylogenetic reconstruction - Genomics (gene prediction: how to establish a pattern for the structure of a gene, multiple alignment of sequences) We will do projects for the first and second part (20 and 20%). Mathematical approach \u00b6 Stochastic models (Hidden) Markov processes Algortihmics Algebraic tools: eigenvalues, eigenvectors... (linear algebra) Bibliography \u00b6 Mathametical models in biology: Nice book, very easy and pleasant to read. Focus on Ch 4-5. It can be used also for neuroscience. Algebraic statistics for computational biology: Not as easy, but full of ideas. Some technical points of the project appear in this book. Biological sequence analysis: They solve the same problems but from a biologists perspective. Phylogeny: Discrete and Random Processes in Evolution -> for further reading The Tree of Life GAME \u00b6 Nodes: represent Leaves: current species Interior nodes: ancestral species Edges: represent evolutionary processes Root: common ancestor We will get 10 questions to prepare for the final exam and only 2 will be selected.","title":"Phylogenetics and Genomics"},{"location":"Mathematical%20models%20in%20biology/Phylogenetics%20and%20Genomics%20-%20Outline/#phylogenetics-and-genomics","text":"","title":"Phylogenetics and Genomics"},{"location":"Mathematical%20models%20in%20biology/Phylogenetics%20and%20Genomics%20-%20Outline/#outline","text":"","title":"Outline"},{"location":"Mathematical%20models%20in%20biology/Phylogenetics%20and%20Genomics%20-%20Outline/#plan","text":"Evolutionary models : Understand evolution: define and design substitution models and evolutionary models on trees (this mainly for the project). 2 weeks Projects Phylogenetic reconstruction Up to 4 methods to see the phylogenetic tree. Different methods do not necessarily give the same tree and we don't know the right tree. We can only guess. 2 weeks Projects Genomics : How to find the best alignment and how to find genes in all the genome. 2 weeks","title":"Plan"},{"location":"Mathematical%20models%20in%20biology/Phylogenetics%20and%20Genomics%20-%20Outline/#introduction","text":"Phylogenetic tree: - Where does the virus comes from. - It describes the evolutionary history. - How can we obtain this tree? From the genetic information in the DNA molecule. DNA - Most of the DNA is sequences of DNA with no known meaning (junk DNA). We don't know if they're responsible of some function or not. - Different species have different number of chromosomes and different length.","title":"Introduction"},{"location":"Mathematical%20models%20in%20biology/Phylogenetics%20and%20Genomics%20-%20Outline/#geneomics-gene-prediction","text":"How to find the genes inside the genome? Using maths we can propose statisical models to detect possible genes and compare them to other species. The problem is that different species have different number of chromosomes and different lengths. This leads to the problem of multiple sequence alignment","title":"Geneomics: Gene prediction"},{"location":"Mathematical%20models%20in%20biology/Phylogenetics%20and%20Genomics%20-%20Outline/#multiple-sequence-alignment","text":"Define a criterium to define which sequences are close to another: minimize the number of differences.","title":"Multiple sequence alignment"},{"location":"Mathematical%20models%20in%20biology/Phylogenetics%20and%20Genomics%20-%20Outline/#evoutionary-models","text":"Matrix where each element corresponds to the probability of a nucleitaid A changes to C, for instance.","title":"Evoutionary models"},{"location":"Mathematical%20models%20in%20biology/Phylogenetics%20and%20Genomics%20-%20Outline/#phylogenetic-reconstruction","text":"We will see different methods to find trees that lead to certain species.","title":"Phylogenetic reconstruction"},{"location":"Mathematical%20models%20in%20biology/Phylogenetics%20and%20Genomics%20-%20Outline/#blocks","text":"1st block - Evolutionary models - Phylogenetic reconstruction - Genomics (gene prediction: how to establish a pattern for the structure of a gene, multiple alignment of sequences) We will do projects for the first and second part (20 and 20%).","title":"Blocks"},{"location":"Mathematical%20models%20in%20biology/Phylogenetics%20and%20Genomics%20-%20Outline/#mathematical-approach","text":"Stochastic models (Hidden) Markov processes Algortihmics Algebraic tools: eigenvalues, eigenvectors... (linear algebra)","title":"Mathematical approach"},{"location":"Mathematical%20models%20in%20biology/Phylogenetics%20and%20Genomics%20-%20Outline/#bibliography","text":"Mathametical models in biology: Nice book, very easy and pleasant to read. Focus on Ch 4-5. It can be used also for neuroscience. Algebraic statistics for computational biology: Not as easy, but full of ideas. Some technical points of the project appear in this book. Biological sequence analysis: They solve the same problems but from a biologists perspective. Phylogeny: Discrete and Random Processes in Evolution -> for further reading","title":"Bibliography"},{"location":"Mathematical%20models%20in%20biology/Phylogenetics%20and%20Genomics%20-%20Outline/#the-tree-of-life-game","text":"Nodes: represent Leaves: current species Interior nodes: ancestral species Edges: represent evolutionary processes Root: common ancestor We will get 10 questions to prepare for the final exam and only 2 will be selected.","title":"The Tree of Life GAME"},{"location":"Mathematical%20models%20in%20biology/Population%20dynamics%20lecture/","text":"Population dynamics \u00b6 One dimensional models in ecology. A single species dynamics Continuous Models Discrete models Dynamics of interacting populations Types of species' interactions. One dimensional models in ecology: \u00b6 Fibonacci sequence \u00b6 First naive model. Idealised (biologically unrealistic) rabbit population. Model \u00b6 \\(F_n\\) : number of pairs of rabbits at month \\(n\\) \\(F_{0}\\) =1 (one male and one female) Rabbits are able to mat at one month old (at the end of its second month, a female can produce another pair) Rabbits never die and a mating pair always produces one new pair (one male, one female) every month. How many rabbits at month \\(n\\) ? \\(F_{n}\\) = # newborn pairs of rabbits + # adult pairs of adults = = # young at month n-1 + 2 (# adults at month \\(n-1\\) ) = \\((F_{n-1} - F_{n-2}) + 2(F_{n-1})\\) = \\(F_{n-1}+F_{n-2}\\) # newborn pairs of rabbits = # pairs that did not exist at the month \\(n-1\\) : \\(F_{n}- F_{n-1}\\) # aduult pairs of rabbits = # pairs thatexist at the month \\(n-1\\) : \\(F_{n-1}\\) Malthusian growth \u00b6 From [[Thomas Robert Malthus]] - \\(N(t)\\) : population size at time \\(t\\) - \\(\\frac{dN}{dt}\\) : births - deaths \\(\\pm\\) mgration \\(\\frac{dN}{dt} = bN - dN = (b-d)\\cdot N\\) , for \\(b, d>0\\) The solution to this is $N(t) = N_{0}e^{(b-d)t} $, with \\(N_{0} = N(0)\\) . This is a pretty good approximation for the population growth during the 18-20th century, but eventually populations need to compare for resources. Link to population predictions by UN: https://population.un.org/wpp/Graphs/Probabilistic/POP/TOT/ Logistic model \u00b6 [[Pierre Fran\u00e7ois Verhulst]] It includes a negative term, dependent on the population squared: \\( \\(\\frac{dN}{dt} = m N - n N^{2}\\) \\) This tends to saturation as the time grows: \\( \\(lim_{t\\to \\infty} N(t) = \\frac{m}{n}=k\\) \\) where \\(k\\) is also known as the carrying capacity of the system . An alternative way of writing this is \\( \\(\\frac{dN}{dt} = m N\\left(1 - \\frac{n}{m}N\\right) = rN\\left(1-\\frac{N}{k}\\right)\\) \\) For small \\(N\\) small, \\(\\dot{N} \\sim rN\\) . For large \\(N\\) , \\(N \\sim k\\) . Simple analysis \u00b6 \\( \\(\\frac{dN}{dt}= 0 = f(N) = rN\\left(\\frac{1-N}{k}\\right)= 0\\) \\) We have two equilibrim points, \\(N=0\\) , \\(N=k\\) . (photo) THis also has an analytical solution: \\( \\(N(t) = \\frac{kN_{0}}{N_{0}+(k-N_{0})e^{-rt}}, \\qquad r>0\\) \\) The limit behavior is: \\( \\(lim_{t\\to \\infty} N(t) = \\frac{kN_{0}}{N_{0}}=k \\qquad \\textrm{if } N_{0}\\neq 0\\) \\) \\( \\(N_{0}=0, \\quad N(t) = 0 \\qquad \\forall t\\geq0\\) \\) When we do a simulation with \\(N_{0}\\) small, we observe a sigmoid behavior: \\( \\(S(x) = \\frac{B}{1+e^{-\\frac{x-a}{\\mu}}}\\) \\) where \\(\\mu\\) is related to the slope of the sigmoid (see pictures) Logistic model with predator \u00b6 Example: a model for Spruce Budworm \u00b6 Most of the time, the population of this insect is under control. But every once in a while (10-15 years), it grows dramatically and destroys large parts of the forest. After the outbreaks, it goes back to original levels. Some birds eat this animals. When there's an outbreak, the birds are not able to keep the population of insects under control. Model \u00b6 Ludwig, Jones and Holling (1978) \\( \\(N(t) \\textrm{: population size of budworms}\\) \\) \\( \\(\\frac{dN}{dt} = r_BN\\left(1-\\frac{N}{k_{B}}\\right)- p(N)\\) \\) where now we account a predator species (birds): \\( \\(p(N) = \\frac{BN^{2}}{A^{2} + N^{2}}\\) \\) where \\(p(n)\\) models the mortality of budworms due to the predatory birds. This saturates at a value \\(B\\) . So, even if the population of budworm increases, then \\(p\\) is still the same. We call \\(B\\) the maximum predation rate. The parameter \\(A\\) is the value at which the predation rate is the half of its maximum. Adimensionalisation \u00b6 Change of variables \\( \\(u=\\frac{N}{a} \\quad r=\\frac{Ar_{b}}{B} \\quad q=\\frac{k_{B}}{A} \\quad \\tau=\\frac{Bt}{A}\\) \\) $$\\begin{align } \\frac{du}{d\\tau}&=\\frac{du}{dt}\\frac{dt}{d\\tau} = \\frac{1}{A}\\frac{dN}{dt} \\frac{A}{B} = \\frac{1}{B} \\frac{dN}{dt}\\ & =\\frac{1}{B}\\left[r_{B}A u\\left(1 - \\frac{Au}{k_{B}}\\right) - \\frac{Bu {2}A 2}{A {2}+u {2}A^{2}}\\right]\\ u' &= ru\\left(1-\\frac{u}{q}\\right) - \\frac{u {2}}{1+u {2}}= f(u)\\end{align } $$ Now we have a new parameter \\(q = \\frac{k_{B}}{A}\\) . If we consider \\(A\\) , \\(B\\) , \\(r_{B}\\) fixed, we can see that for low \\(q\\) , there's only one fixed point. If \\(q\\) increases, two more fixed points appear, one of them stable as well. However, if you have a small starting population, you won't reach it. If then you keep increasing \\(q\\) , you lose the first stable point, and we have a jump to \\(u^*_3\\) . Now the birds can no longercontrol the worms populations. But these worms kill the forest, so \\(q\\) will start decreasing until eventually \\(q\\) is so low that you are back to scenario 1. (Cusp bifurcation?) Example to play with Discrete models for a signle species dynamics \u00b6 \\[N_{k} \\textrm{ : population size at time } t_{k,}\\quad k\\in \\mathbb{N}$$ This is a sequence: $\\{N_k\\}_{k\\in\\mathbb{N}}$ and a quite general expression for them is $$N_{k}= N_{k-1} f(N_{k-1})\\] For instance, when \\(f(N_{k}) = r\\) , we have discrete Malthusian, \\(N_{k}= N_{k-1} r\\) . For \\(r>1\\) , \\(N_{k} \\to \\infty\\) for \\(k\\to\\infty\\) and for \\(r<1\\) , \\(N_{k}\\to0\\) . Dynamics of interacting populations. Types of species' interactions. \u00b6 Types of interactions \u00b6 Interespecific interactions : One population affects another species' populations. Name Mutualism + + Competition - - Predation or parasitism + - Commensalism + 0 Amensalism - 0 Neutralism 0 0 Kolmogorov systems \u00b6 \\( \\(\\begin{cases} x_{i} &\\textrm{ : density of species }i \\qquad i=1\\dots n \\\\ n & \\textrm{: \\# species} \\end{cases}\\) \\) The system are \\( \\(\\dot{x}_{i} = x_{i}f_{i}(\\vec{x})\\) \\) with \\(\\vec{x} = (x_{1}, x_{2}, \\dots, x_{n})\\) , and \\(f_{i}: \\mathbb{R}^{n} \\longrightarrow \\mathbb{R} \\quad \\mathcal{C}^1\\) which models the interaction with other species as well of self interaction. We know \\(x_{i}= 0\\) means that \\(\\dot{x}_{i}=0\\) , so that \\(x_{i}(t) = 0, \\quad \\forall t \\geq 0\\) . In other ords, the coordinate axes are invariant subspaces of the system. Lotka-Volterra \u00b6 \\( \\(\\frac{\\dot{x}_{i}}{x_{i}}= r_{i}+ \\sum\\limits_{j=1}^{n} a_{ij} x_{j}\\) \\) where \\(r_i\\) is the growth rate per individual of species \\(i\\) and \\(a_{ij}\\) is the interactions of species \\(j\\) over species \\(i\\) , so that: - \\(a_{ij}>0 \\quad j \\rightarrow i\\) , then + - \\(a_{ij}<0 \\quad j \\rightarrow i\\) , then - For the matrix, \\( \\(\\begin{pmatrix}a_{11} & a_{12} & \\dots & \\dots\\\\ \\dots & a_22 & \\dots & \\dots \\\\ \\vdots & & & \\vdots \\\\ a_{n1} & a_n2 & \\dots & a_{nn}\\end{pmatrix}\\) \\) the diagonal corresponds to the intraspecific interactions. As an example, we can take \\(n=2\\) : \\( \\(\\begin{align*} \\dot{x}_{1} &= r_{1}x_{1}\\frac{1-x_{1}}{k_{1}}- c_{1}x_{1}x_{2} \\quad c_{1}> 0\\\\ \\dot{x}_{2} &= r_{2}x_{2}\\frac{1-x_{2}}{k_{2}}- c_{2}x_{1}x_{2} \\quad c_{2}> 0 \\end{align*}\\) \\) The linear system will be: \\( \\(\\begin{pmatrix} \\frac{\\dot{x}_{1}}{x_{1}} \\\\ \\frac{\\dot{x}_{2}}{x_{2}} \\end{pmatrix} = \\begin{pmatrix} r_1 \\\\ r_{2}\\end{pmatrix} + \\begin{pmatrix} - \\frac{r_{1}}{k_{1}} & -c_{1} \\\\ -c_{2} & - r_{2}/k_{2}\\end{pmatrix}\\begin{pmatrix}x_{1} \\\\ x_{2}\\end{pmatrix}\\) \\) We will perform a change of variables: \\( \\(u = \\frac{x_{1}}{k_{1}} \\qquad v = \\frac{x_{2}}{k_{2}}\\qquad \\tau = r_{1}t\\) \\) so that: $$ \\frac{du}{dt}= \\frac{1}{k_{1}}\\frac{dx_{1}}{dt} \\qquad \\frac{dt}{d\\tau}=\\frac{1}{r_{1}}$$ and then \\( \\(\\begin{align*} {u}' &= \\frac{du}{d\\tau} = \\frac{du}{dt}\\frac{dt}{d\\tau} = \\frac{1}{k_{1}r_{1}}\\left(r_{1}uk_{1}\\left(\\frac{1-uk_{1}}{k_{1}}\\right)-c_{1}uk_{1}(vk_{2})\\right) = u\\left(\\frac{1-u-c_{1}k_{2}}{r_{1}}uv\\right) \\end{align*}\\) \\) and we define ... (ask for notes) Unit square \\([0, 1] \\times [0, 1]\\) is invariant by the dynamics: \\( \\(\\begin{align*} \\{u = 0\\} \\quad u'=0 \\quad \\{u = 0\\} \\textrm{ invariant}\\\\ \\{v = 0\\} \\quad v'=0 \\quad \\{v = 0\\} \\textrm{ invariant} \\end{align*}\\) \\) and \\( \\(\\begin{align*} \\{u = 1\\} \\quad &u'=- av < 0 \\quad &v>0\\\\ \\{v = 1\\} \\quad &v'=-pbw < 0 &u>0 \\end{align*}\\) \\) If I start in the unit square, I cannot leave: they will remain inside the unit square. From the [[Poincar\u00e9-Bendixson theorem]], we can apply it so that we know we only have fixed points or orbits (check slides). Nullclines \u00b6 u-nullclines : \\(u'=0 \\Rightarrow u(1-u-av) = 0\\) , so that we have: \\(u = 0\\) \\(u = 1-av\\) v-nullclines : \\(v'=0 \\Rightarrow pv(1-v-bu) = 0\\) , so that we have: \\(v = 0\\) \\(v = 1-bu\\) Equilibrium points of the system \u00b6 The equilibrium points of the system are when nullclines cross: \\( \\(\\begin{align*} u' = 0\\\\ v' = 0 \\end{align*}\\) \\) so \\( \\((u^{*}, v^{*}) = \\begin{cases} u = 1-av \\\\ v = 1-bu \\end{cases}\\) \\) This point falls inside the \\([0, 1] \\times [0, 1]\\) if \\( \\(0 \\leq u^{*} = \\frac{1-a}{1-ab} \\leq 1 \\qquad 0 \\leq v^{*} = \\frac{1-b}{1-ab} \\leq 1\\) \\) or, in other words if \\(a, b\\) are both \\(> 1\\) or \\(< 1\\) . Check picture for both cases. The equilibrium point is a point of coexistence for both species. Linear stability analysis \u00b6 \\(J(u, v) = DF(u, v) = \\begin{pmatrix}1-2u-av & -au \\\\ -bpv & \\rho(1-2v-bu)\\end{pmatrix}\\) - For the origin: \\( \\(J(0, 0) = \\begin{pmatrix}1 & 0 \\\\ 0 & \\rho\\end{pmatrix} \\Longrightarrow \\textrm{eigenvalues} \\begin{cases} \\lambda_{1}= 1 > 0 \\rightarrow e_{1}=(1,0)\\\\ \\lambda_{2}= \\rho > 0 \\rightarrow e_{1}=(0,1) \\end{cases}\\) \\) \\(\\Rightarrow (0, 0)\\) is a repulsor. If we have some individuals from one (or both species) the population will grow For the points in the axis: $$J(1, 0) = \\begin{pmatrix}-1 & -a \\ 0 & \\rho(1-b)\\end{pmatrix} \\Longrightarrow \\textrm{eigenvalues} \\begin{cases} \\lambda_{1}= -1 < 0 \\rightarrow e_{1}=(1,0)\\ \\lambda_{2}= \\rho(1-b)\\begin{cases} 0 \\textrm{ if } b < 1 \\ < 0 \\textrm{ if } b > 1 \\end{cases} \\rightarrow e_{2}=(0,1) \\end{cases} $$ \\(\\Rightarrow\\) (1, 0) is an attractor if \\(b>1\\) and a saddle point if \\(b<0\\) $$J(0, 1) = \\begin{pmatrix}1-a & 0 \\ -b\\rho & \\rho\\end{pmatrix} \\Longrightarrow \\textrm{eigenvalues} \\begin{cases} \\lambda_{1}= 1 -a &\\rightarrow e_{1}=(1,\\beta)\\ \\lambda_{2}= -\\rho &\\rightarrow e_{2}=(0,1) \\end{cases} $$ \\(\\lambda_{1} > 0\\) if \\(a<1\\) \\(\\Rightarrow\\) saddle point \\(\\lambda_{1} < 0\\) if \\(a>1\\) \\(\\Rightarrow\\) attractor Finally, for the point inside the unit square: \\( \\(J(u^{*}*, v^{*}) = \\begin{pmatrix} \\dots & \\dots \\\\\\dots & \\dots\\end{pmatrix} \\Longrightarrow \\begin{cases} a<1, \\; b<1 \\Rightarrow \\lambda_{1}, \\; \\lambda_{2} < 0 \\quad \\textrm{attractor} \\\\ a>1, \\; b>1 \\Rightarrow \\lambda_{1}\\cdot \\lambda_{2} < 0 \\quad \\textrm{saddle point} \\end{cases}\\) \\) The resulting system is a bistable system : it has two attractors. The orange line is the separatrix: it's the stable manifold of the saddle point and it separates the basin of attraction of each attractor. In the first situation ( \\(a, b< 1\\) ) both species can coexist. In the other situation, \\(a, b>1\\) , one species will lead to the others exclusion. This makes me think of competition for languages: [[Competition in languages]] Classical Lotka-Volterra for predator-prey \u00b6 \\( \\(\\begin{align*} N(t): &\\textrm{ population of preys}\\\\ P(t): &\\textrm{ population of predators} \\end{align*}\\) \\) \\( \\(\\begin{pmatrix} \\frac{\\dot{N}}{N} \\\\ \\frac{\\dot{P}}{P}\\end{pmatrix} = \\begin{pmatrix}a \\\\ -d\\end{pmatrix} + \\begin{pmatrix}0 & -b \\\\ c & 0\\end{pmatrix}\\begin{pmatrix}N \\\\ P\\end{pmatrix}\\) \\) Notice that in this model, the intraspecific interactions (the diagonal) are 0. And the preys lose from the interaction ( \\(-b\\) ) and the predators gain (c). In the absence of predators , the evolutions of preys is described by the Malthusian growth: \\( \\(\\dot{N} = aN \\quad N(t) = N_{0}e^{at}\\) \\) However, in the absence of preys , the predators go exstinct: \\( \\(\\dot{P} = dP \\quad P(t) = P_{0}e^{-dt}\\) \\) Invariant manifolds and equilibrium points \u00b6 \\(\\{N = 0\\}\\) and \\(\\{P = 0\\}\\) are invariant for the dynamics. Equilibrium points : ( \\(\\begin{cases} \\dot{N} = 0 \\quad N(a-bP) = 0 \\rightarrow\\begin{cases} N=0 \\\\ P = \\frac{a}{b}> 0 \\end{cases}\\\\ \\dot{P} = 0 \\quad N(a-bP) = 0 \\rightarrow\\begin{cases} N=0 \\end{cases} \\end{cases}\\) \\) Check picture to finish this Stability analysis \u00b6 \\[J(N, P) = \\begin{pmatrix}a-bP & -bN \\\\ cP & cN-d\\end{pmatrix}$$ For the **origin**: $$J(0, 0) = \\begin{pmatrix}a & 0 \\\\ 0 & -d\\end{pmatrix} \\Longrightarrow \\begin{cases} \\lambda_{1}= a> 0 \\\\ \\lambda_{2}= -d < 0 \\end{cases}\\] \u00c8lia The Lyapunov method \u00b6 [[The Lyapunov method]] Put this on a new section Check slides: The Lyapunov method If \\(\\dot{V} = 0\\) it means that \\(V(x(t)) = cnt\\) and if this happens we say that \\(V\\) is the First integral of the system For Lotka-Volterra, we rewrite: \\( \\(\\begin{align*} \\frac{1}{N}\\frac{dN}{dt}= a-bP\\\\ \\frac{1}{P}\\frac{dP}{dt}= cN -d \\end{align*}\\) \\) as \\( (\\begin{align*} \\frac{dN}{dt}\\left(-c + \\frac{d}{N}\\right) + \\frac{dP}{dt}\\left(\\frac{a}{P}-b\\right)= 0\\\\ \\frac{d}{dt}\\left(cN bP - d\\ln{N} - a\\ln{P}\\right) = 0 \\end{align*}\\) \\) > Check pictures So, \\(x_{0}\\) is lypapunov stable and is a center: the rest of the orbits are periodic around it. The place where I am is determined by the initial conditions. With a suitable change of variables, one can see that this is the [[Hamiltonian]] and that this is not only a local behavior, but also a global. To do so, you introduce the new canonical variables \\(p\\) and \\(q\\) . Then we can see that it's a Hamiltonian system. Since the transformation is valid for the whole system, then the behavior is valid for the whole system. The number of individuals form each population oscillates over time. Volterra's principle \u00b6 Check slide","title":"Population dynamics"},{"location":"Mathematical%20models%20in%20biology/Population%20dynamics%20lecture/#population-dynamics","text":"One dimensional models in ecology. A single species dynamics Continuous Models Discrete models Dynamics of interacting populations Types of species' interactions.","title":"Population dynamics"},{"location":"Mathematical%20models%20in%20biology/Population%20dynamics%20lecture/#one-dimensional-models-in-ecology","text":"","title":"One dimensional models in ecology:"},{"location":"Mathematical%20models%20in%20biology/Population%20dynamics%20lecture/#fibonacci-sequence","text":"First naive model. Idealised (biologically unrealistic) rabbit population.","title":"Fibonacci sequence"},{"location":"Mathematical%20models%20in%20biology/Population%20dynamics%20lecture/#model","text":"\\(F_n\\) : number of pairs of rabbits at month \\(n\\) \\(F_{0}\\) =1 (one male and one female) Rabbits are able to mat at one month old (at the end of its second month, a female can produce another pair) Rabbits never die and a mating pair always produces one new pair (one male, one female) every month. How many rabbits at month \\(n\\) ? \\(F_{n}\\) = # newborn pairs of rabbits + # adult pairs of adults = = # young at month n-1 + 2 (# adults at month \\(n-1\\) ) = \\((F_{n-1} - F_{n-2}) + 2(F_{n-1})\\) = \\(F_{n-1}+F_{n-2}\\) # newborn pairs of rabbits = # pairs that did not exist at the month \\(n-1\\) : \\(F_{n}- F_{n-1}\\) # aduult pairs of rabbits = # pairs thatexist at the month \\(n-1\\) : \\(F_{n-1}\\)","title":"Model"},{"location":"Mathematical%20models%20in%20biology/Population%20dynamics%20lecture/#malthusian-growth","text":"From [[Thomas Robert Malthus]] - \\(N(t)\\) : population size at time \\(t\\) - \\(\\frac{dN}{dt}\\) : births - deaths \\(\\pm\\) mgration \\(\\frac{dN}{dt} = bN - dN = (b-d)\\cdot N\\) , for \\(b, d>0\\) The solution to this is $N(t) = N_{0}e^{(b-d)t} $, with \\(N_{0} = N(0)\\) . This is a pretty good approximation for the population growth during the 18-20th century, but eventually populations need to compare for resources. Link to population predictions by UN: https://population.un.org/wpp/Graphs/Probabilistic/POP/TOT/","title":"Malthusian growth"},{"location":"Mathematical%20models%20in%20biology/Population%20dynamics%20lecture/#logistic-model","text":"[[Pierre Fran\u00e7ois Verhulst]] It includes a negative term, dependent on the population squared: \\( \\(\\frac{dN}{dt} = m N - n N^{2}\\) \\) This tends to saturation as the time grows: \\( \\(lim_{t\\to \\infty} N(t) = \\frac{m}{n}=k\\) \\) where \\(k\\) is also known as the carrying capacity of the system . An alternative way of writing this is \\( \\(\\frac{dN}{dt} = m N\\left(1 - \\frac{n}{m}N\\right) = rN\\left(1-\\frac{N}{k}\\right)\\) \\) For small \\(N\\) small, \\(\\dot{N} \\sim rN\\) . For large \\(N\\) , \\(N \\sim k\\) .","title":"Logistic model"},{"location":"Mathematical%20models%20in%20biology/Population%20dynamics%20lecture/#simple-analysis","text":"\\( \\(\\frac{dN}{dt}= 0 = f(N) = rN\\left(\\frac{1-N}{k}\\right)= 0\\) \\) We have two equilibrim points, \\(N=0\\) , \\(N=k\\) . (photo) THis also has an analytical solution: \\( \\(N(t) = \\frac{kN_{0}}{N_{0}+(k-N_{0})e^{-rt}}, \\qquad r>0\\) \\) The limit behavior is: \\( \\(lim_{t\\to \\infty} N(t) = \\frac{kN_{0}}{N_{0}}=k \\qquad \\textrm{if } N_{0}\\neq 0\\) \\) \\( \\(N_{0}=0, \\quad N(t) = 0 \\qquad \\forall t\\geq0\\) \\) When we do a simulation with \\(N_{0}\\) small, we observe a sigmoid behavior: \\( \\(S(x) = \\frac{B}{1+e^{-\\frac{x-a}{\\mu}}}\\) \\) where \\(\\mu\\) is related to the slope of the sigmoid (see pictures)","title":"Simple analysis"},{"location":"Mathematical%20models%20in%20biology/Population%20dynamics%20lecture/#logistic-model-with-predator","text":"","title":"Logistic model with predator"},{"location":"Mathematical%20models%20in%20biology/Population%20dynamics%20lecture/#example-a-model-for-spruce-budworm","text":"Most of the time, the population of this insect is under control. But every once in a while (10-15 years), it grows dramatically and destroys large parts of the forest. After the outbreaks, it goes back to original levels. Some birds eat this animals. When there's an outbreak, the birds are not able to keep the population of insects under control.","title":"Example: a model for Spruce Budworm"},{"location":"Mathematical%20models%20in%20biology/Population%20dynamics%20lecture/#model_1","text":"Ludwig, Jones and Holling (1978) \\( \\(N(t) \\textrm{: population size of budworms}\\) \\) \\( \\(\\frac{dN}{dt} = r_BN\\left(1-\\frac{N}{k_{B}}\\right)- p(N)\\) \\) where now we account a predator species (birds): \\( \\(p(N) = \\frac{BN^{2}}{A^{2} + N^{2}}\\) \\) where \\(p(n)\\) models the mortality of budworms due to the predatory birds. This saturates at a value \\(B\\) . So, even if the population of budworm increases, then \\(p\\) is still the same. We call \\(B\\) the maximum predation rate. The parameter \\(A\\) is the value at which the predation rate is the half of its maximum.","title":"Model"},{"location":"Mathematical%20models%20in%20biology/Population%20dynamics%20lecture/#adimensionalisation","text":"Change of variables \\( \\(u=\\frac{N}{a} \\quad r=\\frac{Ar_{b}}{B} \\quad q=\\frac{k_{B}}{A} \\quad \\tau=\\frac{Bt}{A}\\) \\) $$\\begin{align } \\frac{du}{d\\tau}&=\\frac{du}{dt}\\frac{dt}{d\\tau} = \\frac{1}{A}\\frac{dN}{dt} \\frac{A}{B} = \\frac{1}{B} \\frac{dN}{dt}\\ & =\\frac{1}{B}\\left[r_{B}A u\\left(1 - \\frac{Au}{k_{B}}\\right) - \\frac{Bu {2}A 2}{A {2}+u {2}A^{2}}\\right]\\ u' &= ru\\left(1-\\frac{u}{q}\\right) - \\frac{u {2}}{1+u {2}}= f(u)\\end{align } $$ Now we have a new parameter \\(q = \\frac{k_{B}}{A}\\) . If we consider \\(A\\) , \\(B\\) , \\(r_{B}\\) fixed, we can see that for low \\(q\\) , there's only one fixed point. If \\(q\\) increases, two more fixed points appear, one of them stable as well. However, if you have a small starting population, you won't reach it. If then you keep increasing \\(q\\) , you lose the first stable point, and we have a jump to \\(u^*_3\\) . Now the birds can no longercontrol the worms populations. But these worms kill the forest, so \\(q\\) will start decreasing until eventually \\(q\\) is so low that you are back to scenario 1. (Cusp bifurcation?) Example to play with","title":"Adimensionalisation"},{"location":"Mathematical%20models%20in%20biology/Population%20dynamics%20lecture/#discrete-models-for-a-signle-species-dynamics","text":"\\[N_{k} \\textrm{ : population size at time } t_{k,}\\quad k\\in \\mathbb{N}$$ This is a sequence: $\\{N_k\\}_{k\\in\\mathbb{N}}$ and a quite general expression for them is $$N_{k}= N_{k-1} f(N_{k-1})\\] For instance, when \\(f(N_{k}) = r\\) , we have discrete Malthusian, \\(N_{k}= N_{k-1} r\\) . For \\(r>1\\) , \\(N_{k} \\to \\infty\\) for \\(k\\to\\infty\\) and for \\(r<1\\) , \\(N_{k}\\to0\\) .","title":"Discrete models for a signle species dynamics"},{"location":"Mathematical%20models%20in%20biology/Population%20dynamics%20lecture/#dynamics-of-interacting-populations-types-of-species-interactions","text":"","title":"Dynamics of interacting populations. Types of species' interactions."},{"location":"Mathematical%20models%20in%20biology/Population%20dynamics%20lecture/#types-of-interactions","text":"Interespecific interactions : One population affects another species' populations. Name Mutualism + + Competition - - Predation or parasitism + - Commensalism + 0 Amensalism - 0 Neutralism 0 0","title":"Types of interactions"},{"location":"Mathematical%20models%20in%20biology/Population%20dynamics%20lecture/#kolmogorov-systems","text":"\\( \\(\\begin{cases} x_{i} &\\textrm{ : density of species }i \\qquad i=1\\dots n \\\\ n & \\textrm{: \\# species} \\end{cases}\\) \\) The system are \\( \\(\\dot{x}_{i} = x_{i}f_{i}(\\vec{x})\\) \\) with \\(\\vec{x} = (x_{1}, x_{2}, \\dots, x_{n})\\) , and \\(f_{i}: \\mathbb{R}^{n} \\longrightarrow \\mathbb{R} \\quad \\mathcal{C}^1\\) which models the interaction with other species as well of self interaction. We know \\(x_{i}= 0\\) means that \\(\\dot{x}_{i}=0\\) , so that \\(x_{i}(t) = 0, \\quad \\forall t \\geq 0\\) . In other ords, the coordinate axes are invariant subspaces of the system.","title":"Kolmogorov systems"},{"location":"Mathematical%20models%20in%20biology/Population%20dynamics%20lecture/#lotka-volterra","text":"\\( \\(\\frac{\\dot{x}_{i}}{x_{i}}= r_{i}+ \\sum\\limits_{j=1}^{n} a_{ij} x_{j}\\) \\) where \\(r_i\\) is the growth rate per individual of species \\(i\\) and \\(a_{ij}\\) is the interactions of species \\(j\\) over species \\(i\\) , so that: - \\(a_{ij}>0 \\quad j \\rightarrow i\\) , then + - \\(a_{ij}<0 \\quad j \\rightarrow i\\) , then - For the matrix, \\( \\(\\begin{pmatrix}a_{11} & a_{12} & \\dots & \\dots\\\\ \\dots & a_22 & \\dots & \\dots \\\\ \\vdots & & & \\vdots \\\\ a_{n1} & a_n2 & \\dots & a_{nn}\\end{pmatrix}\\) \\) the diagonal corresponds to the intraspecific interactions. As an example, we can take \\(n=2\\) : \\( \\(\\begin{align*} \\dot{x}_{1} &= r_{1}x_{1}\\frac{1-x_{1}}{k_{1}}- c_{1}x_{1}x_{2} \\quad c_{1}> 0\\\\ \\dot{x}_{2} &= r_{2}x_{2}\\frac{1-x_{2}}{k_{2}}- c_{2}x_{1}x_{2} \\quad c_{2}> 0 \\end{align*}\\) \\) The linear system will be: \\( \\(\\begin{pmatrix} \\frac{\\dot{x}_{1}}{x_{1}} \\\\ \\frac{\\dot{x}_{2}}{x_{2}} \\end{pmatrix} = \\begin{pmatrix} r_1 \\\\ r_{2}\\end{pmatrix} + \\begin{pmatrix} - \\frac{r_{1}}{k_{1}} & -c_{1} \\\\ -c_{2} & - r_{2}/k_{2}\\end{pmatrix}\\begin{pmatrix}x_{1} \\\\ x_{2}\\end{pmatrix}\\) \\) We will perform a change of variables: \\( \\(u = \\frac{x_{1}}{k_{1}} \\qquad v = \\frac{x_{2}}{k_{2}}\\qquad \\tau = r_{1}t\\) \\) so that: $$ \\frac{du}{dt}= \\frac{1}{k_{1}}\\frac{dx_{1}}{dt} \\qquad \\frac{dt}{d\\tau}=\\frac{1}{r_{1}}$$ and then \\( \\(\\begin{align*} {u}' &= \\frac{du}{d\\tau} = \\frac{du}{dt}\\frac{dt}{d\\tau} = \\frac{1}{k_{1}r_{1}}\\left(r_{1}uk_{1}\\left(\\frac{1-uk_{1}}{k_{1}}\\right)-c_{1}uk_{1}(vk_{2})\\right) = u\\left(\\frac{1-u-c_{1}k_{2}}{r_{1}}uv\\right) \\end{align*}\\) \\) and we define ... (ask for notes) Unit square \\([0, 1] \\times [0, 1]\\) is invariant by the dynamics: \\( \\(\\begin{align*} \\{u = 0\\} \\quad u'=0 \\quad \\{u = 0\\} \\textrm{ invariant}\\\\ \\{v = 0\\} \\quad v'=0 \\quad \\{v = 0\\} \\textrm{ invariant} \\end{align*}\\) \\) and \\( \\(\\begin{align*} \\{u = 1\\} \\quad &u'=- av < 0 \\quad &v>0\\\\ \\{v = 1\\} \\quad &v'=-pbw < 0 &u>0 \\end{align*}\\) \\) If I start in the unit square, I cannot leave: they will remain inside the unit square. From the [[Poincar\u00e9-Bendixson theorem]], we can apply it so that we know we only have fixed points or orbits (check slides).","title":"Lotka-Volterra"},{"location":"Mathematical%20models%20in%20biology/Population%20dynamics%20lecture/#nullclines","text":"u-nullclines : \\(u'=0 \\Rightarrow u(1-u-av) = 0\\) , so that we have: \\(u = 0\\) \\(u = 1-av\\) v-nullclines : \\(v'=0 \\Rightarrow pv(1-v-bu) = 0\\) , so that we have: \\(v = 0\\) \\(v = 1-bu\\)","title":"Nullclines"},{"location":"Mathematical%20models%20in%20biology/Population%20dynamics%20lecture/#equilibrium-points-of-the-system","text":"The equilibrium points of the system are when nullclines cross: \\( \\(\\begin{align*} u' = 0\\\\ v' = 0 \\end{align*}\\) \\) so \\( \\((u^{*}, v^{*}) = \\begin{cases} u = 1-av \\\\ v = 1-bu \\end{cases}\\) \\) This point falls inside the \\([0, 1] \\times [0, 1]\\) if \\( \\(0 \\leq u^{*} = \\frac{1-a}{1-ab} \\leq 1 \\qquad 0 \\leq v^{*} = \\frac{1-b}{1-ab} \\leq 1\\) \\) or, in other words if \\(a, b\\) are both \\(> 1\\) or \\(< 1\\) . Check picture for both cases. The equilibrium point is a point of coexistence for both species.","title":"Equilibrium points of the system"},{"location":"Mathematical%20models%20in%20biology/Population%20dynamics%20lecture/#linear-stability-analysis","text":"\\(J(u, v) = DF(u, v) = \\begin{pmatrix}1-2u-av & -au \\\\ -bpv & \\rho(1-2v-bu)\\end{pmatrix}\\) - For the origin: \\( \\(J(0, 0) = \\begin{pmatrix}1 & 0 \\\\ 0 & \\rho\\end{pmatrix} \\Longrightarrow \\textrm{eigenvalues} \\begin{cases} \\lambda_{1}= 1 > 0 \\rightarrow e_{1}=(1,0)\\\\ \\lambda_{2}= \\rho > 0 \\rightarrow e_{1}=(0,1) \\end{cases}\\) \\) \\(\\Rightarrow (0, 0)\\) is a repulsor. If we have some individuals from one (or both species) the population will grow For the points in the axis: $$J(1, 0) = \\begin{pmatrix}-1 & -a \\ 0 & \\rho(1-b)\\end{pmatrix} \\Longrightarrow \\textrm{eigenvalues} \\begin{cases} \\lambda_{1}= -1 < 0 \\rightarrow e_{1}=(1,0)\\ \\lambda_{2}= \\rho(1-b)\\begin{cases} 0 \\textrm{ if } b < 1 \\ < 0 \\textrm{ if } b > 1 \\end{cases} \\rightarrow e_{2}=(0,1) \\end{cases} $$ \\(\\Rightarrow\\) (1, 0) is an attractor if \\(b>1\\) and a saddle point if \\(b<0\\) $$J(0, 1) = \\begin{pmatrix}1-a & 0 \\ -b\\rho & \\rho\\end{pmatrix} \\Longrightarrow \\textrm{eigenvalues} \\begin{cases} \\lambda_{1}= 1 -a &\\rightarrow e_{1}=(1,\\beta)\\ \\lambda_{2}= -\\rho &\\rightarrow e_{2}=(0,1) \\end{cases} $$ \\(\\lambda_{1} > 0\\) if \\(a<1\\) \\(\\Rightarrow\\) saddle point \\(\\lambda_{1} < 0\\) if \\(a>1\\) \\(\\Rightarrow\\) attractor Finally, for the point inside the unit square: \\( \\(J(u^{*}*, v^{*}) = \\begin{pmatrix} \\dots & \\dots \\\\\\dots & \\dots\\end{pmatrix} \\Longrightarrow \\begin{cases} a<1, \\; b<1 \\Rightarrow \\lambda_{1}, \\; \\lambda_{2} < 0 \\quad \\textrm{attractor} \\\\ a>1, \\; b>1 \\Rightarrow \\lambda_{1}\\cdot \\lambda_{2} < 0 \\quad \\textrm{saddle point} \\end{cases}\\) \\) The resulting system is a bistable system : it has two attractors. The orange line is the separatrix: it's the stable manifold of the saddle point and it separates the basin of attraction of each attractor. In the first situation ( \\(a, b< 1\\) ) both species can coexist. In the other situation, \\(a, b>1\\) , one species will lead to the others exclusion. This makes me think of competition for languages: [[Competition in languages]]","title":"Linear stability analysis"},{"location":"Mathematical%20models%20in%20biology/Population%20dynamics%20lecture/#classical-lotka-volterra-for-predator-prey","text":"\\( \\(\\begin{align*} N(t): &\\textrm{ population of preys}\\\\ P(t): &\\textrm{ population of predators} \\end{align*}\\) \\) \\( \\(\\begin{pmatrix} \\frac{\\dot{N}}{N} \\\\ \\frac{\\dot{P}}{P}\\end{pmatrix} = \\begin{pmatrix}a \\\\ -d\\end{pmatrix} + \\begin{pmatrix}0 & -b \\\\ c & 0\\end{pmatrix}\\begin{pmatrix}N \\\\ P\\end{pmatrix}\\) \\) Notice that in this model, the intraspecific interactions (the diagonal) are 0. And the preys lose from the interaction ( \\(-b\\) ) and the predators gain (c). In the absence of predators , the evolutions of preys is described by the Malthusian growth: \\( \\(\\dot{N} = aN \\quad N(t) = N_{0}e^{at}\\) \\) However, in the absence of preys , the predators go exstinct: \\( \\(\\dot{P} = dP \\quad P(t) = P_{0}e^{-dt}\\) \\)","title":"Classical Lotka-Volterra for predator-prey"},{"location":"Mathematical%20models%20in%20biology/Population%20dynamics%20lecture/#invariant-manifolds-and-equilibrium-points","text":"\\(\\{N = 0\\}\\) and \\(\\{P = 0\\}\\) are invariant for the dynamics. Equilibrium points : ( \\(\\begin{cases} \\dot{N} = 0 \\quad N(a-bP) = 0 \\rightarrow\\begin{cases} N=0 \\\\ P = \\frac{a}{b}> 0 \\end{cases}\\\\ \\dot{P} = 0 \\quad N(a-bP) = 0 \\rightarrow\\begin{cases} N=0 \\end{cases} \\end{cases}\\) \\) Check picture to finish this","title":"Invariant manifolds and equilibrium points"},{"location":"Mathematical%20models%20in%20biology/Population%20dynamics%20lecture/#stability-analysis","text":"\\[J(N, P) = \\begin{pmatrix}a-bP & -bN \\\\ cP & cN-d\\end{pmatrix}$$ For the **origin**: $$J(0, 0) = \\begin{pmatrix}a & 0 \\\\ 0 & -d\\end{pmatrix} \\Longrightarrow \\begin{cases} \\lambda_{1}= a> 0 \\\\ \\lambda_{2}= -d < 0 \\end{cases}\\] \u00c8lia","title":"Stability analysis"},{"location":"Mathematical%20models%20in%20biology/Population%20dynamics%20lecture/#the-lyapunov-method","text":"[[The Lyapunov method]] Put this on a new section Check slides: The Lyapunov method If \\(\\dot{V} = 0\\) it means that \\(V(x(t)) = cnt\\) and if this happens we say that \\(V\\) is the First integral of the system For Lotka-Volterra, we rewrite: \\( \\(\\begin{align*} \\frac{1}{N}\\frac{dN}{dt}= a-bP\\\\ \\frac{1}{P}\\frac{dP}{dt}= cN -d \\end{align*}\\) \\) as \\( (\\begin{align*} \\frac{dN}{dt}\\left(-c + \\frac{d}{N}\\right) + \\frac{dP}{dt}\\left(\\frac{a}{P}-b\\right)= 0\\\\ \\frac{d}{dt}\\left(cN bP - d\\ln{N} - a\\ln{P}\\right) = 0 \\end{align*}\\) \\) > Check pictures So, \\(x_{0}\\) is lypapunov stable and is a center: the rest of the orbits are periodic around it. The place where I am is determined by the initial conditions. With a suitable change of variables, one can see that this is the [[Hamiltonian]] and that this is not only a local behavior, but also a global. To do so, you introduce the new canonical variables \\(p\\) and \\(q\\) . Then we can see that it's a Hamiltonian system. Since the transformation is valid for the whole system, then the behavior is valid for the whole system. The number of individuals form each population oscillates over time.","title":"The Lyapunov method"},{"location":"Mathematical%20models%20in%20biology/Population%20dynamics%20lecture/#volterras-principle","text":"Check slide","title":"Volterra's principle"},{"location":"Mathematical%20models%20in%20biology/Projects%20in%20Evolutionary%20models/","text":"Projects \u00b6 [[Basics on substitution models]] [[Evolutionary distances]] [[Continuous-time Markov models]] [[Models on phylogenetic trees]] [[Hidden Markov Models and the Forward algorithm]]","title":"Projects"},{"location":"Mathematical%20models%20in%20biology/Projects%20in%20Evolutionary%20models/#projects","text":"[[Basics on substitution models]] [[Evolutionary distances]] [[Continuous-time Markov models]] [[Models on phylogenetic trees]] [[Hidden Markov Models and the Forward algorithm]]","title":"Projects"},{"location":"Mathematical%20models%20in%20biology/Projects%20in%20Phylogenetic%20Reconstruction/","text":"Projects \u00b6 [[UPGMA]] [[Neighbour-Joining]] [[Maximum Parsimony]] [[Maximum Likelihood]] [[Quartet-Based methods]]","title":"Projects"},{"location":"Mathematical%20models%20in%20biology/Projects%20in%20Phylogenetic%20Reconstruction/#projects","text":"[[UPGMA]] [[Neighbour-Joining]] [[Maximum Parsimony]] [[Maximum Likelihood]] [[Quartet-Based methods]]","title":"Projects"},{"location":"Mathematical%20models%20in%20biology/Projects%20in%20Population%20Dynamics/","text":"Projects \u00b6 Introduction \u00b6 Each paper has a paper, where they discuss a problem coming from species, demographics, etc. They present the problem and then describe the mathematical model We need to identify what do the parameters and the variables mean (biological meaning) and then study the dynamics. What happens when you change a paramter, etc. We may also need to study some bifurcations. Do a bit more than what's done in the project","title":"Projects"},{"location":"Mathematical%20models%20in%20biology/Projects%20in%20Population%20Dynamics/#projects","text":"","title":"Projects"},{"location":"Mathematical%20models%20in%20biology/Projects%20in%20Population%20Dynamics/#introduction","text":"Each paper has a paper, where they discuss a problem coming from species, demographics, etc. They present the problem and then describe the mathematical model We need to identify what do the parameters and the variables mean (biological meaning) and then study the dynamics. What happens when you change a paramter, etc. We may also need to study some bifurcations. Do a bit more than what's done in the project","title":"Introduction"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/00.%20Introduction%20to%20the%20class%20to%20NMDS/","text":"Introduction \u00b6 Concepts \u00b6 The dynamical systems concept is a mathematical formalization for \"any given rule\" that describes the time dependence of a point's position in the [[Ambient Space|ambient space]], that is, dynamical systems provide a way of modelling events that change over time . So we try to model an event with a system of mathematical equations that can be used for prediction. What type of dynamical systems will be study? In theory, any problem. We will consider problems in mechanics, biology, economy, medicine, neuroscience, chemistry, celestial mechanics, astrodynamics. Typically, we'd like to know the analytical solution of the full set. But most times, we will not be able to find the analytic expression of the whole set of solutions. What we will do? 1. If we can use analytical tools, great 2. We can complement them with numerical methods. Goal \u00b6 Given a problem (model) and a system of mathematical equations, we want to find the whole set of solutions . 1. We would like to have an analytical expression of the solutions. 2. We will apply numerical methods. In particular, we will focus on systems of ordinary differential equations, ODEs . Contents of the course \u00b6 Numerical tools / methods to describe the behavior of the (some) solutions of a given dynamical system: equilibrium points, periodic orbits, stability, invariant manifolds, persistence of solutions, bifurcations Organization of the course \u00b6 Theoretical session (Room 102) Session at the PC4 room Grading system \u00b6 Class attendance is compulsory Assignments: 65% 2 short exams: 35% References \u00b6 Dynamical systems and numerical analysis. A.M. Stuart, A.R. Humphries Nonlinear dynamics and chaos . S. H. Strogatz An introduction to dynamical systems , D. K. Arrowsmith, C. M. Place Introduction to Hamiltonian dynamical systems and the N-body problem . K. Meyer, G. Hall and published papers Website \u00b6 https://mat-web.upc.edu/fme/nmds Atenea Consultation \u00b6 By sending an email merce.olle@upc.edu Software \u00b6 Use any software we want. We need to work with, at least, double presition. We need a numerical integrator to solve ODEs. It's not asked for us to write an integrator. We can take an integrator as a black box, like ode45 from Matlab.","title":"Introduction"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/00.%20Introduction%20to%20the%20class%20to%20NMDS/#introduction","text":"","title":"Introduction"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/00.%20Introduction%20to%20the%20class%20to%20NMDS/#concepts","text":"The dynamical systems concept is a mathematical formalization for \"any given rule\" that describes the time dependence of a point's position in the [[Ambient Space|ambient space]], that is, dynamical systems provide a way of modelling events that change over time . So we try to model an event with a system of mathematical equations that can be used for prediction. What type of dynamical systems will be study? In theory, any problem. We will consider problems in mechanics, biology, economy, medicine, neuroscience, chemistry, celestial mechanics, astrodynamics. Typically, we'd like to know the analytical solution of the full set. But most times, we will not be able to find the analytic expression of the whole set of solutions. What we will do? 1. If we can use analytical tools, great 2. We can complement them with numerical methods.","title":"Concepts"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/00.%20Introduction%20to%20the%20class%20to%20NMDS/#goal","text":"Given a problem (model) and a system of mathematical equations, we want to find the whole set of solutions . 1. We would like to have an analytical expression of the solutions. 2. We will apply numerical methods. In particular, we will focus on systems of ordinary differential equations, ODEs .","title":"Goal"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/00.%20Introduction%20to%20the%20class%20to%20NMDS/#contents-of-the-course","text":"Numerical tools / methods to describe the behavior of the (some) solutions of a given dynamical system: equilibrium points, periodic orbits, stability, invariant manifolds, persistence of solutions, bifurcations","title":"Contents of the course"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/00.%20Introduction%20to%20the%20class%20to%20NMDS/#organization-of-the-course","text":"Theoretical session (Room 102) Session at the PC4 room","title":"Organization of the course"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/00.%20Introduction%20to%20the%20class%20to%20NMDS/#grading-system","text":"Class attendance is compulsory Assignments: 65% 2 short exams: 35%","title":"Grading system"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/00.%20Introduction%20to%20the%20class%20to%20NMDS/#references","text":"Dynamical systems and numerical analysis. A.M. Stuart, A.R. Humphries Nonlinear dynamics and chaos . S. H. Strogatz An introduction to dynamical systems , D. K. Arrowsmith, C. M. Place Introduction to Hamiltonian dynamical systems and the N-body problem . K. Meyer, G. Hall and published papers","title":"References"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/00.%20Introduction%20to%20the%20class%20to%20NMDS/#website","text":"https://mat-web.upc.edu/fme/nmds Atenea","title":"Website"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/00.%20Introduction%20to%20the%20class%20to%20NMDS/#consultation","text":"By sending an email merce.olle@upc.edu","title":"Consultation"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/00.%20Introduction%20to%20the%20class%20to%20NMDS/#software","text":"Use any software we want. We need to work with, at least, double presition. We need a numerical integrator to solve ODEs. It's not asked for us to write an integrator. We can take an integrator as a black box, like ode45 from Matlab.","title":"Software"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/1.1.%20Preliminaries%20-%20Continuous%20dynamical%20systems/","text":"Preliminaries: Continuous dynamical systems \u00b6 We have an autonomous system of ODEs \\[ \\vec{x}' = \\vec{f} (\\vec{x}) = \\begin{cases} x_1' = f_1(x_1, \\dots, x_n) \\\\ x_n' = f_n(x_1, \\dots, x_n)\\end{cases}\\] We assume \\(\\vec{f}\\) smoth enough and we use \\(x = \\vec{x}\\) , \\(f = \\vec{f}\\) . The associated flow is \\(\\phi_t(x) = \\phi(t,x)\\) such that we have the following Cauchy problem \\(\\begin{cases}\\frac{d\\phi_t(x)}{dt} = f(\\phi_t(x)) \\\\ \\phi_0(x) = x \\end{cases}\\) The initial notation at \\(t=0\\) is \\(x\\) . We will use the following notation: $$ \\phi_t(t_{0,}x) = \\phi(t; t_{0,}x) = \\phi_{t0}(t, x)$$ Another notation: \\(x(t, t_{0,}x_0)\\) \\( \\(\\frac{dx(t; t_0,x_0)}{dt} = f(x(t; t_{0,}x_{0})\\) \\) \\( \\(x(t; t_{0}, x_{0}) = x_{0}\\) \\) Def: Equilibrium point \u00b6 An equilibrium point is a point \\(\\vec{e}\\) such that \\(\\vec{f}(\\vec{e}) = 0\\) . Then \\(\\phi_{t}(\\vec{e})=\\vec{e} \\qquad \\forall t\\) is a solution. Def: Periodic orbit \u00b6 \\(\\phi_{t}(x)\\) is T-periodic if \\(\\phi_{0}(x) = \\phi_{T}(x)= x\\) and \\(\\phi_{t}(x) \\neq x\\) for \\(0 < t< T\\) . Def: Invariant set \u00b6 \\(A \\subset \\mathcal{R}^n\\) is invariant for system if for any \\(x\\in A\\) , the solution passing through \\(x\\) belongs to \\(A\\) . Remark \u00b6 Consider a non autonomous system of ODE $$ \\vec{x}' = \\frac{d\\vec{x}}{dt} = g(t, \\vec{x}), \\quad t \\in I \\subset \\mathcal R, \\quad \\vec{x}\\in U \\subset \\mathcal R^{n}$$we may consider an autonomous system of ODE defining $$ \\vec{y} = (t, \\vec{x}) = (t, x_{1}, \\dots, x_{n}) = (y_{0}, y_{1}, \\dots, y_{n})$$ and $$ \\vec{y}' = \\vec{G}(\\vec{y}) \\quad \\textrm{or} \\quad \\begin{cases}y_{0}' = 1 \\ y_{1}' = g_{1}(t, x_{1}, \\dots, x_{n}) \\ \\dots \\ y_{n}' =g_{n(t}, x_{1}, \\dots, x_{n})\\end{cases}$$ Motivating examples \u00b6 Case n = 1 \u00b6 $$ x' = \\frac{dx}{dt} = f(x) \\quad x\\in \\mathcal R^{n}$$ If \\(n = 1\\) , \\(\\frac{dx}{dt}= f(x) \\leftrightarrow \\int{\\frac{dx}{f(x)}} = \\int{dt} = t + C\\) Approaches - 1st. To compute the analytical expression of all the solutions (not trivial): compute \\(\\int{\\frac{dx}{f(x)}}\\) . - 2nd . Qualitative or geometric behavior of all the solutions. 1. Compute the zeros of \\(f(x) = 0\\) . 2. Compute the sign of \\(f(x)\\) in the corresponding intervals. \\( \\(\\begin{cases}x' = x^{2}\\\\ x(0) = x_{0}\\end{cases}\\) \\) the analytical expression is \\( \\(x(t, x_0)= \\frac{x_{0}}{1-x_{0}t}\\) \\) Exercise \\(x' = sin(x)\\) . Case n = 2 \u00b6 \\[\\begin{pmatrix} x' \\\\ y' \\end{pmatrix} = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}\\begin{pmatrix} x \\\\ y \\end{pmatrix}\\] There are 14 different examples. A pendulum \u00b6 \\( \\(\\begin{cases} F = m a \\\\ ml \\frac{d^2\\theta}{dt^{2}}= - mg \\sin{\\theta}\\end{cases}\\) \\) We can write a system with \\(x = \\begin{pmatrix} \\theta' \\\\ \\theta' \\end{pmatrix} = \\begin{pmatrix} \\theta' \\\\ \\omega \\end{pmatrix}\\) . \\( \\(\\begin{cases} \\theta' = \\omega \\\\ \\omega '= - \\frac{g}{l} \\sin{\\theta}\\end{cases}\\) \\) This system has a first integral \\( \\(H(\\theta, \\omega) = \\frac{ml^{2}\\omega^{2}}{2} + mgl(1-\\cos{\\theta})\\) \\) (the energy) We need to check \\( \\(\\frac{dH}{dt}(\\theta(t), \\omega(t)) = \\frac{\\partial H}{\\partial\\theta} \\frac{d\\theta}{dt} + = \\frac{\\partial H}{\\partial\\omega} \\frac{d\\omega}{dt} = mgl\\omega\\sin{\\theta} + ml^{2}{\\omega(-\\frac{g}{l}\\sin{\\theta})} = 0\\) \\) We simply need to plot \\(H(\\theta, \\omega) = constant\\) , that is, the level curves of \\(H(\\theta, \\omega)\\) . ![[Pendulum_equations]] Phase diagram: ! If \\(E=0\\) , equilibrium point If \\(E \\in (0, 2mgl)\\) , periodic orbits If \\(E = 2mgl\\) , a Separatrix or heteroclinic orbit (check if it's exactly the same or not) If \\(E > 2mgl\\) , full rounds Predator-prey problem \u00b6 Examples: sharks-fish, lions-zebra, lynx-rabbits ... - \\(x(t)\\) : population of predators - \\(y(t)\\) : population of predators The system of ODEs for \\(a, b, \\alpha, \\beta > 0\\) . \\[\\begin{cases} x' = -bx + \\alpha xy = x(-b + \\alpha y) \\\\ y' = ay - \\beta xy = y(a - \\beta y) \\end{cases}$$ Equilibrium points: - Saddle $(0, 0)$ - Center $(\\frac{a}{\\beta}, \\frac{b}{\\alpha})$ First integral (you cannot know a priori, with mechanics is energy, but otherwise is unknown) $$V(x, y) = \\beta x - a \\ln{x} + \\alpha y - b \\ln{y} \\] Another model of predator-prey \u00b6 \\( \\(\\begin{cases}x' = -x + 3xy \\\\ y' = 3y(1-y) - xy -\\lambda(1-e^{5y})\\end{cases}\\) \\) \\(0 < \\lambda\\) paramter depending on eternal conditions (food supply, environment properties, ...). Goal : To know the equilibrium points. They depend on \\(\\lambda\\) and they're plotted in the slides. Bifurcations appear. Chaos \u00b6 Def : High sensitivity with respect to the initial conditions. 3D-Lorenz system \u00b6 \\( \\(\\begin{cases}\\frac{dx}{dt}= \\sigma (y-x) \\\\ \\frac{dy}{dt}= x(r -z) - y \\\\ \\frac{dz}{dt}= xy -bz \\end{cases}\\) \\) In the particular case of \\(\\sigma = 10\\) , \\(r = 28\\) , \\(b=8/3\\) . Double pendulum \u00b6 Wiki Celestial mechanics \u00b6 The branch of the astronomy (classical mechanics and maths) that deals with the motions in outer space in terms of the gravitational effects amont the bodies involved. The N-body problem \u00b6 \\(N\\) masses. The equations of motion are \\( \\(m_{i}\\frac{d^{2}\\vec{q}_{i}}{dt^{2}} = \\sum\\limits_{j = 1, j \\neq i}^{N} \\frac{Gm_{i}m_{j}(\\vec q_{j}- \\vec q_{i})}{||{\\vec q_{j} - \\vec q_{i}||}^{3}}\\quad\\textrm{for } i = 1, \\cdots, N\\) \\) For \\(N = 2\\) , we know everything: we can have elliptic, parabolic or hyperbolic orbits. The elliptic orbit is periodic. The parabolic arrives to infinity with velocity 0 and the hyperbolic arrives at infinity with positive velocity. With this simple case, you can explain - Planets around the Sun - Asteroids - Comets. For space missions, with more sophisticated trajectories, \\(n=2\\) is not enough. \\(N=3\\) \u00b6 You can consider earth, sun and spacecraft. Here we have no idea of the general solution. What can we do? Let's consider \\(n=3\\) with some particular conditions: restriced three-body problem (RTBP). The idea is: let's assume we have only 2 bodies (with known movement, since they can only be ciruclar, parbolic, hhyperbolic). For example, earth and moon. Now we add a third body, for example a spacecraft, which has an infinitessimal mass (small compared to the other ones, \\(m_{3}\\approx 0\\) ). The motion of \\(P_{3}\\) will depend on \\(P_1\\) and \\(P_2\\) , but their motion, won't depend on \\(P_1\\) . The \"big\" masses are called primaries . We have different possibilities: - Circular RTBP - Elliptic RTBP - Parabolic RTBP - Hyperbolic RTBP We can take a simplified model, where all the bodies are in the same plan. We can take new coordinates, \\(\\eta\\) and \\(\\xi\\) , such that they rotate together with the primaries ( \\(P_1\\) and \\(P_2\\) ). So \\(P_1\\) and \\(P_2\\) will remain fix at all time. In this case, we have a system of equations (to see in the future). We start by the simplest solutions: the equilibrium points. For this problem we have 5 equilibrium points and this has manny applications. (We will reduced masses to \\(u = \\frac{m_{2}}{m_{1}+m_{2}}\\) .) Genesis mission (2001 - 2004) \u00b6 The idea was to take the sun and earth as primaries. The idea was to send an spacecraft from Earth to the Halo orbits, collect some data, and then come back to the arth. Why do we follow specific trajectories? The answer lies on the 5 equilibrium points. You can create a trajectory so that you don't need any fuel. We follow the cheapest, the optimized one. What do you compute? 1. Equilibrium points 2. Periodic orbits (PO) 3. Invariant manifolds of the periodic orbit. This means the full set of orbits tending to this periodic orbit. They are going to tend asymptotically to the periodic orbit. It's not a realy problem but it provides a really good insight into the real one. Artemis-P1 spacecrafts orbit \u00b6 We take as primaries Earth and moon. We play with L1 and L2. You compute the invariant manifold of one orbit and the other and you compute the intersection In this case we have heteroclinic orbits ubt instead of starting/ending at equlibirium points, they do it with periodic orbits. Galaxies \u00b6 What happens when galaxies become closer? We can get bridges and tails with simulations as well. For some cases, \\(m_1\\) and \\(m_3\\) interchange mass. Microscopic problem \u00b6 Example : Atoms dynamics, Hidrogen atom. We can consider it from the mechanical space of view. Ionization \u00b6 Check slides Purpose of these slides \u00b6 Differente xamples/context Tools for a given system may be applied to any system Focus on celestial mechanics Equilibrium points, periodic orbits, invarinat manifolds, homoclinic, heteroclinic phenomena, ... Many interesting questions remain open","title":"Preliminaries: Continuous dynamical systems"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/1.1.%20Preliminaries%20-%20Continuous%20dynamical%20systems/#preliminaries-continuous-dynamical-systems","text":"We have an autonomous system of ODEs \\[ \\vec{x}' = \\vec{f} (\\vec{x}) = \\begin{cases} x_1' = f_1(x_1, \\dots, x_n) \\\\ x_n' = f_n(x_1, \\dots, x_n)\\end{cases}\\] We assume \\(\\vec{f}\\) smoth enough and we use \\(x = \\vec{x}\\) , \\(f = \\vec{f}\\) . The associated flow is \\(\\phi_t(x) = \\phi(t,x)\\) such that we have the following Cauchy problem \\(\\begin{cases}\\frac{d\\phi_t(x)}{dt} = f(\\phi_t(x)) \\\\ \\phi_0(x) = x \\end{cases}\\) The initial notation at \\(t=0\\) is \\(x\\) . We will use the following notation: $$ \\phi_t(t_{0,}x) = \\phi(t; t_{0,}x) = \\phi_{t0}(t, x)$$ Another notation: \\(x(t, t_{0,}x_0)\\) \\( \\(\\frac{dx(t; t_0,x_0)}{dt} = f(x(t; t_{0,}x_{0})\\) \\) \\( \\(x(t; t_{0}, x_{0}) = x_{0}\\) \\)","title":"Preliminaries: Continuous dynamical systems"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/1.1.%20Preliminaries%20-%20Continuous%20dynamical%20systems/#def-equilibrium-point","text":"An equilibrium point is a point \\(\\vec{e}\\) such that \\(\\vec{f}(\\vec{e}) = 0\\) . Then \\(\\phi_{t}(\\vec{e})=\\vec{e} \\qquad \\forall t\\) is a solution.","title":"Def: Equilibrium point"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/1.1.%20Preliminaries%20-%20Continuous%20dynamical%20systems/#def-periodic-orbit","text":"\\(\\phi_{t}(x)\\) is T-periodic if \\(\\phi_{0}(x) = \\phi_{T}(x)= x\\) and \\(\\phi_{t}(x) \\neq x\\) for \\(0 < t< T\\) .","title":"Def: Periodic orbit"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/1.1.%20Preliminaries%20-%20Continuous%20dynamical%20systems/#def-invariant-set","text":"\\(A \\subset \\mathcal{R}^n\\) is invariant for system if for any \\(x\\in A\\) , the solution passing through \\(x\\) belongs to \\(A\\) .","title":"Def: Invariant set"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/1.1.%20Preliminaries%20-%20Continuous%20dynamical%20systems/#remark","text":"Consider a non autonomous system of ODE $$ \\vec{x}' = \\frac{d\\vec{x}}{dt} = g(t, \\vec{x}), \\quad t \\in I \\subset \\mathcal R, \\quad \\vec{x}\\in U \\subset \\mathcal R^{n}$$we may consider an autonomous system of ODE defining $$ \\vec{y} = (t, \\vec{x}) = (t, x_{1}, \\dots, x_{n}) = (y_{0}, y_{1}, \\dots, y_{n})$$ and $$ \\vec{y}' = \\vec{G}(\\vec{y}) \\quad \\textrm{or} \\quad \\begin{cases}y_{0}' = 1 \\ y_{1}' = g_{1}(t, x_{1}, \\dots, x_{n}) \\ \\dots \\ y_{n}' =g_{n(t}, x_{1}, \\dots, x_{n})\\end{cases}$$","title":"Remark"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/1.1.%20Preliminaries%20-%20Continuous%20dynamical%20systems/#motivating-examples","text":"","title":"Motivating examples"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/1.1.%20Preliminaries%20-%20Continuous%20dynamical%20systems/#case-n-1","text":"$$ x' = \\frac{dx}{dt} = f(x) \\quad x\\in \\mathcal R^{n}$$ If \\(n = 1\\) , \\(\\frac{dx}{dt}= f(x) \\leftrightarrow \\int{\\frac{dx}{f(x)}} = \\int{dt} = t + C\\) Approaches - 1st. To compute the analytical expression of all the solutions (not trivial): compute \\(\\int{\\frac{dx}{f(x)}}\\) . - 2nd . Qualitative or geometric behavior of all the solutions. 1. Compute the zeros of \\(f(x) = 0\\) . 2. Compute the sign of \\(f(x)\\) in the corresponding intervals. \\( \\(\\begin{cases}x' = x^{2}\\\\ x(0) = x_{0}\\end{cases}\\) \\) the analytical expression is \\( \\(x(t, x_0)= \\frac{x_{0}}{1-x_{0}t}\\) \\) Exercise \\(x' = sin(x)\\) .","title":"Case n = 1"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/1.1.%20Preliminaries%20-%20Continuous%20dynamical%20systems/#case-n-2","text":"\\[\\begin{pmatrix} x' \\\\ y' \\end{pmatrix} = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}\\begin{pmatrix} x \\\\ y \\end{pmatrix}\\] There are 14 different examples.","title":"Case n = 2"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/1.1.%20Preliminaries%20-%20Continuous%20dynamical%20systems/#a-pendulum","text":"\\( \\(\\begin{cases} F = m a \\\\ ml \\frac{d^2\\theta}{dt^{2}}= - mg \\sin{\\theta}\\end{cases}\\) \\) We can write a system with \\(x = \\begin{pmatrix} \\theta' \\\\ \\theta' \\end{pmatrix} = \\begin{pmatrix} \\theta' \\\\ \\omega \\end{pmatrix}\\) . \\( \\(\\begin{cases} \\theta' = \\omega \\\\ \\omega '= - \\frac{g}{l} \\sin{\\theta}\\end{cases}\\) \\) This system has a first integral \\( \\(H(\\theta, \\omega) = \\frac{ml^{2}\\omega^{2}}{2} + mgl(1-\\cos{\\theta})\\) \\) (the energy) We need to check \\( \\(\\frac{dH}{dt}(\\theta(t), \\omega(t)) = \\frac{\\partial H}{\\partial\\theta} \\frac{d\\theta}{dt} + = \\frac{\\partial H}{\\partial\\omega} \\frac{d\\omega}{dt} = mgl\\omega\\sin{\\theta} + ml^{2}{\\omega(-\\frac{g}{l}\\sin{\\theta})} = 0\\) \\) We simply need to plot \\(H(\\theta, \\omega) = constant\\) , that is, the level curves of \\(H(\\theta, \\omega)\\) . ![[Pendulum_equations]] Phase diagram: ! If \\(E=0\\) , equilibrium point If \\(E \\in (0, 2mgl)\\) , periodic orbits If \\(E = 2mgl\\) , a Separatrix or heteroclinic orbit (check if it's exactly the same or not) If \\(E > 2mgl\\) , full rounds","title":"A pendulum"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/1.1.%20Preliminaries%20-%20Continuous%20dynamical%20systems/#predator-prey-problem","text":"Examples: sharks-fish, lions-zebra, lynx-rabbits ... - \\(x(t)\\) : population of predators - \\(y(t)\\) : population of predators The system of ODEs for \\(a, b, \\alpha, \\beta > 0\\) . \\[\\begin{cases} x' = -bx + \\alpha xy = x(-b + \\alpha y) \\\\ y' = ay - \\beta xy = y(a - \\beta y) \\end{cases}$$ Equilibrium points: - Saddle $(0, 0)$ - Center $(\\frac{a}{\\beta}, \\frac{b}{\\alpha})$ First integral (you cannot know a priori, with mechanics is energy, but otherwise is unknown) $$V(x, y) = \\beta x - a \\ln{x} + \\alpha y - b \\ln{y} \\]","title":"Predator-prey problem"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/1.1.%20Preliminaries%20-%20Continuous%20dynamical%20systems/#another-model-of-predator-prey","text":"\\( \\(\\begin{cases}x' = -x + 3xy \\\\ y' = 3y(1-y) - xy -\\lambda(1-e^{5y})\\end{cases}\\) \\) \\(0 < \\lambda\\) paramter depending on eternal conditions (food supply, environment properties, ...). Goal : To know the equilibrium points. They depend on \\(\\lambda\\) and they're plotted in the slides. Bifurcations appear.","title":"Another model of predator-prey"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/1.1.%20Preliminaries%20-%20Continuous%20dynamical%20systems/#chaos","text":"Def : High sensitivity with respect to the initial conditions.","title":"Chaos"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/1.1.%20Preliminaries%20-%20Continuous%20dynamical%20systems/#3d-lorenz-system","text":"\\( \\(\\begin{cases}\\frac{dx}{dt}= \\sigma (y-x) \\\\ \\frac{dy}{dt}= x(r -z) - y \\\\ \\frac{dz}{dt}= xy -bz \\end{cases}\\) \\) In the particular case of \\(\\sigma = 10\\) , \\(r = 28\\) , \\(b=8/3\\) .","title":"3D-Lorenz system"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/1.1.%20Preliminaries%20-%20Continuous%20dynamical%20systems/#double-pendulum","text":"Wiki","title":"Double pendulum"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/1.1.%20Preliminaries%20-%20Continuous%20dynamical%20systems/#celestial-mechanics","text":"The branch of the astronomy (classical mechanics and maths) that deals with the motions in outer space in terms of the gravitational effects amont the bodies involved.","title":"Celestial mechanics"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/1.1.%20Preliminaries%20-%20Continuous%20dynamical%20systems/#the-n-body-problem","text":"\\(N\\) masses. The equations of motion are \\( \\(m_{i}\\frac{d^{2}\\vec{q}_{i}}{dt^{2}} = \\sum\\limits_{j = 1, j \\neq i}^{N} \\frac{Gm_{i}m_{j}(\\vec q_{j}- \\vec q_{i})}{||{\\vec q_{j} - \\vec q_{i}||}^{3}}\\quad\\textrm{for } i = 1, \\cdots, N\\) \\) For \\(N = 2\\) , we know everything: we can have elliptic, parabolic or hyperbolic orbits. The elliptic orbit is periodic. The parabolic arrives to infinity with velocity 0 and the hyperbolic arrives at infinity with positive velocity. With this simple case, you can explain - Planets around the Sun - Asteroids - Comets. For space missions, with more sophisticated trajectories, \\(n=2\\) is not enough.","title":"The N-body problem"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/1.1.%20Preliminaries%20-%20Continuous%20dynamical%20systems/#n3","text":"You can consider earth, sun and spacecraft. Here we have no idea of the general solution. What can we do? Let's consider \\(n=3\\) with some particular conditions: restriced three-body problem (RTBP). The idea is: let's assume we have only 2 bodies (with known movement, since they can only be ciruclar, parbolic, hhyperbolic). For example, earth and moon. Now we add a third body, for example a spacecraft, which has an infinitessimal mass (small compared to the other ones, \\(m_{3}\\approx 0\\) ). The motion of \\(P_{3}\\) will depend on \\(P_1\\) and \\(P_2\\) , but their motion, won't depend on \\(P_1\\) . The \"big\" masses are called primaries . We have different possibilities: - Circular RTBP - Elliptic RTBP - Parabolic RTBP - Hyperbolic RTBP We can take a simplified model, where all the bodies are in the same plan. We can take new coordinates, \\(\\eta\\) and \\(\\xi\\) , such that they rotate together with the primaries ( \\(P_1\\) and \\(P_2\\) ). So \\(P_1\\) and \\(P_2\\) will remain fix at all time. In this case, we have a system of equations (to see in the future). We start by the simplest solutions: the equilibrium points. For this problem we have 5 equilibrium points and this has manny applications. (We will reduced masses to \\(u = \\frac{m_{2}}{m_{1}+m_{2}}\\) .)","title":"\\(N=3\\)"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/1.1.%20Preliminaries%20-%20Continuous%20dynamical%20systems/#genesis-mission-2001-2004","text":"The idea was to take the sun and earth as primaries. The idea was to send an spacecraft from Earth to the Halo orbits, collect some data, and then come back to the arth. Why do we follow specific trajectories? The answer lies on the 5 equilibrium points. You can create a trajectory so that you don't need any fuel. We follow the cheapest, the optimized one. What do you compute? 1. Equilibrium points 2. Periodic orbits (PO) 3. Invariant manifolds of the periodic orbit. This means the full set of orbits tending to this periodic orbit. They are going to tend asymptotically to the periodic orbit. It's not a realy problem but it provides a really good insight into the real one.","title":"Genesis mission (2001 - 2004)"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/1.1.%20Preliminaries%20-%20Continuous%20dynamical%20systems/#artemis-p1-spacecrafts-orbit","text":"We take as primaries Earth and moon. We play with L1 and L2. You compute the invariant manifold of one orbit and the other and you compute the intersection In this case we have heteroclinic orbits ubt instead of starting/ending at equlibirium points, they do it with periodic orbits.","title":"Artemis-P1 spacecrafts orbit"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/1.1.%20Preliminaries%20-%20Continuous%20dynamical%20systems/#galaxies","text":"What happens when galaxies become closer? We can get bridges and tails with simulations as well. For some cases, \\(m_1\\) and \\(m_3\\) interchange mass.","title":"Galaxies"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/1.1.%20Preliminaries%20-%20Continuous%20dynamical%20systems/#microscopic-problem","text":"Example : Atoms dynamics, Hidrogen atom. We can consider it from the mechanical space of view.","title":"Microscopic problem"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/1.1.%20Preliminaries%20-%20Continuous%20dynamical%20systems/#ionization","text":"Check slides","title":"Ionization"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/1.1.%20Preliminaries%20-%20Continuous%20dynamical%20systems/#purpose-of-these-slides","text":"Differente xamples/context Tools for a given system may be applied to any system Focus on celestial mechanics Equilibrium points, periodic orbits, invarinat manifolds, homoclinic, heteroclinic phenomena, ... Many interesting questions remain open","title":"Purpose of these slides"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/1.2.%20Preliminaries%20-%20Discrete%20dynamical%20systems/","text":"Discrete dynamical systems \u00b6 Definitions \u00b6 Discrete dynamical systems \u00b6 Discrete dynamical systems are defined by maps (or by [[dipheomorfisms]] if we want to use \\(F^{-1}\\) ). Reminder: We always consider \\(F\\) to be as smooth as it needs to be for our goals. \\( \\(\\begin{align} F: U \\subset &\\mathbb{R}^{n} \\longrightarrow \\mathbb{R}^{n}\\\\ &x \\longrightarrow F(x) \\end{align}\\) \\) We also have: \\( \\(\\begin{cases} x_{o}: \\textrm{given} \\\\ x_{n+1}=F(x_{n}) \\end{cases}\\) \\) Notation - \\(F^{0}(x)=x, \\quad F(x)=F(x), \\qquad F^{2}(x)=F(F(x))\\) - \\(F^{-1}=F^{-1}(x), \\quad F^{-2}(x)=F^{-1}(F^{-1}(x))\\) Related orbit \u00b6 Def: Given an initial condition, its related orbit is the set $$ {F^i(x)}_{i\\in \\mathbb{Z}}$$ Fixed point \u00b6 Def : A fixed point is a point \\(x\\) such that \\(F(x) = x\\) m-periodic point \u00b6 Def : An m-periodic point x is a point such that $$ \\begin{cases}F^{m}(x) = x \\ \\ F^{k}(x) \\neq x \\quad \\textrm{if } k<m \\end{cases}$$ the orbit is \\( \\(orbit = \\{x, F(x), F^2(x),\\dots,F^{m-1}(x)\\}\\) \\) Invariant set \u00b6 Def : A set \\(A \\subset \\mathbb{R}^n\\) is invariant if $$ F^{n}(x) \\in A, \\, \\forall n\\in \\mathbb{Z} \\,\\, \\textrm{and} \\,\\, x\\in A$$ Case \\(n = 1\\) \u00b6 $$ x \\longrightarrow f(x)$$ Representation of the orbit: - One possibility: vertical axis \\(x\\) and horizontal one \\(n\\) (see notes) - Another possibility: we have \\(x\\) , \\(y\\) . We plot \\(y = x\\) and \\(y = F(x)\\) and we mark \\(x_0\\) in the horizontal axis, draw a vertical line until it gets to the \\(y=f(x)\\) curve, then draw a horizontal line until it matches the \\(y=x\\) line and go back down. This is going to be \\(x_1\\) . Repeat. ![[Discrete_1d_dynsys_representation]] Example: Finance. Stock exchange market \u00b6 Assume we haev a stock with a price \\(P_{t}\\) and the model to provide the price at time \\(t+1\\) is given by \\( \\(P_{t+1}= a P_{t}- aP_{t}^{2} = a P_{t}(1- P_{t})\\) \\) where \\(a > 0\\) is a given parameter (related to the buying pressure) and we assume a normalized price, \\(P_{t}\\in [0, 1]\\) . We can write the model as $$ x \\longrightarrow f(x) = ax(1-x)$$ or \\( \\(\\begin{cases}x_0 \\\\ x_{n+1}= f(x_n) \\end{cases}\\) \\) Question : How does the evolution of the price ( \\(P_t\\) ) depend on \\(a\\) ? Let's do some simulations! - For example, take \\(a=0.8\\) and \\(P_{0}= 0.3\\) (see slides fig38a.eps or drawing). You will lose money, since for any \\(P_0\\) , the price tends to 0. - Now assume \\(a=2\\) . We have a fix point (around 0.6, where both curves meet). By iteration, a \"steady state\" or a \"fair value price\" of 0.6 is reached, i.e. prices converge to a single value ( \\(\\forall P_{0} \\in (0,1)\\) ) . ![[discrete_dynsys_finance]] - Assume \\(a = 3.2\\) . In this case, there are 2 possible fair prices and the system oscillates between them. If I take the fix point, I stay there, but if not, I'll tend to the periodic orbit. There exist a 2 periodic point. (Provar a casa) - Increasing \\(a\\) , there appear 4, 8, 16, 32... possible fair prices. (Plot with a bifurcation diagram, the typical one with chaos). It's a cascade bifurcation diagram . Conclusion : As far as \\(a\\) increases, the system cannot agree on what the fair prices are, so the prices fluctuates in a seemingly random, chaotic way. To solve a system of ODE: \\( \\(\\begin{cases}\\vec x' = \\vec f(t, \\vec x) \\\\ \\vec x(t_{0})= \\vec x_0 \\end{cases}\\) \\) The unique solution is \\(\\vec x(t)\\) . The goal is to get a numerical solution of the system Homework \u00b6 You need: - Integrator (as a black box). - Work with double precision. The absolute error and the relative error should be \\(1e-15\\) . - Matlab : e.g. ode45. - C : taylor ( info ) Typically: - input: \\(f\\) , \\(t_0\\) , \\(\\vec x_{0}=\\vec x(t_0)\\) , \\(t_{final}\\) and direction ( \\(1\\) forward in time and \\(-1\\) , backward). - output: \\((t_{final}, x(t_{final}))\\) and you can save (or not) the intermediate points Sometimes: - input \\(np\\) : number of points. You have \\(t_0\\) , \\(t_{final}\\) and \\(\\Delta t\\) - output: \\(x(t_{0}) = x_0\\) , \\(x(t_{1}) = x(t_{0}+\\Delta t)\\) ... The relative and absolute tolerance error will always be \\(10^{-15}\\) or \\(10^{-16}\\) . If professor wants \\(x(\\pi)\\) , \\(x(3.14)\\) is not acceptable. We need 16 digits (dobule precision)","title":"Discrete dynamical systems"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/1.2.%20Preliminaries%20-%20Discrete%20dynamical%20systems/#discrete-dynamical-systems","text":"","title":"Discrete dynamical systems"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/1.2.%20Preliminaries%20-%20Discrete%20dynamical%20systems/#definitions","text":"","title":"Definitions"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/1.2.%20Preliminaries%20-%20Discrete%20dynamical%20systems/#discrete-dynamical-systems_1","text":"Discrete dynamical systems are defined by maps (or by [[dipheomorfisms]] if we want to use \\(F^{-1}\\) ). Reminder: We always consider \\(F\\) to be as smooth as it needs to be for our goals. \\( \\(\\begin{align} F: U \\subset &\\mathbb{R}^{n} \\longrightarrow \\mathbb{R}^{n}\\\\ &x \\longrightarrow F(x) \\end{align}\\) \\) We also have: \\( \\(\\begin{cases} x_{o}: \\textrm{given} \\\\ x_{n+1}=F(x_{n}) \\end{cases}\\) \\) Notation - \\(F^{0}(x)=x, \\quad F(x)=F(x), \\qquad F^{2}(x)=F(F(x))\\) - \\(F^{-1}=F^{-1}(x), \\quad F^{-2}(x)=F^{-1}(F^{-1}(x))\\)","title":"Discrete dynamical systems"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/1.2.%20Preliminaries%20-%20Discrete%20dynamical%20systems/#related-orbit","text":"Def: Given an initial condition, its related orbit is the set $$ {F^i(x)}_{i\\in \\mathbb{Z}}$$","title":"Related orbit"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/1.2.%20Preliminaries%20-%20Discrete%20dynamical%20systems/#fixed-point","text":"Def : A fixed point is a point \\(x\\) such that \\(F(x) = x\\)","title":"Fixed point"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/1.2.%20Preliminaries%20-%20Discrete%20dynamical%20systems/#m-periodic-point","text":"Def : An m-periodic point x is a point such that $$ \\begin{cases}F^{m}(x) = x \\ \\ F^{k}(x) \\neq x \\quad \\textrm{if } k<m \\end{cases}$$ the orbit is \\( \\(orbit = \\{x, F(x), F^2(x),\\dots,F^{m-1}(x)\\}\\) \\)","title":"m-periodic point"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/1.2.%20Preliminaries%20-%20Discrete%20dynamical%20systems/#invariant-set","text":"Def : A set \\(A \\subset \\mathbb{R}^n\\) is invariant if $$ F^{n}(x) \\in A, \\, \\forall n\\in \\mathbb{Z} \\,\\, \\textrm{and} \\,\\, x\\in A$$","title":"Invariant set"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/1.2.%20Preliminaries%20-%20Discrete%20dynamical%20systems/#case-n-1","text":"$$ x \\longrightarrow f(x)$$ Representation of the orbit: - One possibility: vertical axis \\(x\\) and horizontal one \\(n\\) (see notes) - Another possibility: we have \\(x\\) , \\(y\\) . We plot \\(y = x\\) and \\(y = F(x)\\) and we mark \\(x_0\\) in the horizontal axis, draw a vertical line until it gets to the \\(y=f(x)\\) curve, then draw a horizontal line until it matches the \\(y=x\\) line and go back down. This is going to be \\(x_1\\) . Repeat. ![[Discrete_1d_dynsys_representation]]","title":"Case \\(n = 1\\)"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/1.2.%20Preliminaries%20-%20Discrete%20dynamical%20systems/#example-finance-stock-exchange-market","text":"Assume we haev a stock with a price \\(P_{t}\\) and the model to provide the price at time \\(t+1\\) is given by \\( \\(P_{t+1}= a P_{t}- aP_{t}^{2} = a P_{t}(1- P_{t})\\) \\) where \\(a > 0\\) is a given parameter (related to the buying pressure) and we assume a normalized price, \\(P_{t}\\in [0, 1]\\) . We can write the model as $$ x \\longrightarrow f(x) = ax(1-x)$$ or \\( \\(\\begin{cases}x_0 \\\\ x_{n+1}= f(x_n) \\end{cases}\\) \\) Question : How does the evolution of the price ( \\(P_t\\) ) depend on \\(a\\) ? Let's do some simulations! - For example, take \\(a=0.8\\) and \\(P_{0}= 0.3\\) (see slides fig38a.eps or drawing). You will lose money, since for any \\(P_0\\) , the price tends to 0. - Now assume \\(a=2\\) . We have a fix point (around 0.6, where both curves meet). By iteration, a \"steady state\" or a \"fair value price\" of 0.6 is reached, i.e. prices converge to a single value ( \\(\\forall P_{0} \\in (0,1)\\) ) . ![[discrete_dynsys_finance]] - Assume \\(a = 3.2\\) . In this case, there are 2 possible fair prices and the system oscillates between them. If I take the fix point, I stay there, but if not, I'll tend to the periodic orbit. There exist a 2 periodic point. (Provar a casa) - Increasing \\(a\\) , there appear 4, 8, 16, 32... possible fair prices. (Plot with a bifurcation diagram, the typical one with chaos). It's a cascade bifurcation diagram . Conclusion : As far as \\(a\\) increases, the system cannot agree on what the fair prices are, so the prices fluctuates in a seemingly random, chaotic way. To solve a system of ODE: \\( \\(\\begin{cases}\\vec x' = \\vec f(t, \\vec x) \\\\ \\vec x(t_{0})= \\vec x_0 \\end{cases}\\) \\) The unique solution is \\(\\vec x(t)\\) . The goal is to get a numerical solution of the system","title":"Example: Finance. Stock exchange market"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/1.2.%20Preliminaries%20-%20Discrete%20dynamical%20systems/#homework","text":"You need: - Integrator (as a black box). - Work with double precision. The absolute error and the relative error should be \\(1e-15\\) . - Matlab : e.g. ode45. - C : taylor ( info ) Typically: - input: \\(f\\) , \\(t_0\\) , \\(\\vec x_{0}=\\vec x(t_0)\\) , \\(t_{final}\\) and direction ( \\(1\\) forward in time and \\(-1\\) , backward). - output: \\((t_{final}, x(t_{final}))\\) and you can save (or not) the intermediate points Sometimes: - input \\(np\\) : number of points. You have \\(t_0\\) , \\(t_{final}\\) and \\(\\Delta t\\) - output: \\(x(t_{0}) = x_0\\) , \\(x(t_{1}) = x(t_{0}+\\Delta t)\\) ... The relative and absolute tolerance error will always be \\(10^{-15}\\) or \\(10^{-16}\\) . If professor wants \\(x(\\pi)\\) , \\(x(3.14)\\) is not acceptable. We need 16 digits (dobule precision)","title":"Homework"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/1.3.%20Variational%20equations/","text":"Variational equations \u00b6 They appear when computing periodic orbits, or determine their stability, or when looking at the dependence on the initial conditions. Consider a system of ODEs \\( \\(x' = f(x), \\quad x\\in U \\subset \\mathbb{R}^{n} \\qquad \\textrm{f is smoth}\\) \\) and assume \\(\\phi(t, x)\\) is the solution such that \\(\\phi(0, x) = x\\) and \\(\\frac{d}{dt}\\phi(t, x) = f(\\phi(t,x))\\) . Question : How do you know \\(\\phi(t, x+h)\\) for small \\(h\\) ? Can I know it exactly from the previous one? \"Yes\", in the following sense: Let's consider Taylor: \\( \\(\\phi(t, x+h) = \\phi(t, x) + \\frac{\\partial\\phi (t, x)}{\\partial x}h + \\frac{1}{2!}\\frac{\\partial^2\\phi (t, x)}{\\partial x^2} h^2\\) \\) To compute this, we need to know the partial derivatives. If \\(\\phi\\) is a vector, \\(\\frac{\\partial\\phi (t, x)}{\\partial x}\\) will be a matrix and multiplied by \\(h\\) will be a vector. The other term, \\(\\frac{\\partial^2\\phi (t, x)}{\\partial x^2} h^{2}\\) is going to be \\(h^{T}Mh\\) ... $$ \\frac{\\partial \\phi}{\\partial x} = \\begin{pmatrix} \\frac{\\partial \\phi_1}{\\partial x_{1}} & \\dots & \\frac{\\partial \\phi_1}{\\partial x_{n}}\\\\vdots & \\dots & \\vdots \\ \\frac{\\partial \\phi_n}{\\partial x_{1}} & \\dots & \\frac{\\partial \\phi_n}{\\partial x_{n}} \\end{pmatrix}$$ And \\(\\frac{\\partial^2 \\phi}{\\partial x^2}\\) is going to be a box, each slice it's the previous derivative matrix but with an extra derivative: \\[ \\frac{\\partial^2 \\phi}{\\partial x^2}_{first slice} = \\begin{pmatrix} \\frac{\\partial}{\\partial x_1}\\left(\\frac{\\partial \\phi_1}{\\partial x_{1}}\\right) & \\dots & \\frac{\\partial}{\\partial x_1}\\left(\\frac{\\partial \\phi_1}{\\partial x_{n}}\\right)\\\\\\vdots & \\dots & \\vdots \\\\ \\frac{\\partial}{\\partial x_1}\\left(\\frac{\\partial \\phi_n}{\\partial x_{1}}\\right) & \\dots & \\frac{\\partial}{\\partial x_1}\\left(\\frac{\\partial \\phi_n}{\\partial x_{n}}\\right) \\end{pmatrix}\\] Since \\(\\phi\\) is known, what we will need are the deravitves. How can we do that? One possibility : if \\(n = 2\\) ( exercise: re-write for any n ) \\[\\phi(t, x+h) = (\\phi_t(t,x+h), \\phi_2(t,x+h)), \\quad x = (x_1,x_{2})\\quad h = (h_1,h_2)$$ Take one component ($\\phi_1$ or $\\phi_2$): $$ \\begin{aligned}\\psi: A \\subset \\mathbb{R}^{3} &\\longrightarrow \\mathbb{R} \\\\ (t, x_1,x_{2}) &\\longrightarrow \\psi(t, x_{1}, x_{2})\\end{aligned}$$ Then, $$\\psi(t, x+h) = \\psi(t,x) + D\\psi(t, x)h + \\frac{1}{2!}D^{2}\\psi(t,x)h^{2} + \\frac{1}{3!}D^{3}\\psi(t,x)h^{3} + \\dots$$ with: $$\\begin{align*} D\\;\\psi(t,x)\\cdot h &= (h_{1}D_{1} + h_{2}D_{2})\\psi(t,x) + h_{1}D_{1}\\psi(t,x) + h_{2}D_{2}\\psi(t,x)\\\\ D^{2}\\psi(t, x)\\;\\cdot h^{2} &= (h_{1}D_{1}+ h_{2}D_{2})^2\\psi(t, x) = h_{1}^{2}D_{11}\\psi(t,x) + 2h_{1}h_{2}D_{12}\\psi(t,x) + h_{2}D_{22}\\psi(t,x)\\\\ D^{3}\\psi(t,x)\\; \\cdot h^{3} &= (h_{1}D_{1}+ h_{2}D_{2})^{3}\\psi(t,x)\\end{align*} \\] You would do taylor for each component. But we won't do this. Our idea is to compute the whole matrix in one step, the whole box in another step, etc. Another possibility (the one we will use) is to compute the whole matrices in one single step by solving a system of ODE's. Variational equations | definition \u00b6 In particular, the system of ODE satisfied by \\(\\frac{\\partial\\phi (t, x)}{\\partial x}\\) , \\(\\frac{\\partial^2 \\phi}{\\partial x^2}\\) , .. are called variational equations . Variational equations | derivation \u00b6 Let us see how to obtain them First order variational equations \u00b6 For \\(\\frac{\\partial\\phi (t, x)}{\\partial x}\\) , from \\( \\(\\frac{d}{dt}\\phi(t, x) = f(\\phi(t,x))\\) \\) we consider \\(\\frac{\\partial}{\\partial x}\\) (and recall that \\(x\\) , \\(t\\) are independent variables): $$ \\frac{\\partial}{\\partial x}\\frac{d}{dt}\\phi(t, x) = D f(\\phi(t,x)) \\frac{\\partial}{\\partial x} \\phi(t, x)$$ Eq: S1 Here \\(D\\) indicates derivative with respect \\(x\\) . Since \\(t\\) and \\(x\\) are independent, we can change the order of the left-hand side $$ \\frac{d}{dt}\\frac{\\partial}{\\partial x}\\phi(t, x) = D f(\\phi(t,x)) \\frac{\\partial}{\\partial x} \\phi(t, x)$$ We obtain a linear system of ODE (the variable to solve is what we want: \\(\\frac{\\partial}{\\partial x}\\phi(t, x)\\) ). We now need the initial conditions, which are also known: \\( \\(\\frac{\\partial \\phi(t, x)}{\\partial x}|_{t=0} = I\\) \\) (the identity matrix, since this \\(\\phi(0, x) = x\\) ). How do we proceed numerically? With our integrator we compute the solution \\(\\phi\\) , but at the same time, we add: \\[\\begin{aligned}\\begin{pmatrix}a_{11} & \\dots & a_{1n} \\\\ \\vdots & & \\vdots \\\\ a_{n1} & \\dots & a_{nn}\\end{pmatrix}' & = & \\begin{pmatrix}D_{x_1}f_1 & \\dots & D_{x_n}f_1 \\\\ \\vdots & & \\vdots \\\\ D_{x_1}f_n & \\dots & D_{x_n}f_n\\end{pmatrix} & \\begin{pmatrix}a_{11} & \\dots & a_{1n} \\\\ \\vdots & & \\vdots \\\\ a_{n1} & \\dots & a_{nn}\\end{pmatrix} \\\\ \\frac{d}{dt}\\frac{\\partial}{\\partial x}\\phi(t, x) & = & D f(\\phi(t,x)) & \\frac{\\partial}{\\partial x} \\phi(t, x) \\end{aligned}\\] So our ODE system will be \\( \\(\\begin{aligned} x_{1}' &=f_1(x_{1}, \\dots, x_{n}) \\\\ \\vdots \\\\ x_{n}' &=f_n(x_{1}, \\dots, x_{n}) \\\\ a_{11}' &= \\sum\\limits_{k=1}^{n}\\frac{\\partial f_1}{ \\partial x_{k}}(x_{1}, \\dots, x_{n}) a_{k1} \\\\ \\vdots \\\\ a_{1n}' &= \\sum\\limits_{k=1}^{n}\\frac{\\partial f_1}{ \\partial x_{k}}(x_{1}, \\dots, x_{n}) a_{kn} \\\\ a_{n1}' &= \\sum\\limits_{k=1}^{n}\\frac{\\partial f_n}{ \\partial x_{k}}(x_{1}, \\dots, x_{n}) a_{k1} \\\\ \\vdots \\\\ a_{nn}' &= \\sum\\limits_{k=1}^{n}\\frac{\\partial f_n}{ \\partial x_{k}}(x_{1}, \\dots, x_{n}) a_{kn} \\end{aligned}\\) \\) And with initial conditions: \\(x_1(0) = x_{10}, \\dots, x_{n}(0) = x_{n(0)}= x_{n0}\\) and \\( \\((a_{ij})|_{t=0} = I_{n\\times n}\\) \\) You implement a new sistem with \\(n+n^2\\) ODE variable. You can also use instead of \\(a_{11} \\rightarrow x_{n+1}\\) for convenience (when you have to use your integrator). Second order variational equations \u00b6 For \\(\\frac{\\partial^{2}\\phi(t, x)}{\\partial x^{2}}\\) . From Eq. S1: $$ \\frac{d}{dt} \\frac{\\partial^2 \\phi(t,x)}{\\partial x^2} = \\frac{\\partial}{\\partial x}\\frac{d}{dt}\\frac{\\partial\\phi(t, x)}{\\partial x} = D_{x} f(\\phi(t,x)) \\frac{\\partial^{2}\\phi(t, x)}{\\partial x^{2}} + D_{xx}f(\\phi(t,x))\\left(\\frac{\\partial\\phi(t, x)}{\\partial x}\\right)^2 $$ This is a nonhomogeneous linear system. In particular, every term of the box will be: \\[\\frac{d}{dt} \\frac{\\partial^{2} \\phi_i(t,x)}{\\partial x_{j}\\partial x_{k}} = \\sum\\limits_{p, q=1}^{n} D_{pq}f_i(\\phi(t,x)) \\frac{\\partial \\phi_p(t,x)}{\\partial x_{j}}\\frac{\\partial \\phi_q(t,x)}{\\partial x_{k}} + \\sum\\limits_{p=1}^{n} D_{p} f_{i}(\\phi(t,x))\\frac{\\partial^{2}\\phi_{p}(t,x)}{\\partial x_{j}\\partial x_{k}}\\] The initial conditions: \\( \\(\\frac{\\partial^{2}\\phi}{\\partial x^{2}(t,x)}|_{t=0} = 0\\) \\) since this is the time derivative of the previous initial conditions (which were the identity matrix, so a constant). We will need to integrate \\(n+n^2+n^3\\) ODE (variables)","title":"Variational equations"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/1.3.%20Variational%20equations/#variational-equations","text":"They appear when computing periodic orbits, or determine their stability, or when looking at the dependence on the initial conditions. Consider a system of ODEs \\( \\(x' = f(x), \\quad x\\in U \\subset \\mathbb{R}^{n} \\qquad \\textrm{f is smoth}\\) \\) and assume \\(\\phi(t, x)\\) is the solution such that \\(\\phi(0, x) = x\\) and \\(\\frac{d}{dt}\\phi(t, x) = f(\\phi(t,x))\\) . Question : How do you know \\(\\phi(t, x+h)\\) for small \\(h\\) ? Can I know it exactly from the previous one? \"Yes\", in the following sense: Let's consider Taylor: \\( \\(\\phi(t, x+h) = \\phi(t, x) + \\frac{\\partial\\phi (t, x)}{\\partial x}h + \\frac{1}{2!}\\frac{\\partial^2\\phi (t, x)}{\\partial x^2} h^2\\) \\) To compute this, we need to know the partial derivatives. If \\(\\phi\\) is a vector, \\(\\frac{\\partial\\phi (t, x)}{\\partial x}\\) will be a matrix and multiplied by \\(h\\) will be a vector. The other term, \\(\\frac{\\partial^2\\phi (t, x)}{\\partial x^2} h^{2}\\) is going to be \\(h^{T}Mh\\) ... $$ \\frac{\\partial \\phi}{\\partial x} = \\begin{pmatrix} \\frac{\\partial \\phi_1}{\\partial x_{1}} & \\dots & \\frac{\\partial \\phi_1}{\\partial x_{n}}\\\\vdots & \\dots & \\vdots \\ \\frac{\\partial \\phi_n}{\\partial x_{1}} & \\dots & \\frac{\\partial \\phi_n}{\\partial x_{n}} \\end{pmatrix}$$ And \\(\\frac{\\partial^2 \\phi}{\\partial x^2}\\) is going to be a box, each slice it's the previous derivative matrix but with an extra derivative: \\[ \\frac{\\partial^2 \\phi}{\\partial x^2}_{first slice} = \\begin{pmatrix} \\frac{\\partial}{\\partial x_1}\\left(\\frac{\\partial \\phi_1}{\\partial x_{1}}\\right) & \\dots & \\frac{\\partial}{\\partial x_1}\\left(\\frac{\\partial \\phi_1}{\\partial x_{n}}\\right)\\\\\\vdots & \\dots & \\vdots \\\\ \\frac{\\partial}{\\partial x_1}\\left(\\frac{\\partial \\phi_n}{\\partial x_{1}}\\right) & \\dots & \\frac{\\partial}{\\partial x_1}\\left(\\frac{\\partial \\phi_n}{\\partial x_{n}}\\right) \\end{pmatrix}\\] Since \\(\\phi\\) is known, what we will need are the deravitves. How can we do that? One possibility : if \\(n = 2\\) ( exercise: re-write for any n ) \\[\\phi(t, x+h) = (\\phi_t(t,x+h), \\phi_2(t,x+h)), \\quad x = (x_1,x_{2})\\quad h = (h_1,h_2)$$ Take one component ($\\phi_1$ or $\\phi_2$): $$ \\begin{aligned}\\psi: A \\subset \\mathbb{R}^{3} &\\longrightarrow \\mathbb{R} \\\\ (t, x_1,x_{2}) &\\longrightarrow \\psi(t, x_{1}, x_{2})\\end{aligned}$$ Then, $$\\psi(t, x+h) = \\psi(t,x) + D\\psi(t, x)h + \\frac{1}{2!}D^{2}\\psi(t,x)h^{2} + \\frac{1}{3!}D^{3}\\psi(t,x)h^{3} + \\dots$$ with: $$\\begin{align*} D\\;\\psi(t,x)\\cdot h &= (h_{1}D_{1} + h_{2}D_{2})\\psi(t,x) + h_{1}D_{1}\\psi(t,x) + h_{2}D_{2}\\psi(t,x)\\\\ D^{2}\\psi(t, x)\\;\\cdot h^{2} &= (h_{1}D_{1}+ h_{2}D_{2})^2\\psi(t, x) = h_{1}^{2}D_{11}\\psi(t,x) + 2h_{1}h_{2}D_{12}\\psi(t,x) + h_{2}D_{22}\\psi(t,x)\\\\ D^{3}\\psi(t,x)\\; \\cdot h^{3} &= (h_{1}D_{1}+ h_{2}D_{2})^{3}\\psi(t,x)\\end{align*} \\] You would do taylor for each component. But we won't do this. Our idea is to compute the whole matrix in one step, the whole box in another step, etc. Another possibility (the one we will use) is to compute the whole matrices in one single step by solving a system of ODE's.","title":"Variational equations"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/1.3.%20Variational%20equations/#variational-equations-definition","text":"In particular, the system of ODE satisfied by \\(\\frac{\\partial\\phi (t, x)}{\\partial x}\\) , \\(\\frac{\\partial^2 \\phi}{\\partial x^2}\\) , .. are called variational equations .","title":"Variational equations | definition"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/1.3.%20Variational%20equations/#variational-equations-derivation","text":"Let us see how to obtain them","title":"Variational equations | derivation"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/1.3.%20Variational%20equations/#first-order-variational-equations","text":"For \\(\\frac{\\partial\\phi (t, x)}{\\partial x}\\) , from \\( \\(\\frac{d}{dt}\\phi(t, x) = f(\\phi(t,x))\\) \\) we consider \\(\\frac{\\partial}{\\partial x}\\) (and recall that \\(x\\) , \\(t\\) are independent variables): $$ \\frac{\\partial}{\\partial x}\\frac{d}{dt}\\phi(t, x) = D f(\\phi(t,x)) \\frac{\\partial}{\\partial x} \\phi(t, x)$$ Eq: S1 Here \\(D\\) indicates derivative with respect \\(x\\) . Since \\(t\\) and \\(x\\) are independent, we can change the order of the left-hand side $$ \\frac{d}{dt}\\frac{\\partial}{\\partial x}\\phi(t, x) = D f(\\phi(t,x)) \\frac{\\partial}{\\partial x} \\phi(t, x)$$ We obtain a linear system of ODE (the variable to solve is what we want: \\(\\frac{\\partial}{\\partial x}\\phi(t, x)\\) ). We now need the initial conditions, which are also known: \\( \\(\\frac{\\partial \\phi(t, x)}{\\partial x}|_{t=0} = I\\) \\) (the identity matrix, since this \\(\\phi(0, x) = x\\) ). How do we proceed numerically? With our integrator we compute the solution \\(\\phi\\) , but at the same time, we add: \\[\\begin{aligned}\\begin{pmatrix}a_{11} & \\dots & a_{1n} \\\\ \\vdots & & \\vdots \\\\ a_{n1} & \\dots & a_{nn}\\end{pmatrix}' & = & \\begin{pmatrix}D_{x_1}f_1 & \\dots & D_{x_n}f_1 \\\\ \\vdots & & \\vdots \\\\ D_{x_1}f_n & \\dots & D_{x_n}f_n\\end{pmatrix} & \\begin{pmatrix}a_{11} & \\dots & a_{1n} \\\\ \\vdots & & \\vdots \\\\ a_{n1} & \\dots & a_{nn}\\end{pmatrix} \\\\ \\frac{d}{dt}\\frac{\\partial}{\\partial x}\\phi(t, x) & = & D f(\\phi(t,x)) & \\frac{\\partial}{\\partial x} \\phi(t, x) \\end{aligned}\\] So our ODE system will be \\( \\(\\begin{aligned} x_{1}' &=f_1(x_{1}, \\dots, x_{n}) \\\\ \\vdots \\\\ x_{n}' &=f_n(x_{1}, \\dots, x_{n}) \\\\ a_{11}' &= \\sum\\limits_{k=1}^{n}\\frac{\\partial f_1}{ \\partial x_{k}}(x_{1}, \\dots, x_{n}) a_{k1} \\\\ \\vdots \\\\ a_{1n}' &= \\sum\\limits_{k=1}^{n}\\frac{\\partial f_1}{ \\partial x_{k}}(x_{1}, \\dots, x_{n}) a_{kn} \\\\ a_{n1}' &= \\sum\\limits_{k=1}^{n}\\frac{\\partial f_n}{ \\partial x_{k}}(x_{1}, \\dots, x_{n}) a_{k1} \\\\ \\vdots \\\\ a_{nn}' &= \\sum\\limits_{k=1}^{n}\\frac{\\partial f_n}{ \\partial x_{k}}(x_{1}, \\dots, x_{n}) a_{kn} \\end{aligned}\\) \\) And with initial conditions: \\(x_1(0) = x_{10}, \\dots, x_{n}(0) = x_{n(0)}= x_{n0}\\) and \\( \\((a_{ij})|_{t=0} = I_{n\\times n}\\) \\) You implement a new sistem with \\(n+n^2\\) ODE variable. You can also use instead of \\(a_{11} \\rightarrow x_{n+1}\\) for convenience (when you have to use your integrator).","title":"First order variational equations"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/1.3.%20Variational%20equations/#second-order-variational-equations","text":"For \\(\\frac{\\partial^{2}\\phi(t, x)}{\\partial x^{2}}\\) . From Eq. S1: $$ \\frac{d}{dt} \\frac{\\partial^2 \\phi(t,x)}{\\partial x^2} = \\frac{\\partial}{\\partial x}\\frac{d}{dt}\\frac{\\partial\\phi(t, x)}{\\partial x} = D_{x} f(\\phi(t,x)) \\frac{\\partial^{2}\\phi(t, x)}{\\partial x^{2}} + D_{xx}f(\\phi(t,x))\\left(\\frac{\\partial\\phi(t, x)}{\\partial x}\\right)^2 $$ This is a nonhomogeneous linear system. In particular, every term of the box will be: \\[\\frac{d}{dt} \\frac{\\partial^{2} \\phi_i(t,x)}{\\partial x_{j}\\partial x_{k}} = \\sum\\limits_{p, q=1}^{n} D_{pq}f_i(\\phi(t,x)) \\frac{\\partial \\phi_p(t,x)}{\\partial x_{j}}\\frac{\\partial \\phi_q(t,x)}{\\partial x_{k}} + \\sum\\limits_{p=1}^{n} D_{p} f_{i}(\\phi(t,x))\\frac{\\partial^{2}\\phi_{p}(t,x)}{\\partial x_{j}\\partial x_{k}}\\] The initial conditions: \\( \\(\\frac{\\partial^{2}\\phi}{\\partial x^{2}(t,x)}|_{t=0} = 0\\) \\) since this is the time derivative of the previous initial conditions (which were the identity matrix, so a constant). We will need to integrate \\(n+n^2+n^3\\) ODE (variables)","title":"Second order variational equations"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/1.4.%20Poincar%C3%A9%20map/","text":"Poincar\u00e9 map \u00b6 Consider \\( \\(x' = f(x), \\quad x \\in U \\subset \\mathbb{R}^{n}\\) \\) (a continous dynamical system). A first method to obtain a discrete DS from the previous continuous one is by taking its time-T flow. This means, integrating and then checking the points every T times.![[cont_to_discrete1]] Another method, which is very convenient in some situations, is through a Poincar\u00e9 section . Poincar\u00e9 section | def \u00b6 Let \\(\\Sigma\\) be a hypersurface in \\(\\mathbb{R}^{n}\\) (typically given by \\(g(x) = 0\\) ) and assume also it is transversal to the vector field, that is the vector field is not tangent to \\(\\Sigma\\) at any point of \\(\\Sigma\\) . In other words, \\(\\forall x \\in \\Sigma\\) , \\(\\nabla g(x) \\not\\perp f(x)\\) , i.e., \\(<\\nabla g(x), f(x)> \\neq 0\\) . Example, imagine \\(n=3\\) . We consider \\(x \\in \\Sigma\\) , and \\(\\nabla g(x) = \\left( \\frac{\\partial g(x)}{\\partial x_{1}},\\dots, \\frac{\\partial g(x)}{\\partial x_{n}} \\right)\\) . Since \\(f(x)\\) defines the tangent space, \\(f(x)\\) and \\(\\nabla g(x)\\) cannot be perpendicular. ![[poincare_section1]] Poincar\u00e9 map | def \u00b6 Assume \\(x_0\\) given (may belong to \\(\\Sigma\\) or not) and assume \\(\\phi(T, x_{0})\\in \\Sigma\\) for \\(T_{0}>0\\) (and minimum with this property). For different \\(x_0\\) , we will need different \\(T_0\\) to obtain this property. Assume a neighbourhood of \\(x_0\\) , \\(U\\) , and a map, \\( \\(\\begin{aligned}\\tau: U &\\longrightarrow \\mathbb{R} \\\\ x &\\longrightarrow \\tau(x)\\end{aligned}\\) \\) known as time-return map such thatt \\(\\phi(\\tau(x), x) \\in \\Sigma\\) , \\(\\forall x \\in U\\) . ![[poincar\u00e9_map]] The map \\(P(x) = \\phi(\\tau(x), x)\\) is called Poincar\u00e9 map (or first return map). Remarks \u00b6 We may have the situation: \\(P: \\Sigma \\longrightarrow \\Sigma\\) . And you can analyze the solutions of the Poincar\u00e9 orbit thorugh this map. For example, if you want to compute a periodic orbit, you can compute a fixed point of the Poincar\u00e9 map.![[poincare_map2]] We may have the situation: \\(P: \\Sigma_{1} \\longrightarrow \\Sigma_{2}\\) so here we have some \\(g_{1}(x) = 0\\) defining \\(\\Sigma_{1}\\) and same for \\(g_2(x)\\) . Numerical computation of the Poincar\u00e9 map \u00b6 We have \\(x\\) (belonging to \\(\\Sigma\\) or not) and we want \\(P(x) = \\phi(\\tau(x), x)\\) , so we need to compute the necessary time \\(\\tau(x)\\) to reach \\(\\Sigma\\) (given by \\(g(x)\\) ). We want \\(0 = g(P(x)) = g(\\phi(\\tau(x), x))\\) , so we want to solve \\(G(t) = g(\\phi(t(x), x)) = 0\\) ( \\(t\\) and \\(\\tau\\) here are the same). We apply Newton's method: we start with \\(t^{0}\\) (which is not the initial time, it's the seed to start Newton's method) and we compute this sequence of values of \\(t\\) : \\( \\(t^{m+1} = t^{m} - \\frac{G(t^{m})}{G'(t^{m})}, \\quad m \\geq 0\\) \\) which will tend to \\(\\tau(x)\\) . \\(G\\) is known. And \\(G'\\) is the derivative of \\(G(t)\\) with respect to time: \\( \\(G'(t) = Dg(\\phi(t(x), x))\\frac{d\\phi}{dt} = Dg(\\phi(t(x), x))f(\\phi(t,x)) = <\\nabla g(\\phi(t(x), x)), f(\\phi(t(x), x))>\\) \\) So we have \\( \\(t^{m+1} = t^{m} - \\frac{G(t^{m})}{G'(t^{m})} = t^{m} - \\frac{G(t^{m})}{<\\nabla g(\\phi(t(x), x)), f(\\phi(t(x), x))>}\\) \\) ,with \\(\\Delta t = \\frac{G(t^{m})}{<\\nabla g(\\phi(t(x), x)), f(\\phi(t(x), x))>}\\) . Here is where we use that \\(\\Sigma\\) must not be transversal to the flow (otherwise it'd be 0). You compute the time evolution of \\(x\\) and compute \\(g_0\\) and \\(g_1\\) and it's product. It's positive (so both have the same sign), until it's not. Then, the moment the product is negative, you have the \\(t^0\\) and then you use the previous formula to get an increment (if you have passed slightly the surface, you'll get a negative increment). Then you have a new time, you compute the increment, etc. When are you at the right point? When \\(\\Delta t < tol\\) . With tolerance usually around \\(10^{-14}\\) . Remark: Differential of the Poincar\u00e9 map \u00b6 Let us compute the differential of the Poincar\u00e9 map. $$ \\begin{aligned}P(x) &= \\phi(\\tau(x), x) \\ DP(x) &= \\frac{d}{dt} \\phi(\\tau, x) D_{x}\\tau(x) + D_{x}\\phi(\\tau(x), x) = f(\\phi(\\tau, x)) D\\tau(x) + D \\phi(\\tau, x)\\end{aligned}$$ We know how to compute everything except for \\(D \\tau(x)\\) . Exercise : We need \\( \\(D \\tau(x) = \\left(\\frac{\\partial \\tau(x)}{\\partial x_{1}}, \\dots, \\frac{\\partial \\tau(x)}{\\partial x_{n}} \\right) = - \\frac{Dg(P(x))\\cdot D \\phi(\\tau(x), x)}{Dg(P(x))\\cdot f(P(x))}\\) \\) compute the steps inbetween. Finally, \\[DP(x)= - f(P(x))\\frac{Dg(P(x))\\cdot D \\phi(\\tau(x), x)}{Dg(P(x))\\cdot f(P(x))} + D\\phi(\\tau(x), x)\\] Poincar\u00e9 map and section \u00b6 \\(\\Sigma : g(x) = 0 \\quad x\\in U \\subset \\mathbb{R}^{n}\\) \\( \\(P(x) = \\phi(\\tau(x), x) \\in \\Sigma\\) \\) \\( \\(DP(x)= -f((x)) \\frac{Dg(P(x))\\cdot D\\phi(\\tau(x), x)}{Dg(P(x)) \\cdot f(P(x))} + D\\phi(\\tau(x), x)\\) \\) Example \u00b6 \\(g(x) = g(x_{1}, \\dots, x_{n}) = x_{n}\\) Then, \\( \\(Dg(x) = \\nabla g(x) = (0, \\dots, 0, 1)\\) \\) \\( \\(Dg(P(x))\\cdot f(P(x)) = (0, \\dots, 0, 1)\\cdot (f_{1}, \\dots, f_{n})|_{P(x)} = f_{n}(P(x))\\) \\) So then \\[\\begin{aligned} DP(x) &= \\frac{-1}{f_{n}(P(x))}\\begin{pmatrix}f_{1}\\\\ \\vdots \\\\ f_{n}\\end{pmatrix}\\cdot (0, \\dots, 0, 1) \\cdot \\begin{pmatrix}a_{11} & \\dots & a_{1n} \\\\ \\vdots & & \\vdots \\\\ a_{n1} & \\dots & a_{nn} + \\end{pmatrix} + \\begin{pmatrix}a_{11} & \\dots & a_{1n} \\\\ \\vdots & & \\vdots \\\\ a_{n1} & \\dots & a_{nn} + \\end{pmatrix} \\\\ & \\begin{pmatrix}a_{11} - \\frac{f_1}{f_{n}}a_{n1} & \\dots & a_{1n} - \\frac{f_1}{f_{n}}a_{nn} \\\\ \\vdots & & \\vdots \\\\ a_{n-1, 1} - - \\frac{f_{n-1}}{f_{n}}a_{n1} & \\dots & a_{n-1,n} - \\frac{f_{n-1}}{f_{n}}a_{nn} \\\\ 0 & \\dots & 0\\end{pmatrix}\\end{aligned}\\]","title":"Poincar\u00e9 map"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/1.4.%20Poincar%C3%A9%20map/#poincare-map","text":"Consider \\( \\(x' = f(x), \\quad x \\in U \\subset \\mathbb{R}^{n}\\) \\) (a continous dynamical system). A first method to obtain a discrete DS from the previous continuous one is by taking its time-T flow. This means, integrating and then checking the points every T times.![[cont_to_discrete1]] Another method, which is very convenient in some situations, is through a Poincar\u00e9 section .","title":"Poincar\u00e9 map"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/1.4.%20Poincar%C3%A9%20map/#poincare-section-def","text":"Let \\(\\Sigma\\) be a hypersurface in \\(\\mathbb{R}^{n}\\) (typically given by \\(g(x) = 0\\) ) and assume also it is transversal to the vector field, that is the vector field is not tangent to \\(\\Sigma\\) at any point of \\(\\Sigma\\) . In other words, \\(\\forall x \\in \\Sigma\\) , \\(\\nabla g(x) \\not\\perp f(x)\\) , i.e., \\(<\\nabla g(x), f(x)> \\neq 0\\) . Example, imagine \\(n=3\\) . We consider \\(x \\in \\Sigma\\) , and \\(\\nabla g(x) = \\left( \\frac{\\partial g(x)}{\\partial x_{1}},\\dots, \\frac{\\partial g(x)}{\\partial x_{n}} \\right)\\) . Since \\(f(x)\\) defines the tangent space, \\(f(x)\\) and \\(\\nabla g(x)\\) cannot be perpendicular. ![[poincare_section1]]","title":"Poincar\u00e9 section | def"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/1.4.%20Poincar%C3%A9%20map/#poincare-map-def","text":"Assume \\(x_0\\) given (may belong to \\(\\Sigma\\) or not) and assume \\(\\phi(T, x_{0})\\in \\Sigma\\) for \\(T_{0}>0\\) (and minimum with this property). For different \\(x_0\\) , we will need different \\(T_0\\) to obtain this property. Assume a neighbourhood of \\(x_0\\) , \\(U\\) , and a map, \\( \\(\\begin{aligned}\\tau: U &\\longrightarrow \\mathbb{R} \\\\ x &\\longrightarrow \\tau(x)\\end{aligned}\\) \\) known as time-return map such thatt \\(\\phi(\\tau(x), x) \\in \\Sigma\\) , \\(\\forall x \\in U\\) . ![[poincar\u00e9_map]] The map \\(P(x) = \\phi(\\tau(x), x)\\) is called Poincar\u00e9 map (or first return map).","title":"Poincar\u00e9 map | def"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/1.4.%20Poincar%C3%A9%20map/#remarks","text":"We may have the situation: \\(P: \\Sigma \\longrightarrow \\Sigma\\) . And you can analyze the solutions of the Poincar\u00e9 orbit thorugh this map. For example, if you want to compute a periodic orbit, you can compute a fixed point of the Poincar\u00e9 map.![[poincare_map2]] We may have the situation: \\(P: \\Sigma_{1} \\longrightarrow \\Sigma_{2}\\) so here we have some \\(g_{1}(x) = 0\\) defining \\(\\Sigma_{1}\\) and same for \\(g_2(x)\\) .","title":"Remarks"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/1.4.%20Poincar%C3%A9%20map/#numerical-computation-of-the-poincare-map","text":"We have \\(x\\) (belonging to \\(\\Sigma\\) or not) and we want \\(P(x) = \\phi(\\tau(x), x)\\) , so we need to compute the necessary time \\(\\tau(x)\\) to reach \\(\\Sigma\\) (given by \\(g(x)\\) ). We want \\(0 = g(P(x)) = g(\\phi(\\tau(x), x))\\) , so we want to solve \\(G(t) = g(\\phi(t(x), x)) = 0\\) ( \\(t\\) and \\(\\tau\\) here are the same). We apply Newton's method: we start with \\(t^{0}\\) (which is not the initial time, it's the seed to start Newton's method) and we compute this sequence of values of \\(t\\) : \\( \\(t^{m+1} = t^{m} - \\frac{G(t^{m})}{G'(t^{m})}, \\quad m \\geq 0\\) \\) which will tend to \\(\\tau(x)\\) . \\(G\\) is known. And \\(G'\\) is the derivative of \\(G(t)\\) with respect to time: \\( \\(G'(t) = Dg(\\phi(t(x), x))\\frac{d\\phi}{dt} = Dg(\\phi(t(x), x))f(\\phi(t,x)) = <\\nabla g(\\phi(t(x), x)), f(\\phi(t(x), x))>\\) \\) So we have \\( \\(t^{m+1} = t^{m} - \\frac{G(t^{m})}{G'(t^{m})} = t^{m} - \\frac{G(t^{m})}{<\\nabla g(\\phi(t(x), x)), f(\\phi(t(x), x))>}\\) \\) ,with \\(\\Delta t = \\frac{G(t^{m})}{<\\nabla g(\\phi(t(x), x)), f(\\phi(t(x), x))>}\\) . Here is where we use that \\(\\Sigma\\) must not be transversal to the flow (otherwise it'd be 0). You compute the time evolution of \\(x\\) and compute \\(g_0\\) and \\(g_1\\) and it's product. It's positive (so both have the same sign), until it's not. Then, the moment the product is negative, you have the \\(t^0\\) and then you use the previous formula to get an increment (if you have passed slightly the surface, you'll get a negative increment). Then you have a new time, you compute the increment, etc. When are you at the right point? When \\(\\Delta t < tol\\) . With tolerance usually around \\(10^{-14}\\) .","title":"Numerical computation of the Poincar\u00e9 map"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/1.4.%20Poincar%C3%A9%20map/#remark-differential-of-the-poincare-map","text":"Let us compute the differential of the Poincar\u00e9 map. $$ \\begin{aligned}P(x) &= \\phi(\\tau(x), x) \\ DP(x) &= \\frac{d}{dt} \\phi(\\tau, x) D_{x}\\tau(x) + D_{x}\\phi(\\tau(x), x) = f(\\phi(\\tau, x)) D\\tau(x) + D \\phi(\\tau, x)\\end{aligned}$$ We know how to compute everything except for \\(D \\tau(x)\\) . Exercise : We need \\( \\(D \\tau(x) = \\left(\\frac{\\partial \\tau(x)}{\\partial x_{1}}, \\dots, \\frac{\\partial \\tau(x)}{\\partial x_{n}} \\right) = - \\frac{Dg(P(x))\\cdot D \\phi(\\tau(x), x)}{Dg(P(x))\\cdot f(P(x))}\\) \\) compute the steps inbetween. Finally, \\[DP(x)= - f(P(x))\\frac{Dg(P(x))\\cdot D \\phi(\\tau(x), x)}{Dg(P(x))\\cdot f(P(x))} + D\\phi(\\tau(x), x)\\]","title":"Remark: Differential of the Poincar\u00e9 map"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/1.4.%20Poincar%C3%A9%20map/#poincare-map-and-section","text":"\\(\\Sigma : g(x) = 0 \\quad x\\in U \\subset \\mathbb{R}^{n}\\) \\( \\(P(x) = \\phi(\\tau(x), x) \\in \\Sigma\\) \\) \\( \\(DP(x)= -f((x)) \\frac{Dg(P(x))\\cdot D\\phi(\\tau(x), x)}{Dg(P(x)) \\cdot f(P(x))} + D\\phi(\\tau(x), x)\\) \\)","title":"Poincar\u00e9 map and section"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/1.4.%20Poincar%C3%A9%20map/#example","text":"\\(g(x) = g(x_{1}, \\dots, x_{n}) = x_{n}\\) Then, \\( \\(Dg(x) = \\nabla g(x) = (0, \\dots, 0, 1)\\) \\) \\( \\(Dg(P(x))\\cdot f(P(x)) = (0, \\dots, 0, 1)\\cdot (f_{1}, \\dots, f_{n})|_{P(x)} = f_{n}(P(x))\\) \\) So then \\[\\begin{aligned} DP(x) &= \\frac{-1}{f_{n}(P(x))}\\begin{pmatrix}f_{1}\\\\ \\vdots \\\\ f_{n}\\end{pmatrix}\\cdot (0, \\dots, 0, 1) \\cdot \\begin{pmatrix}a_{11} & \\dots & a_{1n} \\\\ \\vdots & & \\vdots \\\\ a_{n1} & \\dots & a_{nn} + \\end{pmatrix} + \\begin{pmatrix}a_{11} & \\dots & a_{1n} \\\\ \\vdots & & \\vdots \\\\ a_{n1} & \\dots & a_{nn} + \\end{pmatrix} \\\\ & \\begin{pmatrix}a_{11} - \\frac{f_1}{f_{n}}a_{n1} & \\dots & a_{1n} - \\frac{f_1}{f_{n}}a_{nn} \\\\ \\vdots & & \\vdots \\\\ a_{n-1, 1} - - \\frac{f_{n-1}}{f_{n}}a_{n1} & \\dots & a_{n-1,n} - \\frac{f_{n-1}}{f_{n}}a_{nn} \\\\ 0 & \\dots & 0\\end{pmatrix}\\end{aligned}\\]","title":"Example"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.%20Equilibrium%20points%20and%20stability/","text":"Chapter 2: Equilibrium points and stability \u00b6 Def: Stability \u00b6 We say that a solution \\(x(t; t_{0}, x_{0})\\) of \\(x' = f(t, x)\\) , \\(x\\in U\\subset \\mathbb{R}^{n}\\) , is stable if \\(\\forall \\epsilon > 0 \\quad \\exists \\delta(\\epsilon, t_0)>0\\) , such that for any solution \\(y(t; t_{0}, x_{1})\\) with \\(||x_{1}- x_{0}||<\\delta\\) is defined for \\([t_{0}, \\infty)\\) and \\(||x(t; t_{0}, x_{0}) - y(t; t_{0,}x_{1})||<\\epsilon\\) Everything should be in vector notation ![[ex_stable_solution]] You can draw a tube around an specific solution \\(x(t; t_{0}, x_{0})\\) so that any initial condition inside the tube, its solution remains inside this defined tube. If, moreover, \\(\\lim_{t \\to \\infty }{||x(t; t_{0}, x_{0} - y(t; t_{0}, x_{1})||} = 0\\) , we say that the solution \\(x(t; t_{0}, x_{0})\\) is asymptotically stable. The solution \\(x(t; t_{0}, x_{0})\\) is unstable if it is not stable. Def: Equilibrium point \u00b6 \\(x = p\\) for \\(x'=f(x)\\) , then \\(f(p)= 0\\) , that is \\(x(t) = p\\) is a solution for all time. Numerical computation of equilibrium points Given the system of ODEs, we must apply some method to solve \\(f(x) = 0\\) , using Netwon or whatever. Stability of an equilibrium point \u00b6 For \\(p\\) an equilibrim point of \\(x' = f(x)\\) , consider an initial condition \\(p+\\delta\\) , \\(\\delta\\) \"small\", the corresponding solution is $$\\begin{align } \\phi_{t}(p + \\delta) &= \\phi_{t}(p)+ D\\phi_{t}(p)\\delta + O(\\delta^{2})\\ & \\qquad p &\\end{align } $$ \\(D\\phi_t(p)\\) is the solution of the 1st order variational equation: \\( \\(\\begin{cases} \\frac{d}{dt}D\\phi_{t}(p) = Df(\\phi_{t}(p))D\\phi_{t}(p) \\\\ (D\\phi_{t}(p))|_{t=0} = I \\end{cases}\\) \\) where \\(Df(\\phi_{t}(p))\\) with \\(\\phi_{t}(p) = p\\) is a constant matrix. The solution is \\(D\\phi_{t}(p) = e^{At}\\) , so \\(\\phi_{t}(p+\\delta) = p + e^{At}\\delta + O(\\delta^2)\\) . The eigenvalues of \\(A\\) \"will decide\" the stability of \\(p\\) Def: Characteristic exponents \u00b6 The eigenvalues of \\(A = Df(p)\\) are called characteristic exponents Def: Hyperbolic matrix or equilibrium points \u00b6 If there is no eigenvalue with zero real part, then the matrix \\(A\\) is called hyperbolic or the equilibrim point is called hyperbolic Def: Elliptic matrix or equilibrium points \u00b6 If all eigenvalues are purely imaginary and the associated Jordan matrix is diagonal, then \\(A\\) or \\(p\\) is called elliptic . Theorem: Stability of equilibrim points in an autonomous linear system of ODE \u00b6 Consider \\( \\(x' = A(x-p)\\) \\) \\(A\\) a matrix ( \\(n\\times n\\) ), \\(x = p\\) is an equilibrium point. Taking \\(y=x-p\\) , the system becomes \\(y' = Ay\\) . (i) If all the eigenvalues of \\(A\\) satisfy that \\(Re(\\lambda)<0\\) , then \\(p\\) is asymptotically stable . (ii) If there exists eigenvalues with \\(Re(\\lambda) = 0\\) and they are semisimple (that is that the corresponding Jordan block is diagonal) and the remaining eigenvalues satisfy tht \\(Re(\\lambda) < 0\\) , then \\(p\\) is stable (but not asymptotically stable) (iii) In any other situation, \\(p\\) is unstable Examples \u00b6 \\(n = 2\\) . Exercise : There are exactly 14 different phase portratis Hyperbolic points example \u00b6 Assume \\(x' = A(x-p)\\) where the eigenvalues of \\(A\\) are \\(\\alpha > 0\\) ( \\(\\vec{v}^+\\) ) and \\(\\beta > 0\\) ( \\(\\vec{v}^-\\) ) We have \\(p\\) with the two eigenvectors. \\(\\vec{v}^+\\) defines 2 branches, wich corresponds to the invariant unstable 1d manifold ( \\(W^{u_{1}+}(p)\\) and \\(W^{u_{1}-}(p)\\) ). If we take a point in the semiline \\(W^{u_{1}+}(p)\\) , and we integrate backwards in time, we will tend to the equilibrium point. For \\(\\vec{v}^{-}\\) , we have an invariant stable 1d manifold with two branches \\(W^{s+}(p)\\) and \\(W^{s-}(p)\\) In total: 2 manifolds, 4 branches. ![[linear_manifolds]] These are hyperbolic equilibrium points Elliptic points example \u00b6 Assume \\(x' = A(x-p)\\) where the eigenvalues of \\(A\\) are purely imaginary \\(\\pm i\\omega\\) and eigenvectors \\(\\vec{v}^{\\pm} = \\vec{r} \\pm i\\vec{s}\\) Lemma \u00b6 The solution of \\( \\(\\begin{cases} \\vec{x}' &= A \\vec{x} \\\\ \\vec{x}(0) &= \\vec{x}_{0} = a_{0}\\vec{r} + b_{0}\\vec{s} \\end{cases}\\) \\) is \\(\\vec{x}(t) = a_{0}\\vec{x}_{1}(t) + b_{0}\\vec{x}_{2}(t)\\) , with \\( \\(\\begin{cases} \\vec{x}_{1}(t) = \\vec{r} \\cos{\\omega t} - \\vec{s} \\sin{\\omega t} \\\\ \\vec{x}_{2}(t) = \\vec{r} \\sin{\\omega t} + \\vec{s} \\cos{\\omega t} \\end{cases}\\) \\) The period of this solutions will be \\(\\frac{2\\pi}{w}\\) If I take another system similar to this one, it makes sense that if you have eigenvalues like the first case, it seems reasonable that the eigenvalues will be similar. But for the second case, when you perturbe it a bit, you may get another thing. This means that the case 1 is more robust. The hyperbolic euqilibrium points are very useful in the following sense. Theorem: Hartman \u00b6 Assume \\(x' = f(x)\\) , \\(p\\) is a hyperbolic equilibrium point and \\(f \\subset \\mathcal{C}^k\\) . Then the local flow around \\(p\\) is \"equivalent\" to the linear flow \\(x' = A(x-p)\\) , \\(A = Df(p)\\) around \\(x = p\\) . Example \u00b6 The spectrum of A \\(spec A = \\{\\lambda, \\beta\\}\\) (spectrum means the set of eigenvalues), with \\(\\lambda>0\\) , \\(\\beta < 0\\) . From a dynamical point of view, it's equivalent to have invariant lines ore invariant curves. This will be key to compute the real curves. Theorem: Stability of equilibrium points for a nonlinear system of ODE \u00b6 \\(x' = f(x)\\) , \\(x \\in U \\subset \\mathbf{R}^n\\) , \\(f\\subset \\mathcal{C}^k(U)\\) , and an equilibrium point, \\(f(p) = 0\\) and \\(A = Df(p)\\) (i) If all the eigenvalues of \\(A\\) satisfy \\(Re(\\lambda)<0\\) , then \\(p\\) is asympotatically stable for the nonlinear system. (ii) If there exists an eigenvalue such that \\(Re(\\lambda) > 0\\) , then \\(p\\) is unstable for the nonlinear system. For the other possibilities we don't know, we cannot conclude. Date: 11-10-2022 Examples \u00b6 Example 1 \\( \\(\\begin{cases} x' &= -y + ax(x^{2}+y^2) \\\\ \\\\ y' &= x + ay(x^{2}+ y^{2}) \\end{cases}\\) \\) For \\(p=(0, 0)\\) and equilibrium point, \\( \\(A = Df(x, y)|_{(0,0)} = \\begin{pmatrix}0 & -1 \\\\ 1 & 0\\end{pmatrix}\\) \\) The eigenvalues are: \\(\\det{(A - \\lambda \\mathbb{I})} = \\lambda^{2}+1 \\rightarrow \\lambda = \\pm i\\) We cannot apply the theorem for this case. For the linearized system , \\((0, 0)\\) is a center. For the nonlinear system, we will see that he orbits have the following shape depending on \\(a\\) : - For \\(a<0\\) , we have a stable point and the orbits are an inward spiral (towards the center). - For \\(a=0\\) , we have a center. - For \\(a>0\\) , we have an unstable point and the orbits are an outward spiral (from the center towards the outside).![[2. Equilibrium points and stability 2022-11-13 17.05.42.excalidraw]] Example 2: The pendulum (Assignment) \\( \\(\\begin{cases} \\frac{d\\theta}{dt} &= \\omega \\\\ \\frac{d\\omega}{dt} &= - \\frac{g}{l}\\sin\\theta \\end{cases}\\) \\) 1. Equilibrium points 2. Stability of the equilibrium points taking into account the linearized system. At some points (e.g. \\((\\pi, 0)\\) we can apply the theorem and at others like \\((0, 0)\\) we cannot) 3. Plot the phase portrait (orbits and direction) Hint : Use the first integral: \\(E = \\frac{ml^{2}\\omega^{2}}{2} + mgl(1-\\cos{\\theta})\\) 4. Energy level for the heteroclinic orbits (the ones that connect the unstable points in \\((-\\pi, 0)\\) and \\((\\pi, 0)\\) ) Example 3: Lotka-Volterra Model of competition between two species: \\[\\begin{cases} x' &= x(3-x-2y) \\\\ y' &= y(2-x-y) \\end{cases}$$ $$DF = \\begin{pmatrix}3-2x-2y & -2x \\\\-y & 2-x-2y\\end{pmatrix}\\] with - \\(x(t)\\) : population of rabbits - \\(y(t)\\) : population of sheep (so \\(x, y \\geq 0\\) ) Prove that the axes are invariant lines Compute the equilibrium points Classify them according to the linearized system (each point has a name, e.g. saddle, center, etc.) Stability of the equilibrium points (from the linearized system) Plot the local phase space around each equilibrium point So far, this can be done by hand Plot the global phase space in \\(x, y \\geq 0\\) Are there homoclinic or heteroclinic orbits. In particular, compute the heteroclinic orbit from \\((1, 1)\\) to \\((0, 0)\\) (we can do it with what's explained below) Describe shortly a biological interpretation of the results The 9-points can be done by hand, but specially for 8 is better with computer. Invariant manifolds \u00b6 It will play a key role from now to the end of this master's course. Ref: Meyer-Hall book Assume we have this non-linear system of ODE's: \\(x'=f(x)\\) , \\(x\\in U\\subset \\mathbf{R}^{n}\\) , \\(f\\) smooth, \\(p\\) is a hyperbolic equilibrium point (the Jacobian matrix at this equilibrium point has no eigenvalue with real part equal to 0). Local stable manifold \u00b6 Let \\(\\epsilon > 0\\) be given, then the local stable manifold is \\( \\(W^{s}_{loc}(\\varepsilon) = \\{x\\in \\mathbb{R}^{n} \\quad ||\\phi(t, x) - p|| < \\varepsilon, \\textrm{ for } t\\geq 0\\}\\) \\) Local unstable manifold \u00b6 Let \\(\\epsilon > 0\\) be given, then the local unstable manifold is \\( \\(W^{u}_{loc}(\\varepsilon) = \\{x\\in \\mathbb{R}^{n} \\quad ||\\phi(t, x) - p|| < \\varepsilon, \\textrm{ for } t\\leq 0\\}\\) \\) the set of points in \\(R^{n}\\) such that backwards in time was inside a ball surrounding \\(p\\) Theorem: Local stable/unstable manifolds for flows \u00b6 Let \\(x' = f(x)\\) , \\(p\\) and equilibrium point, \\(f\\) smooth and assume \\(A = Df(p)\\) . Assume \\(A\\) has - \\(d\\) eigenvalues with negative real parts (strictly negative) - \\(k\\) eigenvalues with positive real part and \\(E^{s}\\) , \\(E^{u}\\) are the corresponding linear subspaces It could have eigenvalues with real part equal to 0 Then for \\(\\varepsilon > 0\\) small enough, \\(W^{s}_{loc} (\\varepsilon)\\) , \\(W^{u}_{loc} (\\varepsilon)\\) are smooth manifolds of dimension \\(d\\) , \\(k\\) and tangent to \\(E^{s}\\) and \\(E^{u}\\) , respectively. If \\(x\\in W^{s}_{loc}(\\varepsilon)\\) , then \\(\\phi(t, x) \\xrightarrow[t \\to +\\infty]{}p\\) If \\(x\\in W^{u}_{loc}(\\varepsilon)\\) , then \\(\\phi(t, x) \\xrightarrow[t \\to -\\infty]{}p\\) Global stable/unstable invariant manifolds \u00b6 The global stable/unstable invariant manifolds are given by \\( \\(\\begin{align} W^{s}(p) &= \\{x\\in \\mathbb{R}^{n} \\quad \\phi(t, x) \\to p \\quad \\textrm{as } t\\to +\\infty \\} \\\\ W^{u}(p) &= \\{x\\in \\mathbb{R}^{n} \\quad \\phi(t, x) \\to p \\quad \\textrm{as } t\\to -\\infty \\} \\end{align}\\) \\) How do we do it numerically? - For the stable manifold, we take the local stable manifold and for each point we integrate it backward intime - For the unstable manifold, we take the local unstable manifold and for each point we integrate it forward intime Def: Homoclinic orbit \u00b6 Given \\(p\\) an equilibrim point, a homoclinic orbit of \\(p\\) is an orbit \\(\\phi(t, x)\\) such that \\( \\(lim_{t\\to \\pm \\infty} \\phi(t, x) = p\\) \\) or equivalently, \\(\\phi(t, x) \\in W^{s}(p) \\cap W^{u}(p)\\) . Def: Heteroclinic orbit \u00b6 Given \\(p\\) , \\(q\\) two equilibrim points, a heteroclinic orbit from \\(p\\) to \\(q\\) is an orbit \\(\\phi(t, x)\\) such that \\( \\(lim_{t\\to + \\infty} \\phi(t, x) = p \\qquad lim_{t\\to - \\infty} \\phi(t, x) = q\\) \\) or equivalently, \\(\\phi(t, x) \\in W^{u}(p) \\cap W^{s}(q)\\) . Numerical computation \u00b6 Question : How do we proceed to compute numerically \\(W^{s, u}(p)\\) ? Example 1: 1 dim manifold \u00b6 Given \\(x' = f(x), \\; x\\in U \\subset \\mathbb{R}^{n}\\) , \\(p\\) an equilibrium point s.t. \\( \\(A = Df(p) \\textrm{ has } \\begin{cases} \\lambda^{+}> 0, \\, v^{+} \\textrm{a unitary eigenvector} \\\\ \\\\ \\\\ \\lambda^{-}< 0, \\, v^{-} \\textrm{a unitary eigenvector} \\end{cases}\\) \\) Unstable manifold We know that the eigenvector of the positive eigenvalue is tangent to the unstable manifold. We will consider two branches: 1. For \\(W^{u, +}(p)\\) : 1. Take initial condition \\(q = p+s\\vec{v}^{+}\\) (s small) 2. Integrate forward in time 3. Check for a specific poincar\u00e9 map 4. Repeat with different \\(s\\) (so \\(s=1e-6, 1e-4, 1e-7\\) ) and check that the computation is robust (this is a possible way to check) For \\(W^{u, -}(p)\\) : 1. Take initial condition \\(q = p-s\\vec{v}^{+}\\) (s small) 2. Same as before... Stable manifold Same as before, we consider the two branches 1. For \\(W^{s, +}(p)\\) : 1. Take initial condition \\(q = p+s\\vec{v}^{-}\\) (s small) 2. Integrate backwards in time 3. Check robustness For \\(W^{u, +}(p)\\) : 1. Take initial condition \\(q = p-s\\vec{v}^{-}\\) (s small) 2. Same as before...","title":"Chapter 2: Equilibrium points and stability"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.%20Equilibrium%20points%20and%20stability/#chapter-2-equilibrium-points-and-stability","text":"","title":"Chapter 2: Equilibrium points and stability"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.%20Equilibrium%20points%20and%20stability/#def-stability","text":"We say that a solution \\(x(t; t_{0}, x_{0})\\) of \\(x' = f(t, x)\\) , \\(x\\in U\\subset \\mathbb{R}^{n}\\) , is stable if \\(\\forall \\epsilon > 0 \\quad \\exists \\delta(\\epsilon, t_0)>0\\) , such that for any solution \\(y(t; t_{0}, x_{1})\\) with \\(||x_{1}- x_{0}||<\\delta\\) is defined for \\([t_{0}, \\infty)\\) and \\(||x(t; t_{0}, x_{0}) - y(t; t_{0,}x_{1})||<\\epsilon\\) Everything should be in vector notation ![[ex_stable_solution]] You can draw a tube around an specific solution \\(x(t; t_{0}, x_{0})\\) so that any initial condition inside the tube, its solution remains inside this defined tube. If, moreover, \\(\\lim_{t \\to \\infty }{||x(t; t_{0}, x_{0} - y(t; t_{0}, x_{1})||} = 0\\) , we say that the solution \\(x(t; t_{0}, x_{0})\\) is asymptotically stable. The solution \\(x(t; t_{0}, x_{0})\\) is unstable if it is not stable.","title":"Def: Stability"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.%20Equilibrium%20points%20and%20stability/#def-equilibrium-point","text":"\\(x = p\\) for \\(x'=f(x)\\) , then \\(f(p)= 0\\) , that is \\(x(t) = p\\) is a solution for all time. Numerical computation of equilibrium points Given the system of ODEs, we must apply some method to solve \\(f(x) = 0\\) , using Netwon or whatever.","title":"Def: Equilibrium point"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.%20Equilibrium%20points%20and%20stability/#stability-of-an-equilibrium-point","text":"For \\(p\\) an equilibrim point of \\(x' = f(x)\\) , consider an initial condition \\(p+\\delta\\) , \\(\\delta\\) \"small\", the corresponding solution is $$\\begin{align } \\phi_{t}(p + \\delta) &= \\phi_{t}(p)+ D\\phi_{t}(p)\\delta + O(\\delta^{2})\\ & \\qquad p &\\end{align } $$ \\(D\\phi_t(p)\\) is the solution of the 1st order variational equation: \\( \\(\\begin{cases} \\frac{d}{dt}D\\phi_{t}(p) = Df(\\phi_{t}(p))D\\phi_{t}(p) \\\\ (D\\phi_{t}(p))|_{t=0} = I \\end{cases}\\) \\) where \\(Df(\\phi_{t}(p))\\) with \\(\\phi_{t}(p) = p\\) is a constant matrix. The solution is \\(D\\phi_{t}(p) = e^{At}\\) , so \\(\\phi_{t}(p+\\delta) = p + e^{At}\\delta + O(\\delta^2)\\) . The eigenvalues of \\(A\\) \"will decide\" the stability of \\(p\\)","title":"Stability of an equilibrium point"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.%20Equilibrium%20points%20and%20stability/#def-characteristic-exponents","text":"The eigenvalues of \\(A = Df(p)\\) are called characteristic exponents","title":"Def: Characteristic exponents"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.%20Equilibrium%20points%20and%20stability/#def-hyperbolic-matrix-or-equilibrium-points","text":"If there is no eigenvalue with zero real part, then the matrix \\(A\\) is called hyperbolic or the equilibrim point is called hyperbolic","title":"Def: Hyperbolic matrix or equilibrium points"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.%20Equilibrium%20points%20and%20stability/#def-elliptic-matrix-or-equilibrium-points","text":"If all eigenvalues are purely imaginary and the associated Jordan matrix is diagonal, then \\(A\\) or \\(p\\) is called elliptic .","title":"Def: Elliptic matrix or equilibrium points"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.%20Equilibrium%20points%20and%20stability/#theorem-stability-of-equilibrim-points-in-an-autonomous-linear-system-of-ode","text":"Consider \\( \\(x' = A(x-p)\\) \\) \\(A\\) a matrix ( \\(n\\times n\\) ), \\(x = p\\) is an equilibrium point. Taking \\(y=x-p\\) , the system becomes \\(y' = Ay\\) . (i) If all the eigenvalues of \\(A\\) satisfy that \\(Re(\\lambda)<0\\) , then \\(p\\) is asymptotically stable . (ii) If there exists eigenvalues with \\(Re(\\lambda) = 0\\) and they are semisimple (that is that the corresponding Jordan block is diagonal) and the remaining eigenvalues satisfy tht \\(Re(\\lambda) < 0\\) , then \\(p\\) is stable (but not asymptotically stable) (iii) In any other situation, \\(p\\) is unstable","title":"Theorem: Stability of equilibrim points in an autonomous linear system of ODE"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.%20Equilibrium%20points%20and%20stability/#examples","text":"\\(n = 2\\) . Exercise : There are exactly 14 different phase portratis","title":"Examples"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.%20Equilibrium%20points%20and%20stability/#hyperbolic-points-example","text":"Assume \\(x' = A(x-p)\\) where the eigenvalues of \\(A\\) are \\(\\alpha > 0\\) ( \\(\\vec{v}^+\\) ) and \\(\\beta > 0\\) ( \\(\\vec{v}^-\\) ) We have \\(p\\) with the two eigenvectors. \\(\\vec{v}^+\\) defines 2 branches, wich corresponds to the invariant unstable 1d manifold ( \\(W^{u_{1}+}(p)\\) and \\(W^{u_{1}-}(p)\\) ). If we take a point in the semiline \\(W^{u_{1}+}(p)\\) , and we integrate backwards in time, we will tend to the equilibrium point. For \\(\\vec{v}^{-}\\) , we have an invariant stable 1d manifold with two branches \\(W^{s+}(p)\\) and \\(W^{s-}(p)\\) In total: 2 manifolds, 4 branches. ![[linear_manifolds]] These are hyperbolic equilibrium points","title":"Hyperbolic points example"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.%20Equilibrium%20points%20and%20stability/#elliptic-points-example","text":"Assume \\(x' = A(x-p)\\) where the eigenvalues of \\(A\\) are purely imaginary \\(\\pm i\\omega\\) and eigenvectors \\(\\vec{v}^{\\pm} = \\vec{r} \\pm i\\vec{s}\\)","title":"Elliptic points example"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.%20Equilibrium%20points%20and%20stability/#lemma","text":"The solution of \\( \\(\\begin{cases} \\vec{x}' &= A \\vec{x} \\\\ \\vec{x}(0) &= \\vec{x}_{0} = a_{0}\\vec{r} + b_{0}\\vec{s} \\end{cases}\\) \\) is \\(\\vec{x}(t) = a_{0}\\vec{x}_{1}(t) + b_{0}\\vec{x}_{2}(t)\\) , with \\( \\(\\begin{cases} \\vec{x}_{1}(t) = \\vec{r} \\cos{\\omega t} - \\vec{s} \\sin{\\omega t} \\\\ \\vec{x}_{2}(t) = \\vec{r} \\sin{\\omega t} + \\vec{s} \\cos{\\omega t} \\end{cases}\\) \\) The period of this solutions will be \\(\\frac{2\\pi}{w}\\) If I take another system similar to this one, it makes sense that if you have eigenvalues like the first case, it seems reasonable that the eigenvalues will be similar. But for the second case, when you perturbe it a bit, you may get another thing. This means that the case 1 is more robust. The hyperbolic euqilibrium points are very useful in the following sense.","title":"Lemma"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.%20Equilibrium%20points%20and%20stability/#theorem-hartman","text":"Assume \\(x' = f(x)\\) , \\(p\\) is a hyperbolic equilibrium point and \\(f \\subset \\mathcal{C}^k\\) . Then the local flow around \\(p\\) is \"equivalent\" to the linear flow \\(x' = A(x-p)\\) , \\(A = Df(p)\\) around \\(x = p\\) .","title":"Theorem: Hartman"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.%20Equilibrium%20points%20and%20stability/#example","text":"The spectrum of A \\(spec A = \\{\\lambda, \\beta\\}\\) (spectrum means the set of eigenvalues), with \\(\\lambda>0\\) , \\(\\beta < 0\\) . From a dynamical point of view, it's equivalent to have invariant lines ore invariant curves. This will be key to compute the real curves.","title":"Example"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.%20Equilibrium%20points%20and%20stability/#theorem-stability-of-equilibrium-points-for-a-nonlinear-system-of-ode","text":"\\(x' = f(x)\\) , \\(x \\in U \\subset \\mathbf{R}^n\\) , \\(f\\subset \\mathcal{C}^k(U)\\) , and an equilibrium point, \\(f(p) = 0\\) and \\(A = Df(p)\\) (i) If all the eigenvalues of \\(A\\) satisfy \\(Re(\\lambda)<0\\) , then \\(p\\) is asympotatically stable for the nonlinear system. (ii) If there exists an eigenvalue such that \\(Re(\\lambda) > 0\\) , then \\(p\\) is unstable for the nonlinear system. For the other possibilities we don't know, we cannot conclude. Date: 11-10-2022","title":"Theorem: Stability of equilibrium points for a nonlinear system of ODE"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.%20Equilibrium%20points%20and%20stability/#examples_1","text":"Example 1 \\( \\(\\begin{cases} x' &= -y + ax(x^{2}+y^2) \\\\ \\\\ y' &= x + ay(x^{2}+ y^{2}) \\end{cases}\\) \\) For \\(p=(0, 0)\\) and equilibrium point, \\( \\(A = Df(x, y)|_{(0,0)} = \\begin{pmatrix}0 & -1 \\\\ 1 & 0\\end{pmatrix}\\) \\) The eigenvalues are: \\(\\det{(A - \\lambda \\mathbb{I})} = \\lambda^{2}+1 \\rightarrow \\lambda = \\pm i\\) We cannot apply the theorem for this case. For the linearized system , \\((0, 0)\\) is a center. For the nonlinear system, we will see that he orbits have the following shape depending on \\(a\\) : - For \\(a<0\\) , we have a stable point and the orbits are an inward spiral (towards the center). - For \\(a=0\\) , we have a center. - For \\(a>0\\) , we have an unstable point and the orbits are an outward spiral (from the center towards the outside).![[2. Equilibrium points and stability 2022-11-13 17.05.42.excalidraw]] Example 2: The pendulum (Assignment) \\( \\(\\begin{cases} \\frac{d\\theta}{dt} &= \\omega \\\\ \\frac{d\\omega}{dt} &= - \\frac{g}{l}\\sin\\theta \\end{cases}\\) \\) 1. Equilibrium points 2. Stability of the equilibrium points taking into account the linearized system. At some points (e.g. \\((\\pi, 0)\\) we can apply the theorem and at others like \\((0, 0)\\) we cannot) 3. Plot the phase portrait (orbits and direction) Hint : Use the first integral: \\(E = \\frac{ml^{2}\\omega^{2}}{2} + mgl(1-\\cos{\\theta})\\) 4. Energy level for the heteroclinic orbits (the ones that connect the unstable points in \\((-\\pi, 0)\\) and \\((\\pi, 0)\\) ) Example 3: Lotka-Volterra Model of competition between two species: \\[\\begin{cases} x' &= x(3-x-2y) \\\\ y' &= y(2-x-y) \\end{cases}$$ $$DF = \\begin{pmatrix}3-2x-2y & -2x \\\\-y & 2-x-2y\\end{pmatrix}\\] with - \\(x(t)\\) : population of rabbits - \\(y(t)\\) : population of sheep (so \\(x, y \\geq 0\\) ) Prove that the axes are invariant lines Compute the equilibrium points Classify them according to the linearized system (each point has a name, e.g. saddle, center, etc.) Stability of the equilibrium points (from the linearized system) Plot the local phase space around each equilibrium point So far, this can be done by hand Plot the global phase space in \\(x, y \\geq 0\\) Are there homoclinic or heteroclinic orbits. In particular, compute the heteroclinic orbit from \\((1, 1)\\) to \\((0, 0)\\) (we can do it with what's explained below) Describe shortly a biological interpretation of the results The 9-points can be done by hand, but specially for 8 is better with computer.","title":"Examples"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.%20Equilibrium%20points%20and%20stability/#invariant-manifolds","text":"It will play a key role from now to the end of this master's course. Ref: Meyer-Hall book Assume we have this non-linear system of ODE's: \\(x'=f(x)\\) , \\(x\\in U\\subset \\mathbf{R}^{n}\\) , \\(f\\) smooth, \\(p\\) is a hyperbolic equilibrium point (the Jacobian matrix at this equilibrium point has no eigenvalue with real part equal to 0).","title":"Invariant manifolds"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.%20Equilibrium%20points%20and%20stability/#local-stable-manifold","text":"Let \\(\\epsilon > 0\\) be given, then the local stable manifold is \\( \\(W^{s}_{loc}(\\varepsilon) = \\{x\\in \\mathbb{R}^{n} \\quad ||\\phi(t, x) - p|| < \\varepsilon, \\textrm{ for } t\\geq 0\\}\\) \\)","title":"Local stable manifold"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.%20Equilibrium%20points%20and%20stability/#local-unstable-manifold","text":"Let \\(\\epsilon > 0\\) be given, then the local unstable manifold is \\( \\(W^{u}_{loc}(\\varepsilon) = \\{x\\in \\mathbb{R}^{n} \\quad ||\\phi(t, x) - p|| < \\varepsilon, \\textrm{ for } t\\leq 0\\}\\) \\) the set of points in \\(R^{n}\\) such that backwards in time was inside a ball surrounding \\(p\\)","title":"Local unstable manifold"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.%20Equilibrium%20points%20and%20stability/#theorem-local-stableunstable-manifolds-for-flows","text":"Let \\(x' = f(x)\\) , \\(p\\) and equilibrium point, \\(f\\) smooth and assume \\(A = Df(p)\\) . Assume \\(A\\) has - \\(d\\) eigenvalues with negative real parts (strictly negative) - \\(k\\) eigenvalues with positive real part and \\(E^{s}\\) , \\(E^{u}\\) are the corresponding linear subspaces It could have eigenvalues with real part equal to 0 Then for \\(\\varepsilon > 0\\) small enough, \\(W^{s}_{loc} (\\varepsilon)\\) , \\(W^{u}_{loc} (\\varepsilon)\\) are smooth manifolds of dimension \\(d\\) , \\(k\\) and tangent to \\(E^{s}\\) and \\(E^{u}\\) , respectively. If \\(x\\in W^{s}_{loc}(\\varepsilon)\\) , then \\(\\phi(t, x) \\xrightarrow[t \\to +\\infty]{}p\\) If \\(x\\in W^{u}_{loc}(\\varepsilon)\\) , then \\(\\phi(t, x) \\xrightarrow[t \\to -\\infty]{}p\\)","title":"Theorem: Local stable/unstable manifolds for flows"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.%20Equilibrium%20points%20and%20stability/#global-stableunstable-invariant-manifolds","text":"The global stable/unstable invariant manifolds are given by \\( \\(\\begin{align} W^{s}(p) &= \\{x\\in \\mathbb{R}^{n} \\quad \\phi(t, x) \\to p \\quad \\textrm{as } t\\to +\\infty \\} \\\\ W^{u}(p) &= \\{x\\in \\mathbb{R}^{n} \\quad \\phi(t, x) \\to p \\quad \\textrm{as } t\\to -\\infty \\} \\end{align}\\) \\) How do we do it numerically? - For the stable manifold, we take the local stable manifold and for each point we integrate it backward intime - For the unstable manifold, we take the local unstable manifold and for each point we integrate it forward intime","title":"Global stable/unstable invariant manifolds"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.%20Equilibrium%20points%20and%20stability/#def-homoclinic-orbit","text":"Given \\(p\\) an equilibrim point, a homoclinic orbit of \\(p\\) is an orbit \\(\\phi(t, x)\\) such that \\( \\(lim_{t\\to \\pm \\infty} \\phi(t, x) = p\\) \\) or equivalently, \\(\\phi(t, x) \\in W^{s}(p) \\cap W^{u}(p)\\) .","title":"Def: Homoclinic orbit"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.%20Equilibrium%20points%20and%20stability/#def-heteroclinic-orbit","text":"Given \\(p\\) , \\(q\\) two equilibrim points, a heteroclinic orbit from \\(p\\) to \\(q\\) is an orbit \\(\\phi(t, x)\\) such that \\( \\(lim_{t\\to + \\infty} \\phi(t, x) = p \\qquad lim_{t\\to - \\infty} \\phi(t, x) = q\\) \\) or equivalently, \\(\\phi(t, x) \\in W^{u}(p) \\cap W^{s}(q)\\) .","title":"Def: Heteroclinic orbit"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.%20Equilibrium%20points%20and%20stability/#numerical-computation","text":"Question : How do we proceed to compute numerically \\(W^{s, u}(p)\\) ?","title":"Numerical computation"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.%20Equilibrium%20points%20and%20stability/#example-1-1-dim-manifold","text":"Given \\(x' = f(x), \\; x\\in U \\subset \\mathbb{R}^{n}\\) , \\(p\\) an equilibrium point s.t. \\( \\(A = Df(p) \\textrm{ has } \\begin{cases} \\lambda^{+}> 0, \\, v^{+} \\textrm{a unitary eigenvector} \\\\ \\\\ \\\\ \\lambda^{-}< 0, \\, v^{-} \\textrm{a unitary eigenvector} \\end{cases}\\) \\) Unstable manifold We know that the eigenvector of the positive eigenvalue is tangent to the unstable manifold. We will consider two branches: 1. For \\(W^{u, +}(p)\\) : 1. Take initial condition \\(q = p+s\\vec{v}^{+}\\) (s small) 2. Integrate forward in time 3. Check for a specific poincar\u00e9 map 4. Repeat with different \\(s\\) (so \\(s=1e-6, 1e-4, 1e-7\\) ) and check that the computation is robust (this is a possible way to check) For \\(W^{u, -}(p)\\) : 1. Take initial condition \\(q = p-s\\vec{v}^{+}\\) (s small) 2. Same as before... Stable manifold Same as before, we consider the two branches 1. For \\(W^{s, +}(p)\\) : 1. Take initial condition \\(q = p+s\\vec{v}^{-}\\) (s small) 2. Integrate backwards in time 3. Check robustness For \\(W^{u, +}(p)\\) : 1. Take initial condition \\(q = p-s\\vec{v}^{-}\\) (s small) 2. Same as before...","title":"Example 1: 1 dim manifold"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.1.%20The%203%20body%20problem/","text":"The restricted three-body problem (RTBP) \u00b6 Ref : V. Szebehely, \"Theory of orbits\" Assume 2 points bodies, called primaries, \\(P_1\\) , \\(P_2\\) , with masses \\(m_{1}\\leq m_{2}\\) , that describe circular orbits arround their common center of mass located at the origin on the plan ( \\(X, Y\\) ) -- sidereal or nonrotating coordinates. Assume a third body \\(P_3\\) , with infinitessimal mass, \\(m_{3}\\approx 0\\) such that \\(m_{1}\\) and \\(m_2\\) have influence (gravitational force) on \\(P_3\\) , but \\(P_3\\) does not influence the motion of \\(P_{1}\\) and \\(P_{2}\\) . \\( \\(\\begin{cases} \\vec{R}_{1}(t^{*}) &= (X_{1}(t^{*}), Y_{1}(t^{*})) = (b\\cos{(nt^{*})}, b\\sin{(nt^{*})}) \\\\ \\vec{R}_{2}(t^{*}) &= (X_{2}(t^{*}), Y_{2}(t^{*})) = (-a\\cos{(nt^{*})}, -a\\sin{(nt^{*})}) \\end{cases}\\) \\) Now, I'd like to know the motion of \\(P_{3}\\) . ![[3-body_problem_ini_diagram]] By the Newton's law: $$m_{3}\\ddot{\\vec{R}} = - G \\frac{m_{3}m_{1}(\\vec{R}-\\vec{R} 1)}{||\\vec{R} - \\vec{R} {1}||^{3}} - G\\frac{m_{3}m_{2}(\\vec{R}-\\vec{R} 2)}{||\\vec{R} - \\vec{R} {2}||^{3}} = $$ where the dot notation is the derivative with respect to time ( \\(t^*\\) ). This is a system of 2 ODE of 2nd order or ,equivalently, a system of 4 ODE of 1st order, non-autonomous (it depends on \\(t^{*}\\) ). Now let us consider a new system of coordinates \\((x, y)\\) -- synodical or rotating -- such that the primaries remain fixed on the x axis. \\[\\begin{pmatrix}X \\\\ Y\\end{pmatrix} = \\begin{pmatrix}\\cos{nt^{*}} & -\\sin{nt^{*}} \\\\ \\sin{nt^{*}} & \\cos{nt^{*}}\\end{pmatrix}\\] or \\( \\(\\begin{pmatrix}x \\\\ y \\end{pmatrix} = \\begin{pmatrix}\\cos{nt^{*}} & \\sin{nt^{*}} \\\\ - \\sin{nt^{*}} & \\cos{nt^{*}}\\end{pmatrix}\\begin{pmatrix}X \\\\ Y\\end{pmatrix}\\) \\) Now we choose suitable units of mass, length and time such that \\(n=1\\) , \\(a+b=1\\) , \\(G=1\\) , \\(m_{1}+ m_{2}= 1\\) (see V.S ). Denote \\(t\\) this new time. \\[\\begin{cases} x'' - 2y' &= \\frac{\\partial\\Omega}{\\partial x} \\\\ y'' + 2x' &= \\frac{\\partial\\Omega}{\\partial y} \\end{cases}$$ where $^{'} = \\frac{d}{dt}$, $\\Omega(x,y) = \\frac{1}{2}(x^{2}+y^{2}) + \\frac{1-\\mu}{r_{1}} + \\frac{\\mu}{r_{2}} + \\frac{1}{2}\\mu(1-\\mu)$ where $\\mu = \\frac{m_{2}}{m_{1}+m_{2}} \\in (0, 0.5]$ , $r_{1}=dist(P_{1}, P_{3})$, $r_{2}=dist(P_{2}, P_{3}))$. ![[3_body_problem_rotating_frame]] Then, ###### Def | Position vectors $$\\begin{cases} r_{1} &= \\sqrt{(x-\\mu)^{2} + y^{2}} \\\\ r_{2} &= \\sqrt{(x-\\mu + 1)^{2} + y^{2}} \\end{cases}\\] This system is now autonomous. Exercise 1 \u00b6 Check that this expression $$ 2 \\Omega(x, y) - (x'^{2} + y'^{2}) = C$$ is a first integral called [[Jacobi first integral]] Exercise 2 \u00b6 2.1. The system of ODE can be written as a Hamiltonian System with 2 degrees of freedom. taking \\(x_{1}=x\\) , \\(x_{2}=y\\) , \\(y_{1}=x'-y\\) , \\(y_{2}= y' + x\\) and Hamiltonian function \\( \\(H(x_{1}, x_{2}, y_{1}, y_{2}) = \\frac{1}{2}(y_{1}^{2}+y_{2}^{2}) +x_{2}y_{1}-x_{1}y_{2} - \\left(\\frac{1-\\mu}{r_{1}} + \\frac{\\mu}{r_{2}}\\right)\\) \\) ![[2.1. The 3 body problem 2022-11-14 18.14.30.excalidraw]] The Hamiltonian system of ODE is \\( \\(\\begin{cases} x_{i}' &= \\frac{dx_{i}}{dt}=\\frac{\\partial H}{\\partial y_i} \\\\ y_{i}' &= \\frac{dy_{i}}{dt}=-\\frac{\\partial H}{\\partial x_i} \\end{cases}\\) \\) Recall that, in general, given a Hamiltonian function with \\(n\\) degrees of freedom, \\(H(x_{1}, \\dots, x_{n}, y_{1}, \\dots, y_{n})\\) , the associated Hamiltonian system of ODE is \\( \\(\\begin{cases} x_{i}' &= \\frac{dx_{i}}{dt}=\\frac{\\partial H}{\\partial y_i} \\\\ y_{i}' &= \\frac{dy_{i}}{dt}=-\\frac{\\partial H}{\\partial x_i} \\end{cases} \\qquad i = 1,\\dots, n\\) \\) 2.2 Prove that \\(H(x,y) =H(x_{1}, \\dots, x_{n}, y_{1}, \\dots, y_{n})\\) is a first integral for the Hamiltonian System. This is just one line. dH/dt? 2.3 Write the relation between \\(H = h\\) and \\(C\\) (the relation can only contain \\(h\\) , \\(C\\) , \\(\\mu\\) ). For short, we can also write it like \\( \\(\\begin{cases} x'' - 2y' &= \\Omega_x \\\\ y'' + 2x' &= \\Omega_y \\end{cases}\\) \\) Comments : Why do we add the term \\(\\frac{1}{2}\\mu(1-\\mu)\\) for \\(\\Omega\\) if it doesn't play a role in the ODE? Just because you want \\(\\Omega\\) to take easy numbers for specific values of x, y A first generalization of the RTBP is to consider the spatial RTBP, that is, \\(P_1\\) and \\(P_2\\) are fixed on the \\(x\\) axis (they move on the ( \\(X, Y\\) ) system), but \\(P_3\\) moves in \\((x, y, z)\\) (or \\((X, Y, Z)\\) ). We obtain a system of 6 ODE of first order. A second generalization has to do with the 2-body motion of the primaries \\(P_{1}, P_{2}\\) : Circular orbits -> Circular RTBP Elliptic orbits -> Elliptic RTBP Parabolic orbits -> Parabolic RTBP Hyperbolic orbits -> Hyperbolic RTBP The RTBP can be (has been) used as a preliminary model to explain: Several real missions: Genesis, Artemis, etc. Trojan asteroids Coorbital motion of Janus and Epimetheus (atellites of Saturn) Etc. Existence of equilibrium points for the RTBP \u00b6 Take \\(x_{1}= x\\) , \\(x_{2}=y\\) , \\(x_{3}= x'\\) , \\(x_{4}= y'\\) , the system (S) becomes \\( \\(\\begin{cases} \\frac{dx_{1}}{dt}= x_{1}' = x_{3} \\\\ \\frac{dx_{2}}{dt}= x_{2}' = x_{4} \\\\ \\frac{dx_{3}}{dt}= x_{3}' = 2x_{4} + \\Omega_{x_{1}} \\\\ \\frac{dx_{4}}{dt}= x_{4}' = -2x_{3} + \\Omega_{x_{2}} \\\\ \\end{cases}\\) \\) This is the system of ODE's of first order we are going to implement. Def | ODE reformulation \u00b6 \\( \\(\\begin{cases} x_{1}' = x_{3} \\\\ x_{2}' = x_{4} \\\\ x_{3}' = 2x_{4} + \\Omega_{x_{1}} \\\\ x_{4}' = -2x_{3} + \\Omega_{x_{2}} \\\\ \\end{cases} \\qquad \\Leftrightarrow \\qquad X' = F(X)\\) \\) Equilibrium points : \\(F(X) = 0\\) , \\(x_{3}=x_{4}=0\\) \\( \\(\\begin{cases} \\Omega_{x_{1}} = 0 \\\\ \\Omega_{x_{2}} = 0 \\end{cases} \\qquad \\Leftrightarrow \\begin{cases} \\Omega_x = 0 \\\\ \\Omega_y = 0 \\end{cases}\\) \\) So, \\( \\(\\begin{cases} \\Omega_x = 0 \\Leftrightarrow x - \\frac{(1-\\mu)(x-\\mu)}{r_{1}^{3}} - \\frac{\\mu(x-\\mu + 1)}{r_{2}^{3}} = 0\\\\ \\Omega_{y} = 0 \\Leftrightarrow y\\left[ 1- \\frac{(1-\\mu)}{r_{1}^{3}} - \\frac{\\mu}{r_{2}^{3}}\\right] = 0 \\end{cases}\\) \\) We distinguish two cases: Case a: \\(y\\neq 0\\) \u00b6 ![[RTBP_L4|300]] From the 2nd eq, necessarily we have \\( \\(1- \\frac{(1-\\mu)}{r_{1}^{3}} - \\frac{\\mu}{r_{2}^{3}} = 0\\) \\) From the 1st eq: \\( \\(x\\left( 1- \\frac{(1-\\mu)}{r_{1}^{3}} - \\frac{\\mu}{r_{2}^{3}}\\right) + \\frac{\\mu(1-\\mu)}{r_{1}^{3}} - \\frac{\\mu(1-\\mu)}{r_{2}^{3}} = 0\\) \\) The first term is 0 (from the eq above), so we have that if \\(\\mu(1-\\mu) \\neq 0\\) : $$ \\frac{1}{r_{1}^{3}}- \\frac{1}{r_{2}^{3}} = 0 \\Leftrightarrow r_{1}^{3} = r_{2}^{3} \\Leftrightarrow r_{1} = r_{2}$$ Then from the first condition we get: \\( \\(1-\\frac{1-\\mu}{r^{3}} - \\frac{\\mu}{r^{3}} = 1 - \\frac{1}{r^{3}} = 0 \\Leftrightarrow r^{3}=1\\) \\) So we have, \\(r=1\\) as equilibrim point. This means we have an equilateral triangle. Upwards (L4) and downwards (L5). \\(L_{4}, L_{5}\\) are called the triangular (or equilateral) equilibrium points: \\(L_{4,5} = (x_{1},x_{2},x_{3},x_{4}) = (\\mu-\\frac{1}{2}, \\frac{\\pm\\sqrt{3}}{2}, 0, 0)\\) Case b: \\(y = 0\\) \u00b6 Remember, \\( \\(\\begin{cases} \\Omega_x = 0 \\Leftrightarrow x - \\frac{(1-\\mu)(x-\\mu)}{r_{1}^{3}} - \\frac{\\mu(x-\\mu + 1)}{r_{2}^{3}} = 0\\\\ \\Omega_{y} = 0 \\Leftrightarrow y\\left[ 1- \\frac{(1-\\mu)}{r_{1}^{3}} - \\frac{\\mu}{r_{2}^{3}}\\right] = 0 \\end{cases}\\) \\) So, the 2nd equation is satified automatically. We need to solve only the first one. Also, the position vectors are now \\( \\(\\begin{cases} r_{1} &= \\sqrt{(x-\\mu)^{2}} \\\\ r_{2} &= \\sqrt{(x-\\mu + 1)^{2}} \\end{cases}\\) \\) We will distinguish 3 different cases: Case b.1: \\(L_{2}\\) \u00b6 ![[RTBP_L2]] Those equilibrium points on the \\(x\\) -axis located on the left handside of \\(P_2\\) , at \\(\\xi\\) distance from \\(P_2\\) : \\( \\(\\begin{cases} x &= \\mu - 1-\\xi \\\\ r_{1} &= 1 + \\xi \\\\ r_{2} &= \\xi \\end{cases}\\) \\) We plug this into the previous expressions: \\( \\(\\begin{aligned} 0 =& x - \\frac{(1-\\mu)(x-\\mu)}{r_{1}^{3}} - \\frac{\\mu(x-\\mu + 1)}{r_{2}^{3}} \\\\ 0 =& \\mu-1 - \\xi + \\frac{(1-\\mu)(1+\\xi)}{(1+\\xi)^{3}} - \\frac{\\mu\\xi}{\\xi^{3}} \\end{aligned}\\) \\) Multiplying by \\((1+\\xi)^{2}\\xi^{2}\\) , we obtain Euler quintic polynomial equation \u00b6 \\( \\(\\xi^{5}+ (3-\\mu)\\xi^{4} + (3-2\\mu)\\xi^{3} - \\mu\\xi^{2} - 2\\mu\\xi - \\mu = 0\\) \\) Eq(EL2) We want all the possible roots with \\(\\xi> 0\\) . In order to prove that there exists a unique \\(\\xi > 0\\) : 1. First, we use [[Descartes' rule]]: The number of positive roots of a polynomial is less or equal than the number of sign changes in the sequence given by the coefficients of the polynomial. Coefficients: \\(1, \\; 3-\\mu, \\; 3-2\\mu, \\; -\\mu, \\; -2\\mu, -1\\) 1 single change of sign from \\(3-2\\mu\\) to \\(-\\mu\\) (remember \\(\\mu \\in (0, \\frac{1}{2})\\) ), which means the number of positive roots is \\(\\leq 1\\) . 2. Second, we apply [[Bolzano's theorem]]: \\( \\(\\begin{cases} P(0) = -\\mu < 0 \\\\ P(1) = 7-7\\mu=7(1-\\mu) > 0 \\end{cases}\\) \\) To find that there's at least one root between \\(0\\) and \\(1\\) . So, we will have \\(L_{2}=(\\mu-1-\\xi, 0, 0, 0)\\) . Numerical computation of \\(\\xi\\) \u00b6 V. Szebehly's book One way is to rewrite #Euler quintic polynomial equation as \\( \\(\\xi^{3}= \\frac{\\mu(1+\\xi)^{2}}{3-2\\mu + \\xi(3-\\mu + \\xi)} \\quad \\Leftrightarrow \\quad \\xi= \\left[\\frac{\\mu(1+\\xi)^{2}}{3-2\\mu + \\xi(3-\\mu + \\xi)}\\right]^{\\frac{1}{3}} = f_{2}(\\xi)\\) \\) We take an initial \\(\\xi_{0}\\) and we compute the sequence: \\( \\(\\xi_{0}, \\; \\xi_{1}=f_{2}(x_{0}), \\; \\xi_{2}=f_{2}(x_{1}), \\; \\dots \\; \\xi_{n+1}=f_{2}(x_{n}), \\; \\dots\\) \\) We stop the procedure when \\(|f(\\xi) - \\xi| < tol\\) . What value of \\(\\xi_{0}\\) do we take? We write (EL2) as \\( \\(\\left(\\frac{\\mu}{3(1-\\mu)}\\right)^\\frac{1}{3}= \\left(\\frac{\\xi^{3}(1+\\xi+\\xi^{2}/3)}{(1+\\xi)^{2}(1-\\xi)^{3}}\\right)^\\frac{1}{3}\\) \\) which is equivalent to saying \\( \\(\\nu = F(\\xi) \\Leftrightarrow \\xi = F^{-1}(\\nu) = g(\\nu) = g'(0)\\nu + \\dots = \\nu + \\dots\\) \\) We, thus, take as \\(\\xi_{0}=\\nu = \\left(\\frac{\\mu}{3(1-\\mu)}\\right)^{\\frac{1}{3}}\\) . Case b.2: \\(L_{1}\\) \u00b6 ![[RTBP_L1|300]] We look for those equilibrium points that are between \\(P_{1}\\) and \\(P_{2}\\) . It's at \\(\\xi\\) distance from \\(P_{2}\\) \\[\\begin{cases} x &= \\mu - 1 + \\xi \\\\ r_{1} &= 1 - \\xi \\\\ r_{2} &= \\xi \\end{cases}$$ We plug this into the previous expressions: $$\\begin{aligned} 0 =& x - \\frac{(1-\\mu)(x-\\mu)}{r_{1}^{3}} - \\frac{\\mu(x-\\mu + 1)}{r_{2}^{3}} \\\\ 0=& \\mu-1 + \\xi + \\frac{(1-\\mu)(1-\\xi)}{(1-\\xi)^{3}} - \\frac{\\mu\\xi}{\\xi^{3}} \\end{aligned}\\] Euler quintic polynomial equation \u00b6 \\( \\(Q(\\xi) = \\xi^{5} - (3-\\mu)\\xi^{4} + (3-2\\mu)\\xi^{3} - \\mu\\xi^{2} + 2\\mu\\xi - \\mu = 0\\) \\) Eq(EL1) We look for roots \\(\\xi \\in (0, 1)\\) . We do \\( \\(\\begin{cases} Q(0) &= -\\mu < 0 \\\\ Q(1) &= 1 - \\mu > 0 \\end{cases}\\) \\) By [[Bolzano's theorem]], there exists at least one root \\(\\xi \\in (0, 1)\\) . Actually, \\(\\exists !\\) root (to be checked numerically). There exists a unique equilibrium point between \\(P_{1}\\) and \\(P_{2}\\) : \\( \\(L_{1}=(\\mu-1+\\xi, 0, 0, 0).\\) \\) Numerical computation of \\(\\xi\\) \u00b6 We move from a polynomial equation to a fix point equation as before \\( \\(\\xi= \\left[\\frac{\\mu(1-\\xi)^{2}}{3-2\\mu - \\xi(3-\\mu - \\xi)}\\right]^{\\frac{1}{3}} = f_{1}(\\xi)\\) \\) and we will have \\(\\xi = f_{1}(\\xi)\\) We do the same procedure as before. For the initial value of \\(\\xi_{0}=\\nu = \\left(\\frac{\\mu}{3(1-\\mu)}\\right)^{\\frac{1}{3}}\\) . Case b.3: \\(L_{3}\\) \u00b6 ![[L3_RTBP]] We look for those equilibrium points that on the right hand side of \\(P_{1}\\) It's at \\(\\xi\\) distance from \\(P_{1}\\) \\[\\begin{cases} x &= \\mu + \\xi \\\\ r_{1} &= \\xi \\\\ r_{2} &= 1 + \\xi \\end{cases}$$ We plug this into the previous expressions: $$\\begin{aligned} 0 =& x - \\frac{(1-\\mu)(x-\\mu)}{r_{1}^{3}} - \\frac{\\mu(x-\\mu + 1)}{r_{2}^{3}} \\\\ 0=& \\mu + \\xi - \\frac{(1-\\mu)\\xi}{\\xi^{3}} - \\frac{\\mu(1+\\xi)}{(1+\\xi)^{3}} \\end{aligned}\\] Euler quintic polynomial equation \u00b6 \\( \\(R(\\xi) = \\xi^{5} + (2+\\mu)\\xi^{4} + (1+2\\mu)\\xi^{3} - (1-\\mu)\\xi^{2} + 2(1-\\mu)\\xi - (1-\\mu) = 0\\) \\) Eq(EL3) We look for roots \\(\\xi \\in (0, 1)\\) . By Descartes rule, theres only one change of sign, so the number of positive roots is \\(\\leq 1\\) . By Bolzano's rule, \\( \\(\\begin{cases} R(0) &= -(1 - \\mu) < 0 \\\\ Q(1) &= 7\\mu > 0 \\end{cases}\\) \\) so at least \\(\\exists\\) one root. There exists a unique equilibrium at the right hand side of \\(P_{1}\\) : \\( \\(L_{3}=(\\mu+\\xi, 0, 0, 0).\\) \\) with \\(\\xi \\in (0, 1)\\) Numerical computation of \\(\\xi\\) \u00b6 As before \\( \\(\\xi= \\left[\\frac{(1-\\mu)(1+\\xi)^{2}}{1+2\\mu + \\xi(2+\\mu + \\xi)}\\right]^{\\frac{1}{3}} = f_{3}(\\xi)\\) \\) and we will have \\(\\xi = f_{3}(\\xi)\\) For the initial value, we proceed as follow: Our quintic polynomial equation is \\(R(\\xi) = 0\\) with \\(\\xi \\in (0, 1)\\) . Numerically, one can check that \\(\\xi\\) is closed to 1, so we do the change of variables: \\(\\eta = \\xi - 1\\) . We obtain this new quintic: \\( \\(\\eta^{5} + (7+\\mu)\\eta^{4} + (19+6\\mu)\\eta^{3} + (24+13\\mu)\\eta^{2} + 2(6+7\\mu)\\eta + 7\\mu = 0\\) \\) or \\( \\(\\eta\\left[12 + 14\\mu + (24+13\\mu)\\eta + (19+6\\mu)\\eta^{2}+(7+\\mu)\\eta^{3} + \\eta^{4} \\right]= -7\\mu\\) \\) So, \\( \\(\\eta = \\frac{-7\\mu}{12 + 14\\mu + O(\\eta)} = \\frac{-7\\mu}{12+14\\mu}(1+O(\\eta)) = \\frac{-7\\mu}{12}(1+O(\\eta))(1+O(\\eta))\\) \\) and then \\( \\(\\xi_{0}= 1 - \\frac{7\\mu}{12}\\) \\) Summary \u00b6 In summary, the RTBP has 5 equlibium points: - \\(L_{1, 2, 3}\\) the collinear ones - \\(L_{4, 5 }\\) the triangular ones. Stability of equilibrium points ( \\(L_{i}, \\quad i=1,\\dots, 5\\) ) \u00b6 Let us compute the eigenvalues of \\(DF(L_{i})\\) \\[DF|_{L_{i}} = \\begin{pmatrix}0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\\\ \\Omega_{xx} & \\Omega_{xy} & 0 & 2 \\\\ \\Omega_{yx} & \\Omega_{yy} & -2 & 0 \\end{pmatrix}$$ The [[characteristic polynomial]] is $\\det{(DF(L_{i}) - \\lambda \\mathbb{I})} = 0$, or equivalently $$\\lambda^{4} + (4-\\Omega_{xx}(L_{i}) - \\Omega_{yy}(L_{i}))\\lambda^{2} + \\Omega_{xx}(L_{i})\\Omega_{yy}(L_{i}) - [\\Omega_{xy}(L_{i})]^{2} = 0\\] For \\(L_{1}, L_{2}, L_{3}\\) \u00b6 Recall $$\\Omega(x, y) = \\frac{1}{2}(x^{2}+ y^{2}) + \\frac{1-\\mu}{r_{1}} + \\frac{\\mu}{r_{2}}+ \\frac{1}{2}\\mu(1-\\mu) $$ and \\( \\(\\begin{align*} \\Omega_{xx}&=1 - \\frac{1-\\mu}{r_{1}^{3}} + \\frac{3(1-\\mu)(x-\\mu)^{2}}{r_{1}^{5}} - \\frac{\\mu}{r_{2}^{3}}+ \\frac{3\\mu(x-\\mu+1)^{2}}{r_{2}^{5}}\\\\ \\Omega_{xy} &= y\\left(\\frac{3(1-\\mu)(x-\\mu)}{r_{1}^{5}} + 3\\mu\\frac{x-\\mu+1}{r_{2}^{5}}\\right)\\\\ \\Omega_{yy} &= 1 - \\frac{1-\\mu}{r_{1}^{3}} - \\frac{\\mu}{r_{2}^{3}}+ y^{2}\\left(\\frac{3(1-\\mu)}{r_1^5}+ \\frac{3\\mu}{r_2^{5}}\\right)\\end{align*}\\) \\) Lemma \u00b6 \\[\\begin{align*} \\Omega_{xx}(L_{i}) &= 1 + 2\\frac{1-\\mu}{|x-\\mu|^{3}} + \\frac{2\\mu}{|x-\\mu+1|^{3}}>0 \\\\ \\Omega_{xy}(L_{i}) &= 0\\\\ \\Omega_{yy}(L_{i}) &< 0 \\qquad \\textrm{(To be checked numerically)} \\end{align*}$$ So the characteristic equation becomes $$\\lambda^{4} + (4-\\Omega_{xx}(L_{i}) - \\Omega_{yy}(L_{i}))\\lambda^{2} + \\Omega_{xx}(L_{i})\\Omega_{yy}(L_{i}) = 0\\] Now, we define: \\( \\(\\begin{cases} \\beta_{1} &= 2 - \\frac{(\\Omega_{xx}+\\Omega_{yy})|_{L_{i}}}{2} \\quad\\longleftarrow 2\\beta_{1} = 4-(\\Omega_{xx}+\\Omega_{yy})|_{L_{i}} \\\\ \\beta_{2}^{2} &=-(\\Omega_{xx}\\cdot\\Omega_{yy})|_{L_{i}} \\end{cases}\\) \\) and we can rewrite the characteristic equation as: \\( \\(\\lambda^{4} + 2\\beta_{1}\\lambda^{2}-\\beta_{2}^{2}= 0 \\Rightarrow \\lambda^{2}= -\\beta_{1} \\pm \\sqrt{\\beta_{1}^{2}+ \\beta_{2}^{2}}\\) \\) Since \\(|\\beta_{1}^{2}+ \\beta_{2}^{2}| > |\\beta_{1}|\\) , it means that we will have that the square root is bigger so we have two kind of solutions: \\( \\(\\lambda^{2}= \\begin{cases} A > 0 \\\\ B < 0 \\end{cases}\\) \\) and thus \\( \\(\\Rightarrow \\lambda = \\begin{cases} \\quad\\sqrt{A} > 0 \\\\ - \\sqrt{A} < 0 \\\\ \\quad \\sqrt{-B}i \\\\ -\\sqrt{-B}i \\end{cases}\\) \\) This is going to be called center x saddle , center for the imaginary eigenvalues and and saddle for the real, positive and negative points. (check picture). So \\(\\forall u \\in (0, \\frac{1}{2})\\) , \\(L_{1}, L_{2}, L_{3}\\) are unstable for the original nonlinear system (the RTBP). For any \\(\\mu \\in (0, \\frac{1}{2}]\\) , there exists: - a 1d unstable manifold \\(W^{u}(L_{i})\\) - a 1d stable manifold \\(W^{s}(L_{i})\\) For \\(L_{4}, L_{5}\\) \u00b6 For \\(L_{4, 5} = (\\mu-\\frac{1}{2}, \\pm \\frac{\\sqrt{3}}{2}, 0, 0)\\) , we compute: \\( \\(\\begin{align*} \\Omega_{xx}(L_{4}) = \\Omega_{xx}(L_{5})= \\frac{3}{4}\\\\ \\Omega_{yy}(L_{4}) = \\Omega_{yy}(L_{5})= \\frac{9}{4}\\\\ \\Omega_{xy}(L_{4}) = \\frac{3\\sqrt{3}}{2}\\left(\\mu - \\frac{1}{2}\\right)= - \\Omega_{xy}(L_{5}) \\end{align*}\\) \\) checking this is an exercise We obtain \\( \\(\\lambda^{4}+ \\lambda^{2}+ \\frac{27}{4}\\mu (1-\\mu) = 0\\) \\) so that \\( \\(\\lambda^{2}= \\frac{1}{2}\\left(-1 \\pm \\sqrt{1 - 27\\mu(1-\\mu)}\\right)\\) \\) Now let's discuss the sign of the inside of the root, which we will define as: \\( \\(f(\\mu) = 1-27\\mu(1- \\mu) = 27\\mu^{2}-27\\mu+1, \\quad \\mu\\in (0, 1/2]\\) \\) We are interested in the point where the graph crosses the x-axis: \\(\\mu_{R}\\) : \\( \\(f(\\mu_{R}) = 0 \\rightarrow 27\\mu^{2} - 27\\mu+1 = 0 \\Rightarrow \\mu_{R}=\\frac{1}{2}\\left(1\\pm\\frac{\\sqrt{69}}{9}\\right)\\) \\) since \\(\\mu \\in (0, 1/2]\\) , then we take the minus part: \\( \\(\\mu_{R}=\\frac{1}{2}\\left(1-\\frac{\\sqrt{69}}{9}\\right) = 0.038...\\) \\) We distinguish 3 cases: Case 1: \\(0<\\mu<\\mu_{R}\\) \u00b6 \\( \\(\\lambda^{2}= \\begin{cases} \\Lambda_{1} < 0 \\\\ \\Lambda_{2} < 0 \\end{cases} \\Rightarrow \\lambda = \\begin{cases} \\pm \\sqrt{|\\Lambda_{1|}}i = \\pm w_{1}i \\\\ \\pm \\sqrt{|\\Lambda_{2|}}i = \\pm w_{2}i \\end{cases}\\) \\) with \\( \\(\\begin{align*} w_{1}= \\left(\\frac{1}{2}|-1 - \\sqrt{1 - 27\\mu(1-\\mu)}|\\right)^{\\frac{1}{2}}= \\frac{1}{\\sqrt{2}}\\sqrt{1 + \\sqrt{1 - 27\\mu(1-\\mu)}}\\\\ w_{2}= \\left(\\frac{1}{2}|-1 + \\sqrt{1 - 27\\mu(1-\\mu)}|\\right)^{\\frac{1}{2}}= \\frac{1}{\\sqrt{2}}\\sqrt{1 - \\sqrt{1 - 27\\mu(1-\\mu)}} \\end{align*}\\) \\) so that \\(\\omega_{1}^{2}> \\frac{1}{2}>\\omega_{2}^{2}\\) Picture with plot, all points in the y-axis![[2.1. The 3 body problem 2022-11-15 12.31.55.excalidraw]] In this case we have a center x center : we have no info about the stability for the RTBP. Case 2: \\(\\mu=\\mu_{R}\\) \u00b6 \\(\\lambda^{2}= \\frac{-1}{2}\\Rightarrow \\lambda = \\pm \\frac{1}{\\sqrt{2}}i\\) Picture with plot, all points in the y-axis This is a degenerate center x center ( \\(\\omega_{1}= \\omega_{2}=\\omega\\) ) Case 3: \\(\\mu_{R}<\\mu<1/2\\) \u00b6 \\[\\lambda^{2}= \\frac{-1\\pm ai}{2} \\Rightarrow \\lambda = \\begin{cases} \\quad\\lambda_{1} \\\\ \\quad\\bar{\\lambda}_{1} \\\\ -\\lambda_{2} \\\\ -\\bar{\\lambda}_{2} \\end{cases} \\] This is a complex saddle : -There exists a 2d unstable manifold \\(W^{u}(L_{4,5})\\) -There exists a 2d stable manifold \\(W^{s}(L_{4,5})\\) It's unstable. How to compute it numerically? \u00b6 We have a tangent plane, we take a circle of initial conditions around the point and integrate forward/backward in time. Remark \u00b6 Another very important consequence of the type of eigenvalues is the existence of periodic orbits, guaranteed by Lyapunov Theorem: Lyapunov Theorem \u00b6 Ref Meyer-Hall-Offin book Assume that a system of ODE: \\( \\(\\vec{x}' = \\vec{f}(\\vec{x}), \\quad \\vec{x}\\in U\\subset \\mathbb{R}^{n}\\) \\) has a nondegenerate first integral \\(H(\\vec{x})\\) (that is, \\(\\nabla H(\\vec{r}) \\neq 0\\) for \\(\\vec{r}\\) a regular point), and an equilibrium point with chaaracteristic exponents \\(\\pm i\\omega, \\lambda_{3}, \\dots, \\lambda_{n}\\) . If \\(\\frac{\\lambda_{j}}{i\\omega} \\notin \\mathbb{Z}\\) , \\(j=3, \\dots, n\\) , then there exists a one-paramter family of periodic orbits emanating from the equilibrium point. Moreover, when approaching the equilibrium point along the family, the period tends to \\(\\frac{2\\pi}{\\omega}\\) . If I give the value of one parameter (which will be the first integral), then I can identify the orbit. Application to the RTBP \u00b6 \\( \\(\\vec{X}' = \\vec{F}(\\vec{X}) \\quad \\vec{X}\\in U\\subset\\mathbb{R}^{4}\\) \\) First integral: \\(C(x, y, x', y') = 2\\Omega(x, y) - (x'^{2}+y'^{2})\\) ([[Jacobi first integral]]). *Check: \\(\\nabla C = (2 \\Omega_{x}, 2 \\Omega_{y}, -2x', -2y') = \\vec{0} \\Longleftrightarrow \\Omega_{x}= \\Omega_{y}=x'=y' = 0 \\Longleftrightarrow L_{i}, i=1, \\dots, 5\\) For \\(L_{1}, L_{2}, L_{3}\\) \u00b6 Eigenvalues: \\(\\pm i \\omega, \\pm \\lambda\\) , with \\(\\lambda \\in \\mathbb{R}\\) , we check \\( \\(\\frac{\\lambda}{i \\omega} \\notin \\mathbb{Z}\\) \\) since \\(\\lambda\\) is always real and divided by an imaginary number it will never be an integer number. \\(\\Rightarrow\\) for any value of \\(\\mu\\) , there exists a one-parameter family of periodic orbits emanating at \\(L_{1,2,3}\\) and the period of the periodic orbits tend to \\(\\frac{2\\pi}{\\omega}\\) (when approaching \\(L_{i}\\) ) Check picture For \\(L_{4}, L_{5}\\) \u00b6 Case: \\(\\mu = \\mu_{R}\\) , then \\(\\omega_{1}=\\omega_{2}=\\omega\\) , so the coefficient will be 1, an integer number. Case: \\(\\mu_{R}<\\mu<1/2\\) we don't have exponents in the form of \\(\\pm i \\omega\\) , so I cannot apply the formula. Case : \\(0 < \\mu < \\mu_{R}\\) eigenvalues \\(\\pm i\\omega_{1}, \\pm i\\omega_{2}\\) ( \\(\\omega_{1}>\\omega_{2}, \\, \\omega_{1}^{2}> \\frac{1}{2}>\\omega_{2}^{2}\\) ). First case We have \\(0 <\\frac{ \\pm i \\omega_{2}}{i\\omega_{1}} = \\frac{\\omega_{2}}{\\omega_{1}} < 1\\) (since \\(\\omega_{1}>\\omega_{2}\\) ) , then \\(\\frac{\\pm i \\omega_{2}}{i\\omega_{1}} \\notin \\mathbb{Z} \\Rightarrow\\) There exists a one-parameter family of periodic orbits emanating at \\(L_{4}\\) and \\(L_{5}\\) and the period tends to \\(2\\pi/\\omega_{1}\\) : the so called family of Lyapunov periodic orbits of short period . Second case We have \\(\\frac{ \\pm i \\omega_{2}}{i\\omega_{1}} = \\frac{\\omega_{2}}{\\omega_{1}} \\notin \\mathbb{Z}\\) Let us show that this is true for \\(\\mu \\neq \\mu_r\\) , \\(\\mu_{r}=\\frac{1}{2}-\\frac{1}{2}\\left(\\frac{1-16r^2}{27(r^{2}+1)^{2}}\\right)^{\\frac{1}{2}}\\) , for \\(r=1, 2, 3...\\) then \\(\\frac{\\omega_{1}}{\\omega_{2}\\notin}\\mathbb{Z}\\) . We assume now \\(\\frac{\\omega_{1}}{\\omega_{2}}\\) , which is equivalent to \\( \\(\\frac{\\frac{1}{\\sqrt{2}}\\sqrt{1+x}}{\\frac{1}{\\sqrt{2}}\\sqrt{1-x}}= r\\) \\) where \\(x = \\sqrt{1-27\\mu(1-\\mu)}\\) Same as, \\( \\(\\frac{1+x}{1-x}=r^{2} \\Leftrightarrow x = \\frac{r^{2}-1}{r^{2}+1} \\Leftrightarrow \\mu^{2}-\\mu + \\frac{4r^{2}}{27(r^{2}+1)^{2}}=0\\) \\) same as \\( \\(\\mu = \\frac{1}{2}\\left(1 \\pm \\sqrt{1- \\frac{16r^{2}}{27(r^{2}+1)}}\\right)\\) \\) Since \\(\\mu\\) must \\(\\mu\\in(0, 1/2]\\) , then it means that the values of \\(\\mu\\) for which we have an integer ratio are \\( \\(\\mu_{r} = \\frac{1}{2} - \\frac{1}{2}\\left(1- \\frac{16r^{2}}{27(r^{2}+1)}\\right)^\\frac{1}{2}\\) \\) If \\(\\mu \\neq \\mu_{r}\\) , \\(r=1, 2...\\) then there exists a one-parameter family of Lyapunov periodic orbits emanating at \\(L_{4,5}\\) and the period tends to \\(\\frac{2\\pi}{\\omega_{2}}\\) when approaching \\(L_{4,5}\\) : the so called Lyapunov family of periodic orbits of long period . So, when \\(\\mu\\) is different from \\(\\mu_{r}\\) , then the ratio won't be an integer ratio. We will demonstrate the opposite. Recap: Computation of unstable manifolds taking the linear approximation \u00b6 ![[Unstable_manifold_computation]] We have a circle of initial conditions: \\( \\(\\begin{cases} s_{1} &= r \\cos{\\theta} \\\\ s_{2} &= r \\sin{\\theta} \\end{cases}\\) \\) for \\(\\theta \\in [0, 2\\pi]\\) and \\(r\\) small and for each initial condition we integrate forward in time. However, we will see a more precise way to do this.","title":"2.1. The 3 body problem"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.1.%20The%203%20body%20problem/#the-restricted-three-body-problem-rtbp","text":"Ref : V. Szebehely, \"Theory of orbits\" Assume 2 points bodies, called primaries, \\(P_1\\) , \\(P_2\\) , with masses \\(m_{1}\\leq m_{2}\\) , that describe circular orbits arround their common center of mass located at the origin on the plan ( \\(X, Y\\) ) -- sidereal or nonrotating coordinates. Assume a third body \\(P_3\\) , with infinitessimal mass, \\(m_{3}\\approx 0\\) such that \\(m_{1}\\) and \\(m_2\\) have influence (gravitational force) on \\(P_3\\) , but \\(P_3\\) does not influence the motion of \\(P_{1}\\) and \\(P_{2}\\) . \\( \\(\\begin{cases} \\vec{R}_{1}(t^{*}) &= (X_{1}(t^{*}), Y_{1}(t^{*})) = (b\\cos{(nt^{*})}, b\\sin{(nt^{*})}) \\\\ \\vec{R}_{2}(t^{*}) &= (X_{2}(t^{*}), Y_{2}(t^{*})) = (-a\\cos{(nt^{*})}, -a\\sin{(nt^{*})}) \\end{cases}\\) \\) Now, I'd like to know the motion of \\(P_{3}\\) . ![[3-body_problem_ini_diagram]] By the Newton's law: $$m_{3}\\ddot{\\vec{R}} = - G \\frac{m_{3}m_{1}(\\vec{R}-\\vec{R} 1)}{||\\vec{R} - \\vec{R} {1}||^{3}} - G\\frac{m_{3}m_{2}(\\vec{R}-\\vec{R} 2)}{||\\vec{R} - \\vec{R} {2}||^{3}} = $$ where the dot notation is the derivative with respect to time ( \\(t^*\\) ). This is a system of 2 ODE of 2nd order or ,equivalently, a system of 4 ODE of 1st order, non-autonomous (it depends on \\(t^{*}\\) ). Now let us consider a new system of coordinates \\((x, y)\\) -- synodical or rotating -- such that the primaries remain fixed on the x axis. \\[\\begin{pmatrix}X \\\\ Y\\end{pmatrix} = \\begin{pmatrix}\\cos{nt^{*}} & -\\sin{nt^{*}} \\\\ \\sin{nt^{*}} & \\cos{nt^{*}}\\end{pmatrix}\\] or \\( \\(\\begin{pmatrix}x \\\\ y \\end{pmatrix} = \\begin{pmatrix}\\cos{nt^{*}} & \\sin{nt^{*}} \\\\ - \\sin{nt^{*}} & \\cos{nt^{*}}\\end{pmatrix}\\begin{pmatrix}X \\\\ Y\\end{pmatrix}\\) \\) Now we choose suitable units of mass, length and time such that \\(n=1\\) , \\(a+b=1\\) , \\(G=1\\) , \\(m_{1}+ m_{2}= 1\\) (see V.S ). Denote \\(t\\) this new time. \\[\\begin{cases} x'' - 2y' &= \\frac{\\partial\\Omega}{\\partial x} \\\\ y'' + 2x' &= \\frac{\\partial\\Omega}{\\partial y} \\end{cases}$$ where $^{'} = \\frac{d}{dt}$, $\\Omega(x,y) = \\frac{1}{2}(x^{2}+y^{2}) + \\frac{1-\\mu}{r_{1}} + \\frac{\\mu}{r_{2}} + \\frac{1}{2}\\mu(1-\\mu)$ where $\\mu = \\frac{m_{2}}{m_{1}+m_{2}} \\in (0, 0.5]$ , $r_{1}=dist(P_{1}, P_{3})$, $r_{2}=dist(P_{2}, P_{3}))$. ![[3_body_problem_rotating_frame]] Then, ###### Def | Position vectors $$\\begin{cases} r_{1} &= \\sqrt{(x-\\mu)^{2} + y^{2}} \\\\ r_{2} &= \\sqrt{(x-\\mu + 1)^{2} + y^{2}} \\end{cases}\\] This system is now autonomous.","title":"The restricted three-body problem (RTBP)"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.1.%20The%203%20body%20problem/#exercise-1","text":"Check that this expression $$ 2 \\Omega(x, y) - (x'^{2} + y'^{2}) = C$$ is a first integral called [[Jacobi first integral]]","title":"Exercise 1"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.1.%20The%203%20body%20problem/#exercise-2","text":"2.1. The system of ODE can be written as a Hamiltonian System with 2 degrees of freedom. taking \\(x_{1}=x\\) , \\(x_{2}=y\\) , \\(y_{1}=x'-y\\) , \\(y_{2}= y' + x\\) and Hamiltonian function \\( \\(H(x_{1}, x_{2}, y_{1}, y_{2}) = \\frac{1}{2}(y_{1}^{2}+y_{2}^{2}) +x_{2}y_{1}-x_{1}y_{2} - \\left(\\frac{1-\\mu}{r_{1}} + \\frac{\\mu}{r_{2}}\\right)\\) \\) ![[2.1. The 3 body problem 2022-11-14 18.14.30.excalidraw]] The Hamiltonian system of ODE is \\( \\(\\begin{cases} x_{i}' &= \\frac{dx_{i}}{dt}=\\frac{\\partial H}{\\partial y_i} \\\\ y_{i}' &= \\frac{dy_{i}}{dt}=-\\frac{\\partial H}{\\partial x_i} \\end{cases}\\) \\) Recall that, in general, given a Hamiltonian function with \\(n\\) degrees of freedom, \\(H(x_{1}, \\dots, x_{n}, y_{1}, \\dots, y_{n})\\) , the associated Hamiltonian system of ODE is \\( \\(\\begin{cases} x_{i}' &= \\frac{dx_{i}}{dt}=\\frac{\\partial H}{\\partial y_i} \\\\ y_{i}' &= \\frac{dy_{i}}{dt}=-\\frac{\\partial H}{\\partial x_i} \\end{cases} \\qquad i = 1,\\dots, n\\) \\) 2.2 Prove that \\(H(x,y) =H(x_{1}, \\dots, x_{n}, y_{1}, \\dots, y_{n})\\) is a first integral for the Hamiltonian System. This is just one line. dH/dt? 2.3 Write the relation between \\(H = h\\) and \\(C\\) (the relation can only contain \\(h\\) , \\(C\\) , \\(\\mu\\) ). For short, we can also write it like \\( \\(\\begin{cases} x'' - 2y' &= \\Omega_x \\\\ y'' + 2x' &= \\Omega_y \\end{cases}\\) \\) Comments : Why do we add the term \\(\\frac{1}{2}\\mu(1-\\mu)\\) for \\(\\Omega\\) if it doesn't play a role in the ODE? Just because you want \\(\\Omega\\) to take easy numbers for specific values of x, y A first generalization of the RTBP is to consider the spatial RTBP, that is, \\(P_1\\) and \\(P_2\\) are fixed on the \\(x\\) axis (they move on the ( \\(X, Y\\) ) system), but \\(P_3\\) moves in \\((x, y, z)\\) (or \\((X, Y, Z)\\) ). We obtain a system of 6 ODE of first order. A second generalization has to do with the 2-body motion of the primaries \\(P_{1}, P_{2}\\) : Circular orbits -> Circular RTBP Elliptic orbits -> Elliptic RTBP Parabolic orbits -> Parabolic RTBP Hyperbolic orbits -> Hyperbolic RTBP The RTBP can be (has been) used as a preliminary model to explain: Several real missions: Genesis, Artemis, etc. Trojan asteroids Coorbital motion of Janus and Epimetheus (atellites of Saturn) Etc.","title":"Exercise 2"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.1.%20The%203%20body%20problem/#existence-of-equilibrium-points-for-the-rtbp","text":"Take \\(x_{1}= x\\) , \\(x_{2}=y\\) , \\(x_{3}= x'\\) , \\(x_{4}= y'\\) , the system (S) becomes \\( \\(\\begin{cases} \\frac{dx_{1}}{dt}= x_{1}' = x_{3} \\\\ \\frac{dx_{2}}{dt}= x_{2}' = x_{4} \\\\ \\frac{dx_{3}}{dt}= x_{3}' = 2x_{4} + \\Omega_{x_{1}} \\\\ \\frac{dx_{4}}{dt}= x_{4}' = -2x_{3} + \\Omega_{x_{2}} \\\\ \\end{cases}\\) \\) This is the system of ODE's of first order we are going to implement.","title":"Existence of equilibrium points for the RTBP"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.1.%20The%203%20body%20problem/#def-ode-reformulation","text":"\\( \\(\\begin{cases} x_{1}' = x_{3} \\\\ x_{2}' = x_{4} \\\\ x_{3}' = 2x_{4} + \\Omega_{x_{1}} \\\\ x_{4}' = -2x_{3} + \\Omega_{x_{2}} \\\\ \\end{cases} \\qquad \\Leftrightarrow \\qquad X' = F(X)\\) \\) Equilibrium points : \\(F(X) = 0\\) , \\(x_{3}=x_{4}=0\\) \\( \\(\\begin{cases} \\Omega_{x_{1}} = 0 \\\\ \\Omega_{x_{2}} = 0 \\end{cases} \\qquad \\Leftrightarrow \\begin{cases} \\Omega_x = 0 \\\\ \\Omega_y = 0 \\end{cases}\\) \\) So, \\( \\(\\begin{cases} \\Omega_x = 0 \\Leftrightarrow x - \\frac{(1-\\mu)(x-\\mu)}{r_{1}^{3}} - \\frac{\\mu(x-\\mu + 1)}{r_{2}^{3}} = 0\\\\ \\Omega_{y} = 0 \\Leftrightarrow y\\left[ 1- \\frac{(1-\\mu)}{r_{1}^{3}} - \\frac{\\mu}{r_{2}^{3}}\\right] = 0 \\end{cases}\\) \\) We distinguish two cases:","title":"Def | ODE reformulation"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.1.%20The%203%20body%20problem/#case-a-yneq-0","text":"![[RTBP_L4|300]] From the 2nd eq, necessarily we have \\( \\(1- \\frac{(1-\\mu)}{r_{1}^{3}} - \\frac{\\mu}{r_{2}^{3}} = 0\\) \\) From the 1st eq: \\( \\(x\\left( 1- \\frac{(1-\\mu)}{r_{1}^{3}} - \\frac{\\mu}{r_{2}^{3}}\\right) + \\frac{\\mu(1-\\mu)}{r_{1}^{3}} - \\frac{\\mu(1-\\mu)}{r_{2}^{3}} = 0\\) \\) The first term is 0 (from the eq above), so we have that if \\(\\mu(1-\\mu) \\neq 0\\) : $$ \\frac{1}{r_{1}^{3}}- \\frac{1}{r_{2}^{3}} = 0 \\Leftrightarrow r_{1}^{3} = r_{2}^{3} \\Leftrightarrow r_{1} = r_{2}$$ Then from the first condition we get: \\( \\(1-\\frac{1-\\mu}{r^{3}} - \\frac{\\mu}{r^{3}} = 1 - \\frac{1}{r^{3}} = 0 \\Leftrightarrow r^{3}=1\\) \\) So we have, \\(r=1\\) as equilibrim point. This means we have an equilateral triangle. Upwards (L4) and downwards (L5). \\(L_{4}, L_{5}\\) are called the triangular (or equilateral) equilibrium points: \\(L_{4,5} = (x_{1},x_{2},x_{3},x_{4}) = (\\mu-\\frac{1}{2}, \\frac{\\pm\\sqrt{3}}{2}, 0, 0)\\)","title":"Case a: \\(y\\neq 0\\)"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.1.%20The%203%20body%20problem/#case-b-y-0","text":"Remember, \\( \\(\\begin{cases} \\Omega_x = 0 \\Leftrightarrow x - \\frac{(1-\\mu)(x-\\mu)}{r_{1}^{3}} - \\frac{\\mu(x-\\mu + 1)}{r_{2}^{3}} = 0\\\\ \\Omega_{y} = 0 \\Leftrightarrow y\\left[ 1- \\frac{(1-\\mu)}{r_{1}^{3}} - \\frac{\\mu}{r_{2}^{3}}\\right] = 0 \\end{cases}\\) \\) So, the 2nd equation is satified automatically. We need to solve only the first one. Also, the position vectors are now \\( \\(\\begin{cases} r_{1} &= \\sqrt{(x-\\mu)^{2}} \\\\ r_{2} &= \\sqrt{(x-\\mu + 1)^{2}} \\end{cases}\\) \\) We will distinguish 3 different cases:","title":"Case b: \\(y = 0\\)"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.1.%20The%203%20body%20problem/#case-b1-l_2","text":"![[RTBP_L2]] Those equilibrium points on the \\(x\\) -axis located on the left handside of \\(P_2\\) , at \\(\\xi\\) distance from \\(P_2\\) : \\( \\(\\begin{cases} x &= \\mu - 1-\\xi \\\\ r_{1} &= 1 + \\xi \\\\ r_{2} &= \\xi \\end{cases}\\) \\) We plug this into the previous expressions: \\( \\(\\begin{aligned} 0 =& x - \\frac{(1-\\mu)(x-\\mu)}{r_{1}^{3}} - \\frac{\\mu(x-\\mu + 1)}{r_{2}^{3}} \\\\ 0 =& \\mu-1 - \\xi + \\frac{(1-\\mu)(1+\\xi)}{(1+\\xi)^{3}} - \\frac{\\mu\\xi}{\\xi^{3}} \\end{aligned}\\) \\) Multiplying by \\((1+\\xi)^{2}\\xi^{2}\\) , we obtain","title":"Case b.1: \\(L_{2}\\)"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.1.%20The%203%20body%20problem/#euler-quintic-polynomial-equation","text":"\\( \\(\\xi^{5}+ (3-\\mu)\\xi^{4} + (3-2\\mu)\\xi^{3} - \\mu\\xi^{2} - 2\\mu\\xi - \\mu = 0\\) \\) Eq(EL2) We want all the possible roots with \\(\\xi> 0\\) . In order to prove that there exists a unique \\(\\xi > 0\\) : 1. First, we use [[Descartes' rule]]: The number of positive roots of a polynomial is less or equal than the number of sign changes in the sequence given by the coefficients of the polynomial. Coefficients: \\(1, \\; 3-\\mu, \\; 3-2\\mu, \\; -\\mu, \\; -2\\mu, -1\\) 1 single change of sign from \\(3-2\\mu\\) to \\(-\\mu\\) (remember \\(\\mu \\in (0, \\frac{1}{2})\\) ), which means the number of positive roots is \\(\\leq 1\\) . 2. Second, we apply [[Bolzano's theorem]]: \\( \\(\\begin{cases} P(0) = -\\mu < 0 \\\\ P(1) = 7-7\\mu=7(1-\\mu) > 0 \\end{cases}\\) \\) To find that there's at least one root between \\(0\\) and \\(1\\) . So, we will have \\(L_{2}=(\\mu-1-\\xi, 0, 0, 0)\\) .","title":"Euler quintic polynomial equation"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.1.%20The%203%20body%20problem/#numerical-computation-of-xi","text":"V. Szebehly's book One way is to rewrite #Euler quintic polynomial equation as \\( \\(\\xi^{3}= \\frac{\\mu(1+\\xi)^{2}}{3-2\\mu + \\xi(3-\\mu + \\xi)} \\quad \\Leftrightarrow \\quad \\xi= \\left[\\frac{\\mu(1+\\xi)^{2}}{3-2\\mu + \\xi(3-\\mu + \\xi)}\\right]^{\\frac{1}{3}} = f_{2}(\\xi)\\) \\) We take an initial \\(\\xi_{0}\\) and we compute the sequence: \\( \\(\\xi_{0}, \\; \\xi_{1}=f_{2}(x_{0}), \\; \\xi_{2}=f_{2}(x_{1}), \\; \\dots \\; \\xi_{n+1}=f_{2}(x_{n}), \\; \\dots\\) \\) We stop the procedure when \\(|f(\\xi) - \\xi| < tol\\) . What value of \\(\\xi_{0}\\) do we take? We write (EL2) as \\( \\(\\left(\\frac{\\mu}{3(1-\\mu)}\\right)^\\frac{1}{3}= \\left(\\frac{\\xi^{3}(1+\\xi+\\xi^{2}/3)}{(1+\\xi)^{2}(1-\\xi)^{3}}\\right)^\\frac{1}{3}\\) \\) which is equivalent to saying \\( \\(\\nu = F(\\xi) \\Leftrightarrow \\xi = F^{-1}(\\nu) = g(\\nu) = g'(0)\\nu + \\dots = \\nu + \\dots\\) \\) We, thus, take as \\(\\xi_{0}=\\nu = \\left(\\frac{\\mu}{3(1-\\mu)}\\right)^{\\frac{1}{3}}\\) .","title":"Numerical computation of \\(\\xi\\)"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.1.%20The%203%20body%20problem/#case-b2-l_1","text":"![[RTBP_L1|300]] We look for those equilibrium points that are between \\(P_{1}\\) and \\(P_{2}\\) . It's at \\(\\xi\\) distance from \\(P_{2}\\) \\[\\begin{cases} x &= \\mu - 1 + \\xi \\\\ r_{1} &= 1 - \\xi \\\\ r_{2} &= \\xi \\end{cases}$$ We plug this into the previous expressions: $$\\begin{aligned} 0 =& x - \\frac{(1-\\mu)(x-\\mu)}{r_{1}^{3}} - \\frac{\\mu(x-\\mu + 1)}{r_{2}^{3}} \\\\ 0=& \\mu-1 + \\xi + \\frac{(1-\\mu)(1-\\xi)}{(1-\\xi)^{3}} - \\frac{\\mu\\xi}{\\xi^{3}} \\end{aligned}\\]","title":"Case b.2: \\(L_{1}\\)"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.1.%20The%203%20body%20problem/#euler-quintic-polynomial-equation_1","text":"\\( \\(Q(\\xi) = \\xi^{5} - (3-\\mu)\\xi^{4} + (3-2\\mu)\\xi^{3} - \\mu\\xi^{2} + 2\\mu\\xi - \\mu = 0\\) \\) Eq(EL1) We look for roots \\(\\xi \\in (0, 1)\\) . We do \\( \\(\\begin{cases} Q(0) &= -\\mu < 0 \\\\ Q(1) &= 1 - \\mu > 0 \\end{cases}\\) \\) By [[Bolzano's theorem]], there exists at least one root \\(\\xi \\in (0, 1)\\) . Actually, \\(\\exists !\\) root (to be checked numerically). There exists a unique equilibrium point between \\(P_{1}\\) and \\(P_{2}\\) : \\( \\(L_{1}=(\\mu-1+\\xi, 0, 0, 0).\\) \\)","title":"Euler quintic polynomial equation"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.1.%20The%203%20body%20problem/#numerical-computation-of-xi_1","text":"We move from a polynomial equation to a fix point equation as before \\( \\(\\xi= \\left[\\frac{\\mu(1-\\xi)^{2}}{3-2\\mu - \\xi(3-\\mu - \\xi)}\\right]^{\\frac{1}{3}} = f_{1}(\\xi)\\) \\) and we will have \\(\\xi = f_{1}(\\xi)\\) We do the same procedure as before. For the initial value of \\(\\xi_{0}=\\nu = \\left(\\frac{\\mu}{3(1-\\mu)}\\right)^{\\frac{1}{3}}\\) .","title":"Numerical computation of \\(\\xi\\)"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.1.%20The%203%20body%20problem/#case-b3-l_3","text":"![[L3_RTBP]] We look for those equilibrium points that on the right hand side of \\(P_{1}\\) It's at \\(\\xi\\) distance from \\(P_{1}\\) \\[\\begin{cases} x &= \\mu + \\xi \\\\ r_{1} &= \\xi \\\\ r_{2} &= 1 + \\xi \\end{cases}$$ We plug this into the previous expressions: $$\\begin{aligned} 0 =& x - \\frac{(1-\\mu)(x-\\mu)}{r_{1}^{3}} - \\frac{\\mu(x-\\mu + 1)}{r_{2}^{3}} \\\\ 0=& \\mu + \\xi - \\frac{(1-\\mu)\\xi}{\\xi^{3}} - \\frac{\\mu(1+\\xi)}{(1+\\xi)^{3}} \\end{aligned}\\]","title":"Case b.3: \\(L_{3}\\)"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.1.%20The%203%20body%20problem/#euler-quintic-polynomial-equation_2","text":"\\( \\(R(\\xi) = \\xi^{5} + (2+\\mu)\\xi^{4} + (1+2\\mu)\\xi^{3} - (1-\\mu)\\xi^{2} + 2(1-\\mu)\\xi - (1-\\mu) = 0\\) \\) Eq(EL3) We look for roots \\(\\xi \\in (0, 1)\\) . By Descartes rule, theres only one change of sign, so the number of positive roots is \\(\\leq 1\\) . By Bolzano's rule, \\( \\(\\begin{cases} R(0) &= -(1 - \\mu) < 0 \\\\ Q(1) &= 7\\mu > 0 \\end{cases}\\) \\) so at least \\(\\exists\\) one root. There exists a unique equilibrium at the right hand side of \\(P_{1}\\) : \\( \\(L_{3}=(\\mu+\\xi, 0, 0, 0).\\) \\) with \\(\\xi \\in (0, 1)\\)","title":"Euler quintic polynomial equation"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.1.%20The%203%20body%20problem/#numerical-computation-of-xi_2","text":"As before \\( \\(\\xi= \\left[\\frac{(1-\\mu)(1+\\xi)^{2}}{1+2\\mu + \\xi(2+\\mu + \\xi)}\\right]^{\\frac{1}{3}} = f_{3}(\\xi)\\) \\) and we will have \\(\\xi = f_{3}(\\xi)\\) For the initial value, we proceed as follow: Our quintic polynomial equation is \\(R(\\xi) = 0\\) with \\(\\xi \\in (0, 1)\\) . Numerically, one can check that \\(\\xi\\) is closed to 1, so we do the change of variables: \\(\\eta = \\xi - 1\\) . We obtain this new quintic: \\( \\(\\eta^{5} + (7+\\mu)\\eta^{4} + (19+6\\mu)\\eta^{3} + (24+13\\mu)\\eta^{2} + 2(6+7\\mu)\\eta + 7\\mu = 0\\) \\) or \\( \\(\\eta\\left[12 + 14\\mu + (24+13\\mu)\\eta + (19+6\\mu)\\eta^{2}+(7+\\mu)\\eta^{3} + \\eta^{4} \\right]= -7\\mu\\) \\) So, \\( \\(\\eta = \\frac{-7\\mu}{12 + 14\\mu + O(\\eta)} = \\frac{-7\\mu}{12+14\\mu}(1+O(\\eta)) = \\frac{-7\\mu}{12}(1+O(\\eta))(1+O(\\eta))\\) \\) and then \\( \\(\\xi_{0}= 1 - \\frac{7\\mu}{12}\\) \\)","title":"Numerical computation of \\(\\xi\\)"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.1.%20The%203%20body%20problem/#summary","text":"In summary, the RTBP has 5 equlibium points: - \\(L_{1, 2, 3}\\) the collinear ones - \\(L_{4, 5 }\\) the triangular ones.","title":"Summary"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.1.%20The%203%20body%20problem/#stability-of-equilibrium-points-l_i-quad-i1dots-5","text":"Let us compute the eigenvalues of \\(DF(L_{i})\\) \\[DF|_{L_{i}} = \\begin{pmatrix}0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\\\ \\Omega_{xx} & \\Omega_{xy} & 0 & 2 \\\\ \\Omega_{yx} & \\Omega_{yy} & -2 & 0 \\end{pmatrix}$$ The [[characteristic polynomial]] is $\\det{(DF(L_{i}) - \\lambda \\mathbb{I})} = 0$, or equivalently $$\\lambda^{4} + (4-\\Omega_{xx}(L_{i}) - \\Omega_{yy}(L_{i}))\\lambda^{2} + \\Omega_{xx}(L_{i})\\Omega_{yy}(L_{i}) - [\\Omega_{xy}(L_{i})]^{2} = 0\\]","title":"Stability of equilibrium points (\\(L_{i}, \\quad i=1,\\dots, 5\\))"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.1.%20The%203%20body%20problem/#for-l_1-l_2-l_3","text":"Recall $$\\Omega(x, y) = \\frac{1}{2}(x^{2}+ y^{2}) + \\frac{1-\\mu}{r_{1}} + \\frac{\\mu}{r_{2}}+ \\frac{1}{2}\\mu(1-\\mu) $$ and \\( \\(\\begin{align*} \\Omega_{xx}&=1 - \\frac{1-\\mu}{r_{1}^{3}} + \\frac{3(1-\\mu)(x-\\mu)^{2}}{r_{1}^{5}} - \\frac{\\mu}{r_{2}^{3}}+ \\frac{3\\mu(x-\\mu+1)^{2}}{r_{2}^{5}}\\\\ \\Omega_{xy} &= y\\left(\\frac{3(1-\\mu)(x-\\mu)}{r_{1}^{5}} + 3\\mu\\frac{x-\\mu+1}{r_{2}^{5}}\\right)\\\\ \\Omega_{yy} &= 1 - \\frac{1-\\mu}{r_{1}^{3}} - \\frac{\\mu}{r_{2}^{3}}+ y^{2}\\left(\\frac{3(1-\\mu)}{r_1^5}+ \\frac{3\\mu}{r_2^{5}}\\right)\\end{align*}\\) \\)","title":"For \\(L_{1}, L_{2}, L_{3}\\)"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.1.%20The%203%20body%20problem/#lemma","text":"\\[\\begin{align*} \\Omega_{xx}(L_{i}) &= 1 + 2\\frac{1-\\mu}{|x-\\mu|^{3}} + \\frac{2\\mu}{|x-\\mu+1|^{3}}>0 \\\\ \\Omega_{xy}(L_{i}) &= 0\\\\ \\Omega_{yy}(L_{i}) &< 0 \\qquad \\textrm{(To be checked numerically)} \\end{align*}$$ So the characteristic equation becomes $$\\lambda^{4} + (4-\\Omega_{xx}(L_{i}) - \\Omega_{yy}(L_{i}))\\lambda^{2} + \\Omega_{xx}(L_{i})\\Omega_{yy}(L_{i}) = 0\\] Now, we define: \\( \\(\\begin{cases} \\beta_{1} &= 2 - \\frac{(\\Omega_{xx}+\\Omega_{yy})|_{L_{i}}}{2} \\quad\\longleftarrow 2\\beta_{1} = 4-(\\Omega_{xx}+\\Omega_{yy})|_{L_{i}} \\\\ \\beta_{2}^{2} &=-(\\Omega_{xx}\\cdot\\Omega_{yy})|_{L_{i}} \\end{cases}\\) \\) and we can rewrite the characteristic equation as: \\( \\(\\lambda^{4} + 2\\beta_{1}\\lambda^{2}-\\beta_{2}^{2}= 0 \\Rightarrow \\lambda^{2}= -\\beta_{1} \\pm \\sqrt{\\beta_{1}^{2}+ \\beta_{2}^{2}}\\) \\) Since \\(|\\beta_{1}^{2}+ \\beta_{2}^{2}| > |\\beta_{1}|\\) , it means that we will have that the square root is bigger so we have two kind of solutions: \\( \\(\\lambda^{2}= \\begin{cases} A > 0 \\\\ B < 0 \\end{cases}\\) \\) and thus \\( \\(\\Rightarrow \\lambda = \\begin{cases} \\quad\\sqrt{A} > 0 \\\\ - \\sqrt{A} < 0 \\\\ \\quad \\sqrt{-B}i \\\\ -\\sqrt{-B}i \\end{cases}\\) \\) This is going to be called center x saddle , center for the imaginary eigenvalues and and saddle for the real, positive and negative points. (check picture). So \\(\\forall u \\in (0, \\frac{1}{2})\\) , \\(L_{1}, L_{2}, L_{3}\\) are unstable for the original nonlinear system (the RTBP). For any \\(\\mu \\in (0, \\frac{1}{2}]\\) , there exists: - a 1d unstable manifold \\(W^{u}(L_{i})\\) - a 1d stable manifold \\(W^{s}(L_{i})\\)","title":"Lemma"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.1.%20The%203%20body%20problem/#for-l_4-l_5","text":"For \\(L_{4, 5} = (\\mu-\\frac{1}{2}, \\pm \\frac{\\sqrt{3}}{2}, 0, 0)\\) , we compute: \\( \\(\\begin{align*} \\Omega_{xx}(L_{4}) = \\Omega_{xx}(L_{5})= \\frac{3}{4}\\\\ \\Omega_{yy}(L_{4}) = \\Omega_{yy}(L_{5})= \\frac{9}{4}\\\\ \\Omega_{xy}(L_{4}) = \\frac{3\\sqrt{3}}{2}\\left(\\mu - \\frac{1}{2}\\right)= - \\Omega_{xy}(L_{5}) \\end{align*}\\) \\) checking this is an exercise We obtain \\( \\(\\lambda^{4}+ \\lambda^{2}+ \\frac{27}{4}\\mu (1-\\mu) = 0\\) \\) so that \\( \\(\\lambda^{2}= \\frac{1}{2}\\left(-1 \\pm \\sqrt{1 - 27\\mu(1-\\mu)}\\right)\\) \\) Now let's discuss the sign of the inside of the root, which we will define as: \\( \\(f(\\mu) = 1-27\\mu(1- \\mu) = 27\\mu^{2}-27\\mu+1, \\quad \\mu\\in (0, 1/2]\\) \\) We are interested in the point where the graph crosses the x-axis: \\(\\mu_{R}\\) : \\( \\(f(\\mu_{R}) = 0 \\rightarrow 27\\mu^{2} - 27\\mu+1 = 0 \\Rightarrow \\mu_{R}=\\frac{1}{2}\\left(1\\pm\\frac{\\sqrt{69}}{9}\\right)\\) \\) since \\(\\mu \\in (0, 1/2]\\) , then we take the minus part: \\( \\(\\mu_{R}=\\frac{1}{2}\\left(1-\\frac{\\sqrt{69}}{9}\\right) = 0.038...\\) \\) We distinguish 3 cases:","title":"For \\(L_{4}, L_{5}\\)"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.1.%20The%203%20body%20problem/#case-1-0mumu_r","text":"\\( \\(\\lambda^{2}= \\begin{cases} \\Lambda_{1} < 0 \\\\ \\Lambda_{2} < 0 \\end{cases} \\Rightarrow \\lambda = \\begin{cases} \\pm \\sqrt{|\\Lambda_{1|}}i = \\pm w_{1}i \\\\ \\pm \\sqrt{|\\Lambda_{2|}}i = \\pm w_{2}i \\end{cases}\\) \\) with \\( \\(\\begin{align*} w_{1}= \\left(\\frac{1}{2}|-1 - \\sqrt{1 - 27\\mu(1-\\mu)}|\\right)^{\\frac{1}{2}}= \\frac{1}{\\sqrt{2}}\\sqrt{1 + \\sqrt{1 - 27\\mu(1-\\mu)}}\\\\ w_{2}= \\left(\\frac{1}{2}|-1 + \\sqrt{1 - 27\\mu(1-\\mu)}|\\right)^{\\frac{1}{2}}= \\frac{1}{\\sqrt{2}}\\sqrt{1 - \\sqrt{1 - 27\\mu(1-\\mu)}} \\end{align*}\\) \\) so that \\(\\omega_{1}^{2}> \\frac{1}{2}>\\omega_{2}^{2}\\) Picture with plot, all points in the y-axis![[2.1. The 3 body problem 2022-11-15 12.31.55.excalidraw]] In this case we have a center x center : we have no info about the stability for the RTBP.","title":"Case 1: \\(0&lt;\\mu&lt;\\mu_{R}\\)"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.1.%20The%203%20body%20problem/#case-2-mumu_r","text":"\\(\\lambda^{2}= \\frac{-1}{2}\\Rightarrow \\lambda = \\pm \\frac{1}{\\sqrt{2}}i\\) Picture with plot, all points in the y-axis This is a degenerate center x center ( \\(\\omega_{1}= \\omega_{2}=\\omega\\) )","title":"Case 2: \\(\\mu=\\mu_{R}\\)"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.1.%20The%203%20body%20problem/#case-3-mu_rmu12","text":"\\[\\lambda^{2}= \\frac{-1\\pm ai}{2} \\Rightarrow \\lambda = \\begin{cases} \\quad\\lambda_{1} \\\\ \\quad\\bar{\\lambda}_{1} \\\\ -\\lambda_{2} \\\\ -\\bar{\\lambda}_{2} \\end{cases} \\] This is a complex saddle : -There exists a 2d unstable manifold \\(W^{u}(L_{4,5})\\) -There exists a 2d stable manifold \\(W^{s}(L_{4,5})\\) It's unstable.","title":"Case 3: \\(\\mu_{R}&lt;\\mu&lt;1/2\\)"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.1.%20The%203%20body%20problem/#how-to-compute-it-numerically","text":"We have a tangent plane, we take a circle of initial conditions around the point and integrate forward/backward in time.","title":"How to compute it numerically?"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.1.%20The%203%20body%20problem/#remark","text":"Another very important consequence of the type of eigenvalues is the existence of periodic orbits, guaranteed by Lyapunov Theorem:","title":"Remark"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.1.%20The%203%20body%20problem/#lyapunov-theorem","text":"Ref Meyer-Hall-Offin book Assume that a system of ODE: \\( \\(\\vec{x}' = \\vec{f}(\\vec{x}), \\quad \\vec{x}\\in U\\subset \\mathbb{R}^{n}\\) \\) has a nondegenerate first integral \\(H(\\vec{x})\\) (that is, \\(\\nabla H(\\vec{r}) \\neq 0\\) for \\(\\vec{r}\\) a regular point), and an equilibrium point with chaaracteristic exponents \\(\\pm i\\omega, \\lambda_{3}, \\dots, \\lambda_{n}\\) . If \\(\\frac{\\lambda_{j}}{i\\omega} \\notin \\mathbb{Z}\\) , \\(j=3, \\dots, n\\) , then there exists a one-paramter family of periodic orbits emanating from the equilibrium point. Moreover, when approaching the equilibrium point along the family, the period tends to \\(\\frac{2\\pi}{\\omega}\\) . If I give the value of one parameter (which will be the first integral), then I can identify the orbit.","title":"Lyapunov Theorem"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.1.%20The%203%20body%20problem/#application-to-the-rtbp","text":"\\( \\(\\vec{X}' = \\vec{F}(\\vec{X}) \\quad \\vec{X}\\in U\\subset\\mathbb{R}^{4}\\) \\) First integral: \\(C(x, y, x', y') = 2\\Omega(x, y) - (x'^{2}+y'^{2})\\) ([[Jacobi first integral]]). *Check: \\(\\nabla C = (2 \\Omega_{x}, 2 \\Omega_{y}, -2x', -2y') = \\vec{0} \\Longleftrightarrow \\Omega_{x}= \\Omega_{y}=x'=y' = 0 \\Longleftrightarrow L_{i}, i=1, \\dots, 5\\)","title":"Application to the RTBP"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.1.%20The%203%20body%20problem/#for-l_1-l_2-l_4","text":"Eigenvalues: \\(\\pm i \\omega, \\pm \\lambda\\) , with \\(\\lambda \\in \\mathbb{R}\\) , we check \\( \\(\\frac{\\lambda}{i \\omega} \\notin \\mathbb{Z}\\) \\) since \\(\\lambda\\) is always real and divided by an imaginary number it will never be an integer number. \\(\\Rightarrow\\) for any value of \\(\\mu\\) , there exists a one-parameter family of periodic orbits emanating at \\(L_{1,2,3}\\) and the period of the periodic orbits tend to \\(\\frac{2\\pi}{\\omega}\\) (when approaching \\(L_{i}\\) ) Check picture","title":"For \\(L_{1}, L_{2}, L_{3}\\)"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.1.%20The%203%20body%20problem/#for-l_4-l_6","text":"Case: \\(\\mu = \\mu_{R}\\) , then \\(\\omega_{1}=\\omega_{2}=\\omega\\) , so the coefficient will be 1, an integer number. Case: \\(\\mu_{R}<\\mu<1/2\\) we don't have exponents in the form of \\(\\pm i \\omega\\) , so I cannot apply the formula. Case : \\(0 < \\mu < \\mu_{R}\\) eigenvalues \\(\\pm i\\omega_{1}, \\pm i\\omega_{2}\\) ( \\(\\omega_{1}>\\omega_{2}, \\, \\omega_{1}^{2}> \\frac{1}{2}>\\omega_{2}^{2}\\) ). First case We have \\(0 <\\frac{ \\pm i \\omega_{2}}{i\\omega_{1}} = \\frac{\\omega_{2}}{\\omega_{1}} < 1\\) (since \\(\\omega_{1}>\\omega_{2}\\) ) , then \\(\\frac{\\pm i \\omega_{2}}{i\\omega_{1}} \\notin \\mathbb{Z} \\Rightarrow\\) There exists a one-parameter family of periodic orbits emanating at \\(L_{4}\\) and \\(L_{5}\\) and the period tends to \\(2\\pi/\\omega_{1}\\) : the so called family of Lyapunov periodic orbits of short period . Second case We have \\(\\frac{ \\pm i \\omega_{2}}{i\\omega_{1}} = \\frac{\\omega_{2}}{\\omega_{1}} \\notin \\mathbb{Z}\\) Let us show that this is true for \\(\\mu \\neq \\mu_r\\) , \\(\\mu_{r}=\\frac{1}{2}-\\frac{1}{2}\\left(\\frac{1-16r^2}{27(r^{2}+1)^{2}}\\right)^{\\frac{1}{2}}\\) , for \\(r=1, 2, 3...\\) then \\(\\frac{\\omega_{1}}{\\omega_{2}\\notin}\\mathbb{Z}\\) . We assume now \\(\\frac{\\omega_{1}}{\\omega_{2}}\\) , which is equivalent to \\( \\(\\frac{\\frac{1}{\\sqrt{2}}\\sqrt{1+x}}{\\frac{1}{\\sqrt{2}}\\sqrt{1-x}}= r\\) \\) where \\(x = \\sqrt{1-27\\mu(1-\\mu)}\\) Same as, \\( \\(\\frac{1+x}{1-x}=r^{2} \\Leftrightarrow x = \\frac{r^{2}-1}{r^{2}+1} \\Leftrightarrow \\mu^{2}-\\mu + \\frac{4r^{2}}{27(r^{2}+1)^{2}}=0\\) \\) same as \\( \\(\\mu = \\frac{1}{2}\\left(1 \\pm \\sqrt{1- \\frac{16r^{2}}{27(r^{2}+1)}}\\right)\\) \\) Since \\(\\mu\\) must \\(\\mu\\in(0, 1/2]\\) , then it means that the values of \\(\\mu\\) for which we have an integer ratio are \\( \\(\\mu_{r} = \\frac{1}{2} - \\frac{1}{2}\\left(1- \\frac{16r^{2}}{27(r^{2}+1)}\\right)^\\frac{1}{2}\\) \\) If \\(\\mu \\neq \\mu_{r}\\) , \\(r=1, 2...\\) then there exists a one-parameter family of Lyapunov periodic orbits emanating at \\(L_{4,5}\\) and the period tends to \\(\\frac{2\\pi}{\\omega_{2}}\\) when approaching \\(L_{4,5}\\) : the so called Lyapunov family of periodic orbits of long period . So, when \\(\\mu\\) is different from \\(\\mu_{r}\\) , then the ratio won't be an integer ratio. We will demonstrate the opposite.","title":"For \\(L_{4}, L_{5}\\)"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.1.%20The%203%20body%20problem/#recap-computation-of-unstable-manifolds-taking-the-linear-approximation","text":"![[Unstable_manifold_computation]] We have a circle of initial conditions: \\( \\(\\begin{cases} s_{1} &= r \\cos{\\theta} \\\\ s_{2} &= r \\sin{\\theta} \\end{cases}\\) \\) for \\(\\theta \\in [0, 2\\pi]\\) and \\(r\\) small and for each initial condition we integrate forward in time. However, we will see a more precise way to do this.","title":"Recap: Computation of unstable manifolds taking the linear approximation"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.2%20Expansion%20of%20parametrizations%20of%20invariant%20manifolds%20%28for%20flows%29/","text":"Motivation: [[Motivation for expansion of parametrizations of invariant manifolds (for flows)]] 2.2. Expansion of parametrizations of invariant manifolds (for flows) \u00b6 Reference : The parametrization method for invariant manifolds Alex Haro, Marta Canadell, Jordi Llu\u00eds Figueras, ALejandro Luque, Josep M. Mondelo. Theoretical background : Xavier Cabr\u00e9, Rafel de la Llave, Ernest Fontich (3 papers) Motivation \u00b6 Assume \\(Z' = G(Z)\\) as system of \\(n\\) ODE, \\(Z \\in U \\subset \\mathbb{R}^{n}\\) , \\(Z_0\\) an equilibrium point such that the eigenvalues of \\(DG(Z_0)\\) are \\(\\lambda_{1}, \\lambda_{2}, \\dots, \\lambda_{n}\\) with \\(\\lambda_{1}, \\lambda_{2} > 0\\) . We know, thus, there exists a 2d unstable invariant manifold. For the linearized system, we take the \\(\\vec{v}_{1}^{u}, \\vec{v}_{2}^{u}\\) eigenvectors. If \\(\\lambda_{2}> \\lambda_{1} > 0\\) , then \\(\\lambda_{2}\\) is the fast dimension. The whole plane is the unstable manifold. ![[linear_Wu|200]] For the non-linear original system, we have a 2d invariant manifold, \\(W^{u}\\) . \\( \\(Z_{0} = Z_{0} + s_{1}\\vec{v}_{1}^{u} + s_{2}\\vec{v}_{2}^{u}\\) \\) with \\( \\(\\begin{cases} s_{1} &= r\\cos{\\theta} \\\\ s_{2} &= r\\sin{\\theta} \\end{cases}\\) \\) with \\(r\\) very small and integrate forward in time. That's how we typically do it. See [[Motivation for expansion of parametrizations of invariant manifolds (for flows)]] and Jupyter Notebook for more. Goal \u00b6 To obtain a high order power expansion for a parametrization of invariant manifolds of an equilibrim point of a vector field. Let us start \\(Z' = G(Z)\\) , \\(Z_0\\) equilibrium point, \\(Z \\in U \\subset \\mathbb{R}^{n}\\) . Assume \\(G(Z_{0)}= 0\\) , and the following eigenvalues: \\( \\(P^{-1}DG(Z_{0})P = \\begin{pmatrix}\\lambda_{1} & \\dots \\\\ & \\ddots & \\\\ & \\cdots & \\lambda_{n}\\end{pmatrix}\\) \\) We do a change of variables: \\(z = Z_{0} + P \\cdot z\\) . We obtain \\( \\(z' = F(z),\\) \\) where \\(F(z) = P^{-1}G(Z_{0}+P\\cdot z)\\) . \\(Z' = P z' \\Rightarrow z' = P^{-1}Z' = P^{-1}G(Z) = P^{-1}G(Z_{0}+ P\\cdot z)\\) Now \\(F(0) = 0\\) , \\(DF(0) = P^{-1}DG(Z_{0)P}= \\begin{pmatrix}\\lambda_{1} & \\dots \\\\ & \\ddots & \\\\ & \\cdots & \\lambda_{n}\\end{pmatrix}\\) My input used to be \\(Z_{0}, G\\) and now it's \\(F\\) . We consider the Taylor expansion: \\( \\(F(z) = DF(0)z + F_{2}(z) + F_{3}(z) + \\dots\\) \\) and this will be my input. Notice that \\(F_{2}, F_{3}\\) are the 2nd, 3rd order terms. Now we have this input and we want to compute the invariant manifold of this zero. We choose the first \\(d\\) eigenvalues of \\(DF(0)\\) (we can always reorder them) and let \\(V^{d}\\) be a \\(d\\) dimensional linear subspace spanned by the \\(d\\) eigenvectors. (Assume, for example, that \\(\\lambda_{1\\dots d}>0\\) \\(\\Rightarrow\\) there exists a \\(d\\) -dimensional unstable invariant manifold \\(\\mathfrak{W}^u\\) (this is a gothic W) ) Our goal is to obtain a high order approximation of the \\(d\\) -dimensional manifold \\(\\mathfrak{W}^u\\) for system \\(z' = F(z)\\) tangent to \\(V^{d}\\) In our example, \\(W(s) = \\begin{pmatrix}v_{1}^{1} & v_{1}^{2} & \\dots & v_{1}^{d} \\\\ \\vdots & \\vdots & & \\vdots \\\\ v_{n}^{1} & v_{n}^{2} & \\dots & v_{n}^{d}\\end{pmatrix} \\begin{pmatrix}s_{1} \\\\ \\vdots \\\\ s_{d}\\end{pmatrix} = s_{1}\\vec{v}^{1}+\\dots +s_{d}\\vec{v}^{d} = \\begin{pmatrix}v_{1}^{1} & v_{1}^{2} & \\dots & v_{1}^{d} & \\cdots \\\\ \\vdots & \\vdots & & \\vdots & \\\\ v_{n}^{1} & v_{n}^{2} & \\dots & v_{n}^{d} & \\cdots\\end{pmatrix}\\begin{pmatrix}s_{1} \\\\ \\vdots \\\\ s_{d} \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{pmatrix}\\) where the last matrix is \\(P\\) . With the original variables \\(W(s) = Z_{0}+ P(\\begin{pmatrix}s_{1} \\\\ \\vdots \\\\ s_{d} \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{pmatrix}) = Z_{0}+s_{1}\\vec{v}^{1}+\\dots +s_{d}\\vec{v}^{d}\\) . This is an approximation for parametrizing the plane. But we are not interested in the plane now, only in the \"border\". ![[parametrization_nmds|200]] More concretely, we look for a parametrization \\( \\(\\begin{align*} W: \\tilde{U}\\subset \\mathbb{R}^{d} &\\longrightarrow \\mathbb{R}^n\\\\ s &\\longrightarrow W(s) = z \\end{align*}\\) \\) of the invariant manifold \\(\\mathfrak{W}\\) where \\(s = (s_{1}, \\dots, s_{d})\\) are the coordinates (parameters) of the parametrization and \\(W(0) = 0\\) . The internal dynamics on the manifold is described by a vector field \\( \\(s' = f(s)\\) \\) I have two flows, one for \\(z' = F(z)\\) , \\(\\Phi_{t}\\) and one for \\(s'=f(s)\\) , \\(\\phi_{t}\\) , with \\(f(0) = 0\\) . So, we look for \\(W\\) and \\(f\\) such that is invariant by the flow of \\(F\\) : 1. \\(\\{W(s)\\}_{s\\in \\tilde{W}\\subset \\mathbb{R}^{d}}\\) is invariant by the flow of \\(F\\) . 2. the internal dynamics of \\(s\\) is given by \\(s' =f(s)\\) That is, we want \\( \\(\\Phi_{t}(W(\\tilde{s})) = W(\\phi_{t}(\\tilde{s}))\\) \\) Let us take \\(\\frac{d}{dt}\\) : \\( \\(F(\\Phi_{t}(W(\\tilde{s}))) = DW(\\phi_{t}(\\tilde{s}))\\cdot f(\\phi_{t}(\\tilde{s}))\\) \\) we have that \\(\\phi_{t}(\\tilde{s}) = s\\) and \\(\\Phi_{t}(W(\\tilde{s})) = W(s)\\) . that is we want \\(W(s)\\) , \\(f(s)\\) such that Def: Invariance equation \u00b6 \\( \\(F(W(s)) = DW(s)f(s)\\) \\) And, conversely, if the invariance equation is true, then (IC) is also tre (It can be proved). So, what we will do is to solve the invariance equation. A geometrical interpretation ![[geometrical_interpretation]] Unknowns: \\(W(s), f(s)\\) and known is \\(F\\) The parametrization method \u00b6 Date : 16-11-2022 We have \\( \\(\\begin{align*} Z' &= G(Z), \\quad Z_{0} \\textrm{: an equilibrim point}\\\\ Z &= Z_{0}+Pz \\end{align*}\\) \\) and we transform it so that \\( \\(z' = F(z)\\) \\) with 0 an equilibrium point: \\(F(0) = 0\\) , \\(DF(0) = \\begin{pmatrix}\\lambda_{1} & & \\\\ & \\ddots & \\\\ & & \\lambda_{n}\\end{pmatrix}\\) And we will do the expansion: \\( \\(F(z) = DF(0)z + F_{2}(z) + F_{3}(z) + \\ldots = F_{1}(z) + F_{2}(z) + F_{3}(z) + \\ldots\\) \\) and \\(F_{1}=\\begin{pmatrix}\\lambda_{1}z_{1} \\\\ \\vdots \\\\ \\lambda_{n}z_{n}\\end{pmatrix} + \\sum\\limits_{k\\geq2}F_{k}(z)\\) For example \\( \\(DF(0) = \\begin{pmatrix}\\lambda_{1}>0 & & & & \\\\ & \\ddots & & & \\\\ & & \\lambda_{d}>0 & &\\\\ & & &\\lambda_{d+1}<0 & & &\\\\ & & & &\\ddots & & \\\\ & & & & & &\\lambda_{n}<0\\end{pmatrix}\\) \\) There exists a d-dimensional unstable manifold Specific goal : we look for a d-dim manifold, so we want: \\( \\(W(s) = \\sum\\limits_{k\\geq1}W_{k}(s) = \\begin{pmatrix}s_{1} \\\\ \\vdots \\\\ s_{d} \\\\ 0 \\\\ \\vdots \\\\ 0\\end{pmatrix} + \\sum\\limits_{k\\geq2}W_{k}(s); \\qquad f(s) = \\begin{pmatrix}\\lambda_{1}s_{1} \\\\ \\vdots \\\\ \\lambda_{d}s_{d} \\end{pmatrix} + \\sum\\limits_{k\\geq2}f_{k}(s)\\) \\) where \\((s_{1}, \\ldots, s_{d}, 0, \\ldots, 0)\\) is \\(W_{1}(s)\\) and \\(W(0) = 0\\) , \\(f_{1} = (\\lambda_{1}s_{1}, \\dots)\\) and \\(f(0) = 0\\) that are solution of the invariance equation (eq 4) \\( \\(F(W(s)) = DW(s)\\cdot f(s)\\) \\) which comes from \\(\\phi_{t}(W(s)) = W(\\phi_{t}(s))\\) . Remark 1 \u00b6 Taking \\(W(s)\\) and \\(f(s)\\) as above the invariance equation (4) is satisfied at first order: \\( \\([F(W(s))]_{1} = F_{1}(W_{1}(s)) = F_{1}\\begin{pmatrix}s_{1} \\\\ \\vdots \\\\ s_{d} \\\\ 0 \\\\ \\vdots \\\\ 0\\end{pmatrix} = \\begin{pmatrix}\\lambda_{1}s_{1} \\\\ \\vdots \\\\ \\lambda_{d}s_{d} \\\\ 0 \\\\ \\vdots \\\\ 0\\end{pmatrix}\\) \\) \\( \\([DW(s)f(s)]_{1}=DW_{1}(s)f_{1}(s) = \\begin{pmatrix}I_{d\\times d \\\\ 0}\\end{pmatrix}\\begin{pmatrix}\\lambda_{1}s_{1} \\\\ \\vdots \\\\ \\lambda_{d}s_{d}\\end{pmatrix} = \\begin{pmatrix}\\lambda_{1}s_{1} \\\\ \\vdots \\\\ \\lambda_{d}s_{d} \\\\ 0 \\\\ \\vdots \\\\ 0\\end{pmatrix}\\) \\) so \\([F(W(s))]_{1} = [DW(s)f(s)]_{1}\\) We know that \\(DW_{1}(s)\\) is the derivative of the vector \\(W_{1}\\) , which is just the \\(s\\) variables and zeros with respect to \\(s\\) , so we have \\(DW_{1}(s) \\in \\mathbb{R}^{n\\times s}\\) and we will have an identity matrix stacked with a zero matrix Order k of \\(f\\) means order \\(k\\) of z means a homogeneous polynomial of degree k. \\([F]_1\\) means expansion of order 1 Remark 2 \u00b6 Once we have done the procedure and we have obtained the parametrization \\(W(s)\\) up to a given order, that is \\(z = W(s)\\) we apply the change of variables in order to obtain the parametrization in original variables: \\( \\(W^{\\textrm{orig var}}(s) = Z_{0}+P\\cdot W(s)\\) \\) So we are only gonna focus on \\(W(s)\\) Remark 3 \u00b6 Example 1d For example \\(Z' =G(Z)\\) , \\(Z_{0}\\) with \\(DF(0) = \\begin{pmatrix}\\lambda_{1}>0 & & & \\\\ \\lambda_{2} & & \\\\ & & \\ddots & \\\\ & & & \\lambda_{n}\\end{pmatrix} \\Rightarrow \\exists\\) a 1d unstable manifold \\(W^u\\) \\( \\(P = \\begin{pmatrix}v_{1}^{1} & \\cdots & \\cdots \\\\ \\vdots & \\cdots & \\cdots \\\\ v_{n}^{1} & \\cdots & \\cdots \\end{pmatrix}\\) \\) if we take \\(W(s)\\) up to first order: \\(W(s) \\approx W_{1}(s) = \\begin{pmatrix}s_{1} \\\\ 0 \\\\ \\vdots \\\\ 0\\end{pmatrix}\\) then in the original variable \\( \\(W^{\\textrm{orig variable}}(s) \\approx Z_{0}+PW_{1}(s) = Z_{0}+\\begin{pmatrix}v_{1}^{1} & \\cdots & \\cdots \\\\ \\vdots & \\cdots & \\cdots \\\\ v_{n}^{1} & \\cdots & \\cdots \\end{pmatrix}\\begin{pmatrix}s_{1} \\\\ 0 \\\\ \\vdots \\\\ 0\\end{pmatrix} = Z_{0}+s_{1}v^{1}\\) \\) which is what we've done so far. Typically for linear approximations \\(s\\in(0, 10^{-6})\\) Example 2d \\(Z' =G(Z)\\) , \\(Z_{0}\\) with \\(DF(0) = \\begin{pmatrix}\\lambda_{1}>0 & & & \\\\ \\lambda_{2}>0 & & \\\\ & & \\ddots & \\\\ & & & \\lambda_{n}\\end{pmatrix} \\Rightarrow \\exists\\) a 2d unstable manifold \\(W^u\\) \\( \\(P = \\begin{pmatrix}v_{1}^{1} & v_{1}^2 & \\cdots & \\cdots \\\\ \\vdots & \\vdots & \\cdots & \\cdots \\\\ v_{n}^{1} & v_{n}^2 & \\cdots & \\cdots \\end{pmatrix}\\) \\) if we take \\(W(s)\\) up to first order: \\(W(s) \\approx W_{1}(s) = \\begin{pmatrix}s_{1} \\\\ s_{2} \\\\ \\vdots \\\\ 0\\end{pmatrix}\\) then in the original variable \\( \\(W^{\\textrm{orig variable}}(s) \\approx Z_{0}+PW_{1}(s) = Z_{0}+\\begin{pmatrix}v_{1}^{1} & v_{1}^{2} & \\cdots & \\cdots \\\\ \\vdots & \\vdots & \\cdots & \\cdots \\\\ v_{n}^{1} & v_{n}^{2} & \\cdots & \\cdots \\end{pmatrix}\\begin{pmatrix}s_{1} \\\\ s_{2} \\\\ 0 \\\\ \\vdots \\\\ 0\\end{pmatrix} = Z_{0}+sv^{1} + s_{2}v^{2}\\) \\) which is what we have done so far. Resolution of the invariance equation (up to a high order) \u00b6 \\[F(W(s)) = DF(W(s))f(s)\\] Notation \u00b6 We know \\(F(z)\\) (this is my input) where \\(F_{k}\\) means \\( \\(F_{k}^{i}=\\sum\\limits_{|l|=k=l_{1}+\\dots + l_{n}} F^{i}_{k,l}z^{l}=\\sum\\limits_{|l| = k}F^{i}_{k,l} z_{1}^{l_{1}} \\cdots z_{n}^{l_{n}}\\) \\) For the \\(k\\) term, we have \\(n\\) components. Each component \\(i\\) is a homogeneous polynomial of degree \\(k\\) in variables \\(z_{1, \\dots, n}\\) . \\( \\(W_{k}^{i}(s) = \\sum\\limits_{|m|=k=m_{1}+\\dots + m_{d}} W^{i}_{k,m}s^{m}=\\sum\\limits_{|m| = k}W^{i}_{k,l} s_{1}^{m_{1}} \\cdots s_{d}^{m_{d}} \\qquad i=1, \\dots, n\\) \\) For \\(W(s)\\) , each term of the expansion \\(W_{k}\\) will have \\(n\\) components and each component is a homogeneous polynomial of degree \\(k\\) . \\( \\(f_{k}^{i}(s) = \\sum\\limits_{|m|=k} f^{i}_{k,m}s^{m}=\\sum\\limits_{|m| = k}f^{i}_{k,l} s_{1}^{m_{1}} \\cdots s_{d}^{m_{d}} \\qquad i=1, \\dots, d\\) \\) and \\(f\\) will have d components. Another notation \u00b6 \\(z = (x, y)\\) where \\(x\\) is the first \\(d\\) components and \\(y\\) are the following \\(n-d\\) components. \\(\\lambda^{x}\\) is the first \\(d\\) eigenvalues \\((\\lambda^{x}, m) = \\lambda_{1}m_{1}+\\dots +\\lambda_{d}m_{d}\\) is the scalar product. Resolution \u00b6 We want to solve the invariance equation at order \\(k\\) assuming that it is satisfied up to order \\(k-1\\) , that is, - we know \\(W_{1}, \\dots, W_{k-1}, f_{1}, \\dots, f_{k-1}\\) - we want \\(W_{k}, f_{k}\\) , that is \\( \\(W_{k}=\\begin{pmatrix}W_{k}^{1} \\\\ \\vdots \\\\ W_{k}^{n}\\end{pmatrix} \\quad f_{k}=\\begin{pmatrix}f_{k}^{1} \\\\ \\vdots \\\\ f_{k}^d\\end{pmatrix}\\) \\) \\( \\([F(W(s))]_{k}=[DW(s)f(s)]_{k}\\) \\) LHS \u00b6 Let us consider the left hand side terms of order \\(k\\) \\( \\(\\begin{align*} [F(W(s))]_{k}&= [F(W_{\\leq k})]_{k}=[F(W_{<k} + W_{k})]_{k} =(taylor) = [F(W_{<k})]_{k}+DF(0)W_{k} \\\\ &= [F(W_{<k})]_{k} + \\begin{pmatrix}\\lambda_{1} & & \\\\ & \\ddots & \\\\ & & \\lambda_n\\end{pmatrix}\\begin{pmatrix}W_{k}^1\\\\ \\vdots \\\\ W_{k}^n\\end{pmatrix}\\\\ &= [F(W_{<k})]_{k} + \\begin{pmatrix}\\lambda_{1}W_{k}^{1}\\\\ \\vdots\\\\ \\lambda_{n}W_{k}^{n}\\end{pmatrix}\\\\ &= (I) + (II)\\end{align*}\\) \\) where \\((I)\\) is known and \\((II)\\) is unknown check at home the first line RHS \u00b6 Let us consider the right hand side \\( \\(\\begin{align*} [DW(s)f(s)]_{k} &= \\sum\\limits_{l=0}^{k}[DW]_{l}\\cdot [f]_{k-l}= \\sum\\limits_{l=0}^{k-1}DW_{l+1}\\cdot f_{k-l} \\\\ &=DW_{1}f_{k} + DW_{k}f_{1}+\\sum\\limits_{l=1}^{k-2}DW_{l+1}f_{k-l}\\\\ &=DW_{1}f_{k} + DW_{k}f_{1}+\\sum\\limits_{\\tilde{l}=2}^{k-1}DW_{\\tilde{l}}f_{k-\\tilde{l}+1}\\\\ &= DW_{1}f_{k} + DW_{k}f_{1}+\\sum\\limits_{{l}=2}^{k-1}DW_{{l}}f_{k-{l}+1}\\\\ &= \\begin{pmatrix}I\\\\ 0\\end{pmatrix}f_{k} + DW_{k}\\begin{pmatrix}\\lambda_1s_1\\\\ \\vdots\\\\ \\lambda_{d}s_{d}\\end{pmatrix}+\\sum\\limits_{{l}=2}^{k-1}DW_{{l}}f_{k-{l}+1}\\\\ &= \\begin{pmatrix}f_{k}\\\\ 0\\end{pmatrix} + DW_{k}\\begin{pmatrix}\\lambda_1s_1\\\\ \\vdots\\\\ \\lambda_{d}s_{d}\\end{pmatrix}+\\sum\\limits_{{l}=2}^{k-1}DW_{{l}}f_{k-{l}+1} = (III) + (IV) + (V) \\end{align*}\\) \\) where \\((III), (IV)\\) are unknown (because of \\(f_{k}, DW_{k}\\) ) and \\((V)\\) is known. The first equal is due to \\(DW, f\\) being homogeneous polynomials. The second equal is due to \\([DW]_{l}=DW_{l+1}\\) . Then we also apply that \\(f_{0}=0\\) . And \\(f_{k}, W_{k}\\) are unknown. RHS and LHS \u00b6 \\[(I) + (II) = (III) + (IV) + (V) \\Longleftrightarrow (IV)-(II)+(III)=(I)-(V)$$ which means: $$ DW_{k}\\begin{pmatrix}\\lambda_1s_{1}\\\\ \\vdots\\\\ \\lambda_{d}s_{d}\\end{pmatrix} - \\begin{pmatrix}\\lambda_{1}W_{k}^{1}\\\\ \\vdots\\\\ \\lambda_{n}W_{k}^{n}\\end{pmatrix}+ \\begin{pmatrix}f^{1}_{k}\\\\ \\vdots \\\\ f^{d}_{k} \\\\ 0 \\\\ \\vdots \\\\ 0\\end{pmatrix} = [F(W_{<k})]_{k}+\\sum\\limits_{{l}=2}^{k-1}DW_{{l}}f_{k-{l}+1} = R_{k} \\textrm{ (known)}$$ Now we expand $W, f, R$ in monomials and notice $$\\begin{align*} W_{k}(s) &= \\sum\\limits_{|m|=k}W_{k,m}s_{1}^{m_{1}}\\dots s_{d}^{m_{d}}\\\\ \\partial_{1}W_{k} &= \\sum\\limits_{|m|=k}W_{k,m}s_{1}^{m_{1}-1}\\dots s_{d}^{m_{d}}\\\\ \\vdots\\\\ \\partial_{d}W_{k} &= \\sum\\limits_{|m|=k}W_{k,m}s_{1}^{m_{1}}\\dots s_{d}^{m_{d}-1}\\\\ \\end{align*}$$ and now $$\\begin{align*} DW_{k}\\begin{pmatrix}\\lambda_1s_{1}\\\\ \\vdots\\\\ \\lambda_{d}s_{d}\\end{pmatrix} &= \\begin{pmatrix}\\sum\\limits_{|m|=k}W_{k,m}s_{1}^{m_{1}-1}\\dots s_{d}^{m_{d}} & \\dots & \\sum\\limits_{|m|=k}W_{k,m}s_{1}^{m_{1}}\\dots s_{d}^{m_{d}-1}\\end{pmatrix}\\begin{pmatrix}\\lambda_1s_{1}\\\\ \\vdots\\\\ \\lambda_{d}s_{d}\\end{pmatrix}\\\\ &= \\begin{pmatrix}\\sum\\limits_{|m|=k}W^{1}_{k,m}s_{1}^{m_{1}-1}\\dots s_{d}^{m_{d}} & \\dots & \\sum\\limits_{|m|=k}W^{1}_{k,m}s_{1}^{m_{1}}\\dots s_{d}^{m_{d}-1}\\\\ \\vdots & \\cdots & \\vdots\\\\ \\sum\\limits_{|m|=k}W^{n}_{k,m}s_{1}^{m_{1}-1}\\dots s_{d}^{m_{d}} & \\dots & \\sum\\limits_{|m|=k}W^{n}_{k,m}s_{1}^{m_{1}}\\dots s_{d}^{m_{d}-1}\\end{pmatrix}\\begin{pmatrix}\\lambda_1s_{1}\\\\ \\vdots\\\\ \\lambda_{d}s_{d}\\end{pmatrix}\\\\ &= \\sum\\limits\\begin{pmatrix}W_{k,m}^{1}(m_{1}\\lambda_{1}+ \\dots + m_d\\lambda_{d}s^{m}\\\\ \\ldots \\\\ W_{k,m}^{n}(m_{1}\\lambda_{1}+ \\dots + m_d\\lambda_{d}s^{m}\\end{pmatrix} = \\sum\\limits \\begin{pmatrix}W_{k,m}^{i}\\\\ \\vdots\\\\ W_{k,m}^{n}\\end{pmatrix}(\\lambda^{x}, m)s^{m}\\end{align*}\\] Date: 22-11-2022 Recap: \u00b6 \\( \\(\\begin{align*} Z' &= G(Z) & \\qquad(1)\\\\ Z &= Z_{0}+ Pz \\\\ z' &= F(z) & \\qquad(2) \\end{align*}\\) \\) with \\(Z_0\\) an equilibrium point, \\(F(0) = 0\\) and \\(DF(0) = \\begin{pmatrix}\\lambda_{1} & \\cdots & 0 \\\\ \\\\ 0 & \\cdots & \\lambda_{n}\\end{pmatrix}\\) . Taylor expansion : \\( \\(F(z) = F_{1}(z) + F_{2}(z) + \\dots = DF(0) + F_{2}+\\dots\\) \\) Invariance equation \\( \\(\\Phi_t(W(s))= W(\\phi_{t}(s))\\) \\) and derivative respect \\(t\\) : \\( \\(F(W(s)) = DW(s)f(s)\\) \\) Goal \\( \\(\\begin{align*} W(s) &= \\begin{pmatrix}s_{1} \\\\ \\vdots \\\\ s_{d} \\\\ 0 \\\\ \\vdots \\\\ 0\\end{pmatrix} + \\sum\\limits_{k\\geq2}W_{k}(s)\\\\ f(s) &= \\begin{pmatrix}\\lambda_{1}s_{1} \\\\ \\vdots \\\\ \\lambda_{d}s_{d} \\end{pmatrix} + \\sum\\limits_{k\\geq2}f_{k}(s) \\end{align*}\\) \\) and after: \\(W^{\\textrm{or var}}(s) = Z_{0}+ PW(s)\\) . Last session, we found: \\[DW_{k}\\begin{pmatrix}\\lambda_{1}s_{1} \\\\ \\vdots \\\\ \\lambda_{d}s_{d} \\end{pmatrix} - \\begin{pmatrix}\\lambda_{1}W_{k}^{1} \\\\ \\vdots \\\\ \\lambda_nW_{n}^{d} \\end{pmatrix} + \\begin{pmatrix}f_{k}^{1} \\\\ \\vdots \\\\ f_{k}^{d} \\\\ 0 \\\\ \\vdots \\\\ 0\\end{pmatrix} = R_k$$ And we can rewrite it as: $$\\sum\\limits_{|m|=k} \\begin{pmatrix}W_{k,m}^{i}(\\lambda^{x}, m)\\\\ \\vdots\\\\ W_{k,m}^{n}(\\lambda^{x}, m)\\end{pmatrix}s^{m} - \\sum\\limits_{|m|=k}\\begin{pmatrix}\\lambda_{1}W_{k}^{1} \\\\ \\vdots \\\\ \\lambda_nW_{n}^{d} \\end{pmatrix}s_{m}+ \\sum\\limits_{|m|=k}\\begin{pmatrix}f^{1}_{k,m} \\\\ \\vdots \\\\ f^{d}_{k,m} \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{pmatrix}s_{m} = \\sum\\limits_{|m|=k}\\begin{pmatrix}R^{1}_{k,m} \\\\ \\vdots \\\\ R^{n}_{k,m}\\end{pmatrix}s_m$$ and we take common factor: $$\\sum\\limits_{|m|=k}\\left(\\begin{pmatrix}W_{k,m}^{i}(\\lambda^{x}, m)\\\\ \\vdots\\\\ W_{k,m}^{n}(\\lambda^{x}, m)\\end{pmatrix} - \\begin{pmatrix}\\lambda_{1}W_{k}^{1} \\\\ \\vdots \\\\ \\lambda_nW_{n}^{d} \\end{pmatrix}+ \\begin{pmatrix}f^{1}_{k,m} \\\\ \\vdots \\\\ f^{d}_{k,m} \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{pmatrix}\\right)s_m = \\sum\\limits_{|m|=k}\\begin{pmatrix}R^{1}_{k,m} \\\\ \\vdots \\\\ R^{n}_{k,m}\\end{pmatrix}s_m\\] We distinguish between the first \\(d\\) components and from the \\(d+1\\) -th to the \\(n\\) -th component. Equations (components) \\(i=d+1, \\dots, n\\) \u00b6 \\( \\(\\begin{align*} (\\lambda^{x}, m)W_{k,m}^{i} - \\lambda_{i}W^{i}_{k,m}&= R^{i}_{k,m}\\\\ [(\\lambda^{x},m) - \\lambda_{i}]W^{i}_{k,m} &= R^{i}_{k,m} \\qquad \\textrm{cohomological eqs} \\end{align*}\\) \\) And from the cohomological equations, \\(\\Rightarrow W^{i}_{k,m} = \\frac{R^{i}_{k,m}}{(\\lambda^{x},m) - \\lambda_{i}} \\quad k\\geq 2\\) Remark: This is possible if \\((\\lambda^{x},m) - \\lambda_{i} \\neq 0\\) . Def: Primary (external) resonances \u00b6 The \"couples \\((m, i)\\) \", \\(i = d+1, \\dots, n\\) such that \\((\\lambda^{x},m) - \\lambda_{i} = 0\\) are called primary (external) resonances. Equations (components) \\(i=1, \\dots, d\\) \u00b6 \\[\\begin{align*} (\\lambda^{x}, m)W_{k,m}^{i} - \\lambda_{i}W^{i}_{k,m} + f^{i}_{k,m}&= R^{i}_{k,m}\\\\ [(\\lambda^{x},m) - \\lambda_{i}]W^{i}_{k,m} + f^{i}_{k,m} &= R^{i}_{k,m} \\qquad \\textrm{cohomolog equations}\\end{align*}\\] Now we can choose: - (A): Graph style : \\(W^{i}_{k,m}=0\\) , \\(f^{i}_{k,m} = R^{i}_{k,m}\\) - (B): Normal form style : \\(f^{i}_{k,m}=0\\) , \\(W^{i}_{k,m} = \\frac{R^{i}_{k,m}}{(\\lambda^{x},m) - \\lambda_{i}}\\) Remark : THis is possible if \\((\\lambda^{x},m) - \\lambda_{i} \\neq 0\\) Def: Secondary (internal) resonances \u00b6 The \"couples \\((m, i)\\) \", \\(i = 1, \\dots, d\\) such that \\((\\lambda^{x},m) - \\lambda_{i} = 0\\) are called secondary (internal) resonances. In case (A), we obtain: \\( \\(W(s) = \\begin{pmatrix}\\vec{s} \\\\ 0\\end{pmatrix} + \\sum\\limits_{k\\geq2}\\begin{pmatrix}W^{x}_{k}(s) \\\\ W^{y}_{k}(s)\\end{pmatrix} = \\begin{pmatrix}W^{x}(s) \\\\ W^{y}(s)\\end{pmatrix} = \\begin{pmatrix}s \\\\ W^{y}(s)\\end{pmatrix}\\) \\) which is a graph (as in \\(x\\) and \\(f(x)\\) ), and \\( \\(f(s)=\\begin{pmatrix}\\lambda_{1}s_{1} \\\\ \\vdots \\\\ \\lambda_{d}s_{d}\\end{pmatrix} + \\sum\\limits_{k\\geq2}f_{k}(s)\\) \\) In case (b), we obtain: ( \\(f(s) = \\begin{pmatrix}\\lambda_{1}s_{1} \\\\ \\vdots \\\\ \\lambda_{d}s_{d}\\end{pmatrix}\\) \\) and ( \\(W(s) = \\begin{pmatrix}\\vec{s} \\\\ 0\\end{pmatrix} + \\sum\\limits_{k\\geq2}\\begin{pmatrix}W^{x}_{k}(s) \\\\ W^{y}_{k}(s)\\end{pmatrix} = \\begin{pmatrix}W^{x}(s) \\\\ W^{y}(s)\\end{pmatrix}\\) \\) At each stage, you just have to compute \\(R\\) Comments: \u00b6 Comment 1 \u00b6 There are no primary resonances for an unstable or stable invariant manifold. Why? Let's consider \\(1\\leq i \\leq d\\) , \\(d+1\\leq j \\leq n\\) If we want to compute a \\(d\\) unstable manifold, this means \\(Re{(\\lambda_{i})>0}\\) , \\(Re{(\\lambda_{j})<0}\\) (case 1): \\((\\lambda^{x},m) - \\lambda_{i} = m_{1}\\lambda_{1}+\\dots + m_{d}\\lambda_{d} - \\lambda_{j}\\) Take the real part and we have: \\( \\(m_{1}Re(\\lambda_{1}) + \\dots + m_{d}Re(\\lambda_{d}) - Re(\\lambda_{j}) \\neq 0\\) \\) because the first part is always positive ( \\(m\\) and \\(Re(\\lambda_{i})\\) ) are always positive and \\(Re(\\lambda_{j})\\) will always be negative or zero (since these correspond to the eigenvalues with negative equal part, the ones non-related to the unstable manifold). The equivalent for stable manifolds is true. Comment 2 \u00b6 In case of 1D unstable or stable manifolds, there are no secondary resonances: \\( \\((\\lambda_{1}, m) - \\lambda_{1}=(m-1)\\lambda_{1}\\neq 0\\) \\) and for \\(m\\geq 2\\) , so \\(m-1\\leq 0\\) and \\(\\lambda_{1}\\neq 0\\) . Stable means expansion Unstable means compression Central means no expansion, no compression. Examples \u00b6 Example 1 (by hand) \u00b6 Assignment Consider the pendulum \\(x'' + \\sin{x} = 0\\) or equivalently \\( \\(\\begin{align*} x_1' &= x_2\\\\ x_2' &= -\\sin{x} \\end{align*}\\) \\) and the equilibrium point \\((\\pi, 0)\\) Part 1 : Compute, by hand, a parametrization of \\(\\mathfrak{W}^{u}\\) (the unstable 1d manifold) up to order 3 (included) following the steps just described using: A) Graph style B) Normal form style ![[2.2 Expansion of parametrizations of invariant manifolds (for flows) 2022-11-22 15.13.10.excalidraw]] We will have \\( \\(DG(Z_{0}) = \\begin{pmatrix}0 & 1 \\\\ 1 & 0\\end{pmatrix}\\) \\) with eigenvalues \\(\\lambda_{1}= 1\\) and \\(\\lambda_{2}= -1\\) and \\( \\(P=\\begin{pmatrix}1 & 1 \\\\ 1 & -1\\end{pmatrix}\\) \\) Remember to do the final step!! \\(W^{or var}(s) = Z_{0}+ PW(s)\\) Part 2 : Consider the window \\([0, 2\\pi] \\times [-2.5, 2.5]\\) and plot the phase portrait and on top the \\(W^{s}\\) ![[2.2 Expansion of parametrizations of invariant manifolds (for flows) 2022-11-22 15.20.51.excalidraw]] Example 2: The Lorenz manifold \u00b6 Goal : To ocmpute an expansion for the parametrization of the stable manifold of the origin using the normal form style \\( \\(\\begin{cases} x_{1}' &= \\sigma(x_{2}-x_{1}) \\\\ x_{2}' &= \\rho x_{1}-x_{2}-x_{1}x_3 \\\\ x_{3}' &= x_{1}x_{2}-\\beta x_{3} \\\\ \\end{cases}\\) \\) with \\(\\sigma=10, \\rho=28, \\beta= \\frac{8}{3}\\) . Eq points : \\( \\(\\begin{align*} P_{0}=(0, 0, 0)\\\\ P_{1,2} = (\\pm \\sqrt{\\beta(\\rho-1)}, \\pm \\sqrt{\\beta(\\rho-1)}, \\rho-1)\\\\ \\end{align*}\\) \\) The eigenvalues are \\(\\lambda_{1}=-22.828\\dots\\) , \\(\\lambda_{2}=-2.666\\dots\\) and \\(\\lambda_{3}=11.828\\dots\\) . This means we have a 1d ustable manifold \\(\\mathfrak{W}^{u}\\) and a 2d stable manifold \\(\\mathfrak{W}^{s}\\) , called the Lorenz Manifold . Order of the expansion? In order to know at a given degree \\(k\\) how good is the expansion, you can check with the invariance condition. Fix a \\(k\\) : \\(\\mathfrak{W}^{s}\\) is a 2d manifold. Consider \\(s_{\\delta,\\theta} = (\\delta \\cos{\\theta}, \\delta \\sin{\\theta})\\) with \\(\\delta\\) fixed and \\(\\theta \\in [0, 2\\pi)\\) (a circle of initial conditions). Since we are using the normal form style, \\( \\(\\begin{cases} s_{1}' &= \\lambda_{1}s_{1}\\qquad \\Rightarrow \\qquad s_{1}(t) = s_{10}e^{\\lambda_{1}t} \\\\ s_{2}' &= \\lambda_{2}s_{2}\\qquad \\Rightarrow \\qquad s_{2}(t) = s_{20}e^{\\lambda_{2}t} \\\\ \\end{cases}\\) \\) and \\(\\phi_{t}(s_{10}e^{\\lambda_{1}t}, s_{20}e^{\\lambda_{2}t})\\) Take an \\(s=s_{0}=s_{\\delta, \\theta}\\) for \\(\\theta\\) given. Compute \\(\\phi_t(s)\\) for some \\(t\\) . \\(W(s) = W(s_{0})\\) and compute \\(\\Phi_{t}(W(s))\\) for some \\(t\\) We call \\(e_{0}(t, s_{\\delta, \\theta}) = ||W(s(t)) - Z(t)||\\) You compute the error for several \\(\\theta\\) and you compute \\(\\epsilon(\\delta) = \\max_{\\theta \\in [0, 2\\pi]}{e_{0}(t, s_{\\delta, \\theta})}\\) For a given \\(k\\) , the smaller \\(\\delta\\) is, the smaller the error is. Error depends on \\(t\\) . Here they take \\(t=0.3\\) . The idea is that from \\(s_{0}\\) you go to approx \\(1000s_0\\)","title":"2.2 Expansion of parametrizations of invariant manifolds (for flows)"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.2%20Expansion%20of%20parametrizations%20of%20invariant%20manifolds%20%28for%20flows%29/#22-expansion-of-parametrizations-of-invariant-manifolds-for-flows","text":"Reference : The parametrization method for invariant manifolds Alex Haro, Marta Canadell, Jordi Llu\u00eds Figueras, ALejandro Luque, Josep M. Mondelo. Theoretical background : Xavier Cabr\u00e9, Rafel de la Llave, Ernest Fontich (3 papers)","title":"2.2. Expansion of parametrizations of invariant manifolds (for flows)"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.2%20Expansion%20of%20parametrizations%20of%20invariant%20manifolds%20%28for%20flows%29/#motivation","text":"Assume \\(Z' = G(Z)\\) as system of \\(n\\) ODE, \\(Z \\in U \\subset \\mathbb{R}^{n}\\) , \\(Z_0\\) an equilibrium point such that the eigenvalues of \\(DG(Z_0)\\) are \\(\\lambda_{1}, \\lambda_{2}, \\dots, \\lambda_{n}\\) with \\(\\lambda_{1}, \\lambda_{2} > 0\\) . We know, thus, there exists a 2d unstable invariant manifold. For the linearized system, we take the \\(\\vec{v}_{1}^{u}, \\vec{v}_{2}^{u}\\) eigenvectors. If \\(\\lambda_{2}> \\lambda_{1} > 0\\) , then \\(\\lambda_{2}\\) is the fast dimension. The whole plane is the unstable manifold. ![[linear_Wu|200]] For the non-linear original system, we have a 2d invariant manifold, \\(W^{u}\\) . \\( \\(Z_{0} = Z_{0} + s_{1}\\vec{v}_{1}^{u} + s_{2}\\vec{v}_{2}^{u}\\) \\) with \\( \\(\\begin{cases} s_{1} &= r\\cos{\\theta} \\\\ s_{2} &= r\\sin{\\theta} \\end{cases}\\) \\) with \\(r\\) very small and integrate forward in time. That's how we typically do it. See [[Motivation for expansion of parametrizations of invariant manifolds (for flows)]] and Jupyter Notebook for more.","title":"Motivation"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.2%20Expansion%20of%20parametrizations%20of%20invariant%20manifolds%20%28for%20flows%29/#goal","text":"To obtain a high order power expansion for a parametrization of invariant manifolds of an equilibrim point of a vector field. Let us start \\(Z' = G(Z)\\) , \\(Z_0\\) equilibrium point, \\(Z \\in U \\subset \\mathbb{R}^{n}\\) . Assume \\(G(Z_{0)}= 0\\) , and the following eigenvalues: \\( \\(P^{-1}DG(Z_{0})P = \\begin{pmatrix}\\lambda_{1} & \\dots \\\\ & \\ddots & \\\\ & \\cdots & \\lambda_{n}\\end{pmatrix}\\) \\) We do a change of variables: \\(z = Z_{0} + P \\cdot z\\) . We obtain \\( \\(z' = F(z),\\) \\) where \\(F(z) = P^{-1}G(Z_{0}+P\\cdot z)\\) . \\(Z' = P z' \\Rightarrow z' = P^{-1}Z' = P^{-1}G(Z) = P^{-1}G(Z_{0}+ P\\cdot z)\\) Now \\(F(0) = 0\\) , \\(DF(0) = P^{-1}DG(Z_{0)P}= \\begin{pmatrix}\\lambda_{1} & \\dots \\\\ & \\ddots & \\\\ & \\cdots & \\lambda_{n}\\end{pmatrix}\\) My input used to be \\(Z_{0}, G\\) and now it's \\(F\\) . We consider the Taylor expansion: \\( \\(F(z) = DF(0)z + F_{2}(z) + F_{3}(z) + \\dots\\) \\) and this will be my input. Notice that \\(F_{2}, F_{3}\\) are the 2nd, 3rd order terms. Now we have this input and we want to compute the invariant manifold of this zero. We choose the first \\(d\\) eigenvalues of \\(DF(0)\\) (we can always reorder them) and let \\(V^{d}\\) be a \\(d\\) dimensional linear subspace spanned by the \\(d\\) eigenvectors. (Assume, for example, that \\(\\lambda_{1\\dots d}>0\\) \\(\\Rightarrow\\) there exists a \\(d\\) -dimensional unstable invariant manifold \\(\\mathfrak{W}^u\\) (this is a gothic W) ) Our goal is to obtain a high order approximation of the \\(d\\) -dimensional manifold \\(\\mathfrak{W}^u\\) for system \\(z' = F(z)\\) tangent to \\(V^{d}\\) In our example, \\(W(s) = \\begin{pmatrix}v_{1}^{1} & v_{1}^{2} & \\dots & v_{1}^{d} \\\\ \\vdots & \\vdots & & \\vdots \\\\ v_{n}^{1} & v_{n}^{2} & \\dots & v_{n}^{d}\\end{pmatrix} \\begin{pmatrix}s_{1} \\\\ \\vdots \\\\ s_{d}\\end{pmatrix} = s_{1}\\vec{v}^{1}+\\dots +s_{d}\\vec{v}^{d} = \\begin{pmatrix}v_{1}^{1} & v_{1}^{2} & \\dots & v_{1}^{d} & \\cdots \\\\ \\vdots & \\vdots & & \\vdots & \\\\ v_{n}^{1} & v_{n}^{2} & \\dots & v_{n}^{d} & \\cdots\\end{pmatrix}\\begin{pmatrix}s_{1} \\\\ \\vdots \\\\ s_{d} \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{pmatrix}\\) where the last matrix is \\(P\\) . With the original variables \\(W(s) = Z_{0}+ P(\\begin{pmatrix}s_{1} \\\\ \\vdots \\\\ s_{d} \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{pmatrix}) = Z_{0}+s_{1}\\vec{v}^{1}+\\dots +s_{d}\\vec{v}^{d}\\) . This is an approximation for parametrizing the plane. But we are not interested in the plane now, only in the \"border\". ![[parametrization_nmds|200]] More concretely, we look for a parametrization \\( \\(\\begin{align*} W: \\tilde{U}\\subset \\mathbb{R}^{d} &\\longrightarrow \\mathbb{R}^n\\\\ s &\\longrightarrow W(s) = z \\end{align*}\\) \\) of the invariant manifold \\(\\mathfrak{W}\\) where \\(s = (s_{1}, \\dots, s_{d})\\) are the coordinates (parameters) of the parametrization and \\(W(0) = 0\\) . The internal dynamics on the manifold is described by a vector field \\( \\(s' = f(s)\\) \\) I have two flows, one for \\(z' = F(z)\\) , \\(\\Phi_{t}\\) and one for \\(s'=f(s)\\) , \\(\\phi_{t}\\) , with \\(f(0) = 0\\) . So, we look for \\(W\\) and \\(f\\) such that is invariant by the flow of \\(F\\) : 1. \\(\\{W(s)\\}_{s\\in \\tilde{W}\\subset \\mathbb{R}^{d}}\\) is invariant by the flow of \\(F\\) . 2. the internal dynamics of \\(s\\) is given by \\(s' =f(s)\\) That is, we want \\( \\(\\Phi_{t}(W(\\tilde{s})) = W(\\phi_{t}(\\tilde{s}))\\) \\) Let us take \\(\\frac{d}{dt}\\) : \\( \\(F(\\Phi_{t}(W(\\tilde{s}))) = DW(\\phi_{t}(\\tilde{s}))\\cdot f(\\phi_{t}(\\tilde{s}))\\) \\) we have that \\(\\phi_{t}(\\tilde{s}) = s\\) and \\(\\Phi_{t}(W(\\tilde{s})) = W(s)\\) . that is we want \\(W(s)\\) , \\(f(s)\\) such that","title":"Goal"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.2%20Expansion%20of%20parametrizations%20of%20invariant%20manifolds%20%28for%20flows%29/#def-invariance-equation","text":"\\( \\(F(W(s)) = DW(s)f(s)\\) \\) And, conversely, if the invariance equation is true, then (IC) is also tre (It can be proved). So, what we will do is to solve the invariance equation. A geometrical interpretation ![[geometrical_interpretation]] Unknowns: \\(W(s), f(s)\\) and known is \\(F\\)","title":"Def: Invariance equation"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.2%20Expansion%20of%20parametrizations%20of%20invariant%20manifolds%20%28for%20flows%29/#the-parametrization-method","text":"Date : 16-11-2022 We have \\( \\(\\begin{align*} Z' &= G(Z), \\quad Z_{0} \\textrm{: an equilibrim point}\\\\ Z &= Z_{0}+Pz \\end{align*}\\) \\) and we transform it so that \\( \\(z' = F(z)\\) \\) with 0 an equilibrium point: \\(F(0) = 0\\) , \\(DF(0) = \\begin{pmatrix}\\lambda_{1} & & \\\\ & \\ddots & \\\\ & & \\lambda_{n}\\end{pmatrix}\\) And we will do the expansion: \\( \\(F(z) = DF(0)z + F_{2}(z) + F_{3}(z) + \\ldots = F_{1}(z) + F_{2}(z) + F_{3}(z) + \\ldots\\) \\) and \\(F_{1}=\\begin{pmatrix}\\lambda_{1}z_{1} \\\\ \\vdots \\\\ \\lambda_{n}z_{n}\\end{pmatrix} + \\sum\\limits_{k\\geq2}F_{k}(z)\\) For example \\( \\(DF(0) = \\begin{pmatrix}\\lambda_{1}>0 & & & & \\\\ & \\ddots & & & \\\\ & & \\lambda_{d}>0 & &\\\\ & & &\\lambda_{d+1}<0 & & &\\\\ & & & &\\ddots & & \\\\ & & & & & &\\lambda_{n}<0\\end{pmatrix}\\) \\) There exists a d-dimensional unstable manifold Specific goal : we look for a d-dim manifold, so we want: \\( \\(W(s) = \\sum\\limits_{k\\geq1}W_{k}(s) = \\begin{pmatrix}s_{1} \\\\ \\vdots \\\\ s_{d} \\\\ 0 \\\\ \\vdots \\\\ 0\\end{pmatrix} + \\sum\\limits_{k\\geq2}W_{k}(s); \\qquad f(s) = \\begin{pmatrix}\\lambda_{1}s_{1} \\\\ \\vdots \\\\ \\lambda_{d}s_{d} \\end{pmatrix} + \\sum\\limits_{k\\geq2}f_{k}(s)\\) \\) where \\((s_{1}, \\ldots, s_{d}, 0, \\ldots, 0)\\) is \\(W_{1}(s)\\) and \\(W(0) = 0\\) , \\(f_{1} = (\\lambda_{1}s_{1}, \\dots)\\) and \\(f(0) = 0\\) that are solution of the invariance equation (eq 4) \\( \\(F(W(s)) = DW(s)\\cdot f(s)\\) \\) which comes from \\(\\phi_{t}(W(s)) = W(\\phi_{t}(s))\\) .","title":"The parametrization method"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.2%20Expansion%20of%20parametrizations%20of%20invariant%20manifolds%20%28for%20flows%29/#remark-1","text":"Taking \\(W(s)\\) and \\(f(s)\\) as above the invariance equation (4) is satisfied at first order: \\( \\([F(W(s))]_{1} = F_{1}(W_{1}(s)) = F_{1}\\begin{pmatrix}s_{1} \\\\ \\vdots \\\\ s_{d} \\\\ 0 \\\\ \\vdots \\\\ 0\\end{pmatrix} = \\begin{pmatrix}\\lambda_{1}s_{1} \\\\ \\vdots \\\\ \\lambda_{d}s_{d} \\\\ 0 \\\\ \\vdots \\\\ 0\\end{pmatrix}\\) \\) \\( \\([DW(s)f(s)]_{1}=DW_{1}(s)f_{1}(s) = \\begin{pmatrix}I_{d\\times d \\\\ 0}\\end{pmatrix}\\begin{pmatrix}\\lambda_{1}s_{1} \\\\ \\vdots \\\\ \\lambda_{d}s_{d}\\end{pmatrix} = \\begin{pmatrix}\\lambda_{1}s_{1} \\\\ \\vdots \\\\ \\lambda_{d}s_{d} \\\\ 0 \\\\ \\vdots \\\\ 0\\end{pmatrix}\\) \\) so \\([F(W(s))]_{1} = [DW(s)f(s)]_{1}\\) We know that \\(DW_{1}(s)\\) is the derivative of the vector \\(W_{1}\\) , which is just the \\(s\\) variables and zeros with respect to \\(s\\) , so we have \\(DW_{1}(s) \\in \\mathbb{R}^{n\\times s}\\) and we will have an identity matrix stacked with a zero matrix Order k of \\(f\\) means order \\(k\\) of z means a homogeneous polynomial of degree k. \\([F]_1\\) means expansion of order 1","title":"Remark 1"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.2%20Expansion%20of%20parametrizations%20of%20invariant%20manifolds%20%28for%20flows%29/#remark-2","text":"Once we have done the procedure and we have obtained the parametrization \\(W(s)\\) up to a given order, that is \\(z = W(s)\\) we apply the change of variables in order to obtain the parametrization in original variables: \\( \\(W^{\\textrm{orig var}}(s) = Z_{0}+P\\cdot W(s)\\) \\) So we are only gonna focus on \\(W(s)\\)","title":"Remark 2"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.2%20Expansion%20of%20parametrizations%20of%20invariant%20manifolds%20%28for%20flows%29/#remark-3","text":"Example 1d For example \\(Z' =G(Z)\\) , \\(Z_{0}\\) with \\(DF(0) = \\begin{pmatrix}\\lambda_{1}>0 & & & \\\\ \\lambda_{2} & & \\\\ & & \\ddots & \\\\ & & & \\lambda_{n}\\end{pmatrix} \\Rightarrow \\exists\\) a 1d unstable manifold \\(W^u\\) \\( \\(P = \\begin{pmatrix}v_{1}^{1} & \\cdots & \\cdots \\\\ \\vdots & \\cdots & \\cdots \\\\ v_{n}^{1} & \\cdots & \\cdots \\end{pmatrix}\\) \\) if we take \\(W(s)\\) up to first order: \\(W(s) \\approx W_{1}(s) = \\begin{pmatrix}s_{1} \\\\ 0 \\\\ \\vdots \\\\ 0\\end{pmatrix}\\) then in the original variable \\( \\(W^{\\textrm{orig variable}}(s) \\approx Z_{0}+PW_{1}(s) = Z_{0}+\\begin{pmatrix}v_{1}^{1} & \\cdots & \\cdots \\\\ \\vdots & \\cdots & \\cdots \\\\ v_{n}^{1} & \\cdots & \\cdots \\end{pmatrix}\\begin{pmatrix}s_{1} \\\\ 0 \\\\ \\vdots \\\\ 0\\end{pmatrix} = Z_{0}+s_{1}v^{1}\\) \\) which is what we've done so far. Typically for linear approximations \\(s\\in(0, 10^{-6})\\) Example 2d \\(Z' =G(Z)\\) , \\(Z_{0}\\) with \\(DF(0) = \\begin{pmatrix}\\lambda_{1}>0 & & & \\\\ \\lambda_{2}>0 & & \\\\ & & \\ddots & \\\\ & & & \\lambda_{n}\\end{pmatrix} \\Rightarrow \\exists\\) a 2d unstable manifold \\(W^u\\) \\( \\(P = \\begin{pmatrix}v_{1}^{1} & v_{1}^2 & \\cdots & \\cdots \\\\ \\vdots & \\vdots & \\cdots & \\cdots \\\\ v_{n}^{1} & v_{n}^2 & \\cdots & \\cdots \\end{pmatrix}\\) \\) if we take \\(W(s)\\) up to first order: \\(W(s) \\approx W_{1}(s) = \\begin{pmatrix}s_{1} \\\\ s_{2} \\\\ \\vdots \\\\ 0\\end{pmatrix}\\) then in the original variable \\( \\(W^{\\textrm{orig variable}}(s) \\approx Z_{0}+PW_{1}(s) = Z_{0}+\\begin{pmatrix}v_{1}^{1} & v_{1}^{2} & \\cdots & \\cdots \\\\ \\vdots & \\vdots & \\cdots & \\cdots \\\\ v_{n}^{1} & v_{n}^{2} & \\cdots & \\cdots \\end{pmatrix}\\begin{pmatrix}s_{1} \\\\ s_{2} \\\\ 0 \\\\ \\vdots \\\\ 0\\end{pmatrix} = Z_{0}+sv^{1} + s_{2}v^{2}\\) \\) which is what we have done so far.","title":"Remark 3"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.2%20Expansion%20of%20parametrizations%20of%20invariant%20manifolds%20%28for%20flows%29/#resolution-of-the-invariance-equation-up-to-a-high-order","text":"\\[F(W(s)) = DF(W(s))f(s)\\]","title":"Resolution of the invariance equation (up to a high order)"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.2%20Expansion%20of%20parametrizations%20of%20invariant%20manifolds%20%28for%20flows%29/#notation","text":"We know \\(F(z)\\) (this is my input) where \\(F_{k}\\) means \\( \\(F_{k}^{i}=\\sum\\limits_{|l|=k=l_{1}+\\dots + l_{n}} F^{i}_{k,l}z^{l}=\\sum\\limits_{|l| = k}F^{i}_{k,l} z_{1}^{l_{1}} \\cdots z_{n}^{l_{n}}\\) \\) For the \\(k\\) term, we have \\(n\\) components. Each component \\(i\\) is a homogeneous polynomial of degree \\(k\\) in variables \\(z_{1, \\dots, n}\\) . \\( \\(W_{k}^{i}(s) = \\sum\\limits_{|m|=k=m_{1}+\\dots + m_{d}} W^{i}_{k,m}s^{m}=\\sum\\limits_{|m| = k}W^{i}_{k,l} s_{1}^{m_{1}} \\cdots s_{d}^{m_{d}} \\qquad i=1, \\dots, n\\) \\) For \\(W(s)\\) , each term of the expansion \\(W_{k}\\) will have \\(n\\) components and each component is a homogeneous polynomial of degree \\(k\\) . \\( \\(f_{k}^{i}(s) = \\sum\\limits_{|m|=k} f^{i}_{k,m}s^{m}=\\sum\\limits_{|m| = k}f^{i}_{k,l} s_{1}^{m_{1}} \\cdots s_{d}^{m_{d}} \\qquad i=1, \\dots, d\\) \\) and \\(f\\) will have d components.","title":"Notation"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.2%20Expansion%20of%20parametrizations%20of%20invariant%20manifolds%20%28for%20flows%29/#another-notation","text":"\\(z = (x, y)\\) where \\(x\\) is the first \\(d\\) components and \\(y\\) are the following \\(n-d\\) components. \\(\\lambda^{x}\\) is the first \\(d\\) eigenvalues \\((\\lambda^{x}, m) = \\lambda_{1}m_{1}+\\dots +\\lambda_{d}m_{d}\\) is the scalar product.","title":"Another notation"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.2%20Expansion%20of%20parametrizations%20of%20invariant%20manifolds%20%28for%20flows%29/#resolution","text":"We want to solve the invariance equation at order \\(k\\) assuming that it is satisfied up to order \\(k-1\\) , that is, - we know \\(W_{1}, \\dots, W_{k-1}, f_{1}, \\dots, f_{k-1}\\) - we want \\(W_{k}, f_{k}\\) , that is \\( \\(W_{k}=\\begin{pmatrix}W_{k}^{1} \\\\ \\vdots \\\\ W_{k}^{n}\\end{pmatrix} \\quad f_{k}=\\begin{pmatrix}f_{k}^{1} \\\\ \\vdots \\\\ f_{k}^d\\end{pmatrix}\\) \\) \\( \\([F(W(s))]_{k}=[DW(s)f(s)]_{k}\\) \\)","title":"Resolution"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.2%20Expansion%20of%20parametrizations%20of%20invariant%20manifolds%20%28for%20flows%29/#lhs","text":"Let us consider the left hand side terms of order \\(k\\) \\( \\(\\begin{align*} [F(W(s))]_{k}&= [F(W_{\\leq k})]_{k}=[F(W_{<k} + W_{k})]_{k} =(taylor) = [F(W_{<k})]_{k}+DF(0)W_{k} \\\\ &= [F(W_{<k})]_{k} + \\begin{pmatrix}\\lambda_{1} & & \\\\ & \\ddots & \\\\ & & \\lambda_n\\end{pmatrix}\\begin{pmatrix}W_{k}^1\\\\ \\vdots \\\\ W_{k}^n\\end{pmatrix}\\\\ &= [F(W_{<k})]_{k} + \\begin{pmatrix}\\lambda_{1}W_{k}^{1}\\\\ \\vdots\\\\ \\lambda_{n}W_{k}^{n}\\end{pmatrix}\\\\ &= (I) + (II)\\end{align*}\\) \\) where \\((I)\\) is known and \\((II)\\) is unknown check at home the first line","title":"LHS"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.2%20Expansion%20of%20parametrizations%20of%20invariant%20manifolds%20%28for%20flows%29/#rhs","text":"Let us consider the right hand side \\( \\(\\begin{align*} [DW(s)f(s)]_{k} &= \\sum\\limits_{l=0}^{k}[DW]_{l}\\cdot [f]_{k-l}= \\sum\\limits_{l=0}^{k-1}DW_{l+1}\\cdot f_{k-l} \\\\ &=DW_{1}f_{k} + DW_{k}f_{1}+\\sum\\limits_{l=1}^{k-2}DW_{l+1}f_{k-l}\\\\ &=DW_{1}f_{k} + DW_{k}f_{1}+\\sum\\limits_{\\tilde{l}=2}^{k-1}DW_{\\tilde{l}}f_{k-\\tilde{l}+1}\\\\ &= DW_{1}f_{k} + DW_{k}f_{1}+\\sum\\limits_{{l}=2}^{k-1}DW_{{l}}f_{k-{l}+1}\\\\ &= \\begin{pmatrix}I\\\\ 0\\end{pmatrix}f_{k} + DW_{k}\\begin{pmatrix}\\lambda_1s_1\\\\ \\vdots\\\\ \\lambda_{d}s_{d}\\end{pmatrix}+\\sum\\limits_{{l}=2}^{k-1}DW_{{l}}f_{k-{l}+1}\\\\ &= \\begin{pmatrix}f_{k}\\\\ 0\\end{pmatrix} + DW_{k}\\begin{pmatrix}\\lambda_1s_1\\\\ \\vdots\\\\ \\lambda_{d}s_{d}\\end{pmatrix}+\\sum\\limits_{{l}=2}^{k-1}DW_{{l}}f_{k-{l}+1} = (III) + (IV) + (V) \\end{align*}\\) \\) where \\((III), (IV)\\) are unknown (because of \\(f_{k}, DW_{k}\\) ) and \\((V)\\) is known. The first equal is due to \\(DW, f\\) being homogeneous polynomials. The second equal is due to \\([DW]_{l}=DW_{l+1}\\) . Then we also apply that \\(f_{0}=0\\) . And \\(f_{k}, W_{k}\\) are unknown.","title":"RHS"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.2%20Expansion%20of%20parametrizations%20of%20invariant%20manifolds%20%28for%20flows%29/#rhs-and-lhs","text":"\\[(I) + (II) = (III) + (IV) + (V) \\Longleftrightarrow (IV)-(II)+(III)=(I)-(V)$$ which means: $$ DW_{k}\\begin{pmatrix}\\lambda_1s_{1}\\\\ \\vdots\\\\ \\lambda_{d}s_{d}\\end{pmatrix} - \\begin{pmatrix}\\lambda_{1}W_{k}^{1}\\\\ \\vdots\\\\ \\lambda_{n}W_{k}^{n}\\end{pmatrix}+ \\begin{pmatrix}f^{1}_{k}\\\\ \\vdots \\\\ f^{d}_{k} \\\\ 0 \\\\ \\vdots \\\\ 0\\end{pmatrix} = [F(W_{<k})]_{k}+\\sum\\limits_{{l}=2}^{k-1}DW_{{l}}f_{k-{l}+1} = R_{k} \\textrm{ (known)}$$ Now we expand $W, f, R$ in monomials and notice $$\\begin{align*} W_{k}(s) &= \\sum\\limits_{|m|=k}W_{k,m}s_{1}^{m_{1}}\\dots s_{d}^{m_{d}}\\\\ \\partial_{1}W_{k} &= \\sum\\limits_{|m|=k}W_{k,m}s_{1}^{m_{1}-1}\\dots s_{d}^{m_{d}}\\\\ \\vdots\\\\ \\partial_{d}W_{k} &= \\sum\\limits_{|m|=k}W_{k,m}s_{1}^{m_{1}}\\dots s_{d}^{m_{d}-1}\\\\ \\end{align*}$$ and now $$\\begin{align*} DW_{k}\\begin{pmatrix}\\lambda_1s_{1}\\\\ \\vdots\\\\ \\lambda_{d}s_{d}\\end{pmatrix} &= \\begin{pmatrix}\\sum\\limits_{|m|=k}W_{k,m}s_{1}^{m_{1}-1}\\dots s_{d}^{m_{d}} & \\dots & \\sum\\limits_{|m|=k}W_{k,m}s_{1}^{m_{1}}\\dots s_{d}^{m_{d}-1}\\end{pmatrix}\\begin{pmatrix}\\lambda_1s_{1}\\\\ \\vdots\\\\ \\lambda_{d}s_{d}\\end{pmatrix}\\\\ &= \\begin{pmatrix}\\sum\\limits_{|m|=k}W^{1}_{k,m}s_{1}^{m_{1}-1}\\dots s_{d}^{m_{d}} & \\dots & \\sum\\limits_{|m|=k}W^{1}_{k,m}s_{1}^{m_{1}}\\dots s_{d}^{m_{d}-1}\\\\ \\vdots & \\cdots & \\vdots\\\\ \\sum\\limits_{|m|=k}W^{n}_{k,m}s_{1}^{m_{1}-1}\\dots s_{d}^{m_{d}} & \\dots & \\sum\\limits_{|m|=k}W^{n}_{k,m}s_{1}^{m_{1}}\\dots s_{d}^{m_{d}-1}\\end{pmatrix}\\begin{pmatrix}\\lambda_1s_{1}\\\\ \\vdots\\\\ \\lambda_{d}s_{d}\\end{pmatrix}\\\\ &= \\sum\\limits\\begin{pmatrix}W_{k,m}^{1}(m_{1}\\lambda_{1}+ \\dots + m_d\\lambda_{d}s^{m}\\\\ \\ldots \\\\ W_{k,m}^{n}(m_{1}\\lambda_{1}+ \\dots + m_d\\lambda_{d}s^{m}\\end{pmatrix} = \\sum\\limits \\begin{pmatrix}W_{k,m}^{i}\\\\ \\vdots\\\\ W_{k,m}^{n}\\end{pmatrix}(\\lambda^{x}, m)s^{m}\\end{align*}\\] Date: 22-11-2022","title":"RHS and LHS"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.2%20Expansion%20of%20parametrizations%20of%20invariant%20manifolds%20%28for%20flows%29/#recap","text":"\\( \\(\\begin{align*} Z' &= G(Z) & \\qquad(1)\\\\ Z &= Z_{0}+ Pz \\\\ z' &= F(z) & \\qquad(2) \\end{align*}\\) \\) with \\(Z_0\\) an equilibrium point, \\(F(0) = 0\\) and \\(DF(0) = \\begin{pmatrix}\\lambda_{1} & \\cdots & 0 \\\\ \\\\ 0 & \\cdots & \\lambda_{n}\\end{pmatrix}\\) . Taylor expansion : \\( \\(F(z) = F_{1}(z) + F_{2}(z) + \\dots = DF(0) + F_{2}+\\dots\\) \\) Invariance equation \\( \\(\\Phi_t(W(s))= W(\\phi_{t}(s))\\) \\) and derivative respect \\(t\\) : \\( \\(F(W(s)) = DW(s)f(s)\\) \\) Goal \\( \\(\\begin{align*} W(s) &= \\begin{pmatrix}s_{1} \\\\ \\vdots \\\\ s_{d} \\\\ 0 \\\\ \\vdots \\\\ 0\\end{pmatrix} + \\sum\\limits_{k\\geq2}W_{k}(s)\\\\ f(s) &= \\begin{pmatrix}\\lambda_{1}s_{1} \\\\ \\vdots \\\\ \\lambda_{d}s_{d} \\end{pmatrix} + \\sum\\limits_{k\\geq2}f_{k}(s) \\end{align*}\\) \\) and after: \\(W^{\\textrm{or var}}(s) = Z_{0}+ PW(s)\\) . Last session, we found: \\[DW_{k}\\begin{pmatrix}\\lambda_{1}s_{1} \\\\ \\vdots \\\\ \\lambda_{d}s_{d} \\end{pmatrix} - \\begin{pmatrix}\\lambda_{1}W_{k}^{1} \\\\ \\vdots \\\\ \\lambda_nW_{n}^{d} \\end{pmatrix} + \\begin{pmatrix}f_{k}^{1} \\\\ \\vdots \\\\ f_{k}^{d} \\\\ 0 \\\\ \\vdots \\\\ 0\\end{pmatrix} = R_k$$ And we can rewrite it as: $$\\sum\\limits_{|m|=k} \\begin{pmatrix}W_{k,m}^{i}(\\lambda^{x}, m)\\\\ \\vdots\\\\ W_{k,m}^{n}(\\lambda^{x}, m)\\end{pmatrix}s^{m} - \\sum\\limits_{|m|=k}\\begin{pmatrix}\\lambda_{1}W_{k}^{1} \\\\ \\vdots \\\\ \\lambda_nW_{n}^{d} \\end{pmatrix}s_{m}+ \\sum\\limits_{|m|=k}\\begin{pmatrix}f^{1}_{k,m} \\\\ \\vdots \\\\ f^{d}_{k,m} \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{pmatrix}s_{m} = \\sum\\limits_{|m|=k}\\begin{pmatrix}R^{1}_{k,m} \\\\ \\vdots \\\\ R^{n}_{k,m}\\end{pmatrix}s_m$$ and we take common factor: $$\\sum\\limits_{|m|=k}\\left(\\begin{pmatrix}W_{k,m}^{i}(\\lambda^{x}, m)\\\\ \\vdots\\\\ W_{k,m}^{n}(\\lambda^{x}, m)\\end{pmatrix} - \\begin{pmatrix}\\lambda_{1}W_{k}^{1} \\\\ \\vdots \\\\ \\lambda_nW_{n}^{d} \\end{pmatrix}+ \\begin{pmatrix}f^{1}_{k,m} \\\\ \\vdots \\\\ f^{d}_{k,m} \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{pmatrix}\\right)s_m = \\sum\\limits_{|m|=k}\\begin{pmatrix}R^{1}_{k,m} \\\\ \\vdots \\\\ R^{n}_{k,m}\\end{pmatrix}s_m\\] We distinguish between the first \\(d\\) components and from the \\(d+1\\) -th to the \\(n\\) -th component.","title":"Recap:"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.2%20Expansion%20of%20parametrizations%20of%20invariant%20manifolds%20%28for%20flows%29/#equations-components-id1-dots-n","text":"\\( \\(\\begin{align*} (\\lambda^{x}, m)W_{k,m}^{i} - \\lambda_{i}W^{i}_{k,m}&= R^{i}_{k,m}\\\\ [(\\lambda^{x},m) - \\lambda_{i}]W^{i}_{k,m} &= R^{i}_{k,m} \\qquad \\textrm{cohomological eqs} \\end{align*}\\) \\) And from the cohomological equations, \\(\\Rightarrow W^{i}_{k,m} = \\frac{R^{i}_{k,m}}{(\\lambda^{x},m) - \\lambda_{i}} \\quad k\\geq 2\\) Remark: This is possible if \\((\\lambda^{x},m) - \\lambda_{i} \\neq 0\\) .","title":"Equations (components) \\(i=d+1, \\dots, n\\)"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.2%20Expansion%20of%20parametrizations%20of%20invariant%20manifolds%20%28for%20flows%29/#def-primary-external-resonances","text":"The \"couples \\((m, i)\\) \", \\(i = d+1, \\dots, n\\) such that \\((\\lambda^{x},m) - \\lambda_{i} = 0\\) are called primary (external) resonances.","title":"Def: Primary (external) resonances"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.2%20Expansion%20of%20parametrizations%20of%20invariant%20manifolds%20%28for%20flows%29/#equations-components-i1-dots-d","text":"\\[\\begin{align*} (\\lambda^{x}, m)W_{k,m}^{i} - \\lambda_{i}W^{i}_{k,m} + f^{i}_{k,m}&= R^{i}_{k,m}\\\\ [(\\lambda^{x},m) - \\lambda_{i}]W^{i}_{k,m} + f^{i}_{k,m} &= R^{i}_{k,m} \\qquad \\textrm{cohomolog equations}\\end{align*}\\] Now we can choose: - (A): Graph style : \\(W^{i}_{k,m}=0\\) , \\(f^{i}_{k,m} = R^{i}_{k,m}\\) - (B): Normal form style : \\(f^{i}_{k,m}=0\\) , \\(W^{i}_{k,m} = \\frac{R^{i}_{k,m}}{(\\lambda^{x},m) - \\lambda_{i}}\\) Remark : THis is possible if \\((\\lambda^{x},m) - \\lambda_{i} \\neq 0\\)","title":"Equations (components) \\(i=1, \\dots, d\\)"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.2%20Expansion%20of%20parametrizations%20of%20invariant%20manifolds%20%28for%20flows%29/#def-secondary-internal-resonances","text":"The \"couples \\((m, i)\\) \", \\(i = 1, \\dots, d\\) such that \\((\\lambda^{x},m) - \\lambda_{i} = 0\\) are called secondary (internal) resonances. In case (A), we obtain: \\( \\(W(s) = \\begin{pmatrix}\\vec{s} \\\\ 0\\end{pmatrix} + \\sum\\limits_{k\\geq2}\\begin{pmatrix}W^{x}_{k}(s) \\\\ W^{y}_{k}(s)\\end{pmatrix} = \\begin{pmatrix}W^{x}(s) \\\\ W^{y}(s)\\end{pmatrix} = \\begin{pmatrix}s \\\\ W^{y}(s)\\end{pmatrix}\\) \\) which is a graph (as in \\(x\\) and \\(f(x)\\) ), and \\( \\(f(s)=\\begin{pmatrix}\\lambda_{1}s_{1} \\\\ \\vdots \\\\ \\lambda_{d}s_{d}\\end{pmatrix} + \\sum\\limits_{k\\geq2}f_{k}(s)\\) \\) In case (b), we obtain: ( \\(f(s) = \\begin{pmatrix}\\lambda_{1}s_{1} \\\\ \\vdots \\\\ \\lambda_{d}s_{d}\\end{pmatrix}\\) \\) and ( \\(W(s) = \\begin{pmatrix}\\vec{s} \\\\ 0\\end{pmatrix} + \\sum\\limits_{k\\geq2}\\begin{pmatrix}W^{x}_{k}(s) \\\\ W^{y}_{k}(s)\\end{pmatrix} = \\begin{pmatrix}W^{x}(s) \\\\ W^{y}(s)\\end{pmatrix}\\) \\) At each stage, you just have to compute \\(R\\)","title":"Def: Secondary (internal) resonances"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.2%20Expansion%20of%20parametrizations%20of%20invariant%20manifolds%20%28for%20flows%29/#comments","text":"","title":"Comments:"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.2%20Expansion%20of%20parametrizations%20of%20invariant%20manifolds%20%28for%20flows%29/#comment-1","text":"There are no primary resonances for an unstable or stable invariant manifold. Why? Let's consider \\(1\\leq i \\leq d\\) , \\(d+1\\leq j \\leq n\\) If we want to compute a \\(d\\) unstable manifold, this means \\(Re{(\\lambda_{i})>0}\\) , \\(Re{(\\lambda_{j})<0}\\) (case 1): \\((\\lambda^{x},m) - \\lambda_{i} = m_{1}\\lambda_{1}+\\dots + m_{d}\\lambda_{d} - \\lambda_{j}\\) Take the real part and we have: \\( \\(m_{1}Re(\\lambda_{1}) + \\dots + m_{d}Re(\\lambda_{d}) - Re(\\lambda_{j}) \\neq 0\\) \\) because the first part is always positive ( \\(m\\) and \\(Re(\\lambda_{i})\\) ) are always positive and \\(Re(\\lambda_{j})\\) will always be negative or zero (since these correspond to the eigenvalues with negative equal part, the ones non-related to the unstable manifold). The equivalent for stable manifolds is true.","title":"Comment 1"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.2%20Expansion%20of%20parametrizations%20of%20invariant%20manifolds%20%28for%20flows%29/#comment-2","text":"In case of 1D unstable or stable manifolds, there are no secondary resonances: \\( \\((\\lambda_{1}, m) - \\lambda_{1}=(m-1)\\lambda_{1}\\neq 0\\) \\) and for \\(m\\geq 2\\) , so \\(m-1\\leq 0\\) and \\(\\lambda_{1}\\neq 0\\) . Stable means expansion Unstable means compression Central means no expansion, no compression.","title":"Comment 2"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.2%20Expansion%20of%20parametrizations%20of%20invariant%20manifolds%20%28for%20flows%29/#examples","text":"","title":"Examples"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.2%20Expansion%20of%20parametrizations%20of%20invariant%20manifolds%20%28for%20flows%29/#example-1-by-hand","text":"Assignment Consider the pendulum \\(x'' + \\sin{x} = 0\\) or equivalently \\( \\(\\begin{align*} x_1' &= x_2\\\\ x_2' &= -\\sin{x} \\end{align*}\\) \\) and the equilibrium point \\((\\pi, 0)\\) Part 1 : Compute, by hand, a parametrization of \\(\\mathfrak{W}^{u}\\) (the unstable 1d manifold) up to order 3 (included) following the steps just described using: A) Graph style B) Normal form style ![[2.2 Expansion of parametrizations of invariant manifolds (for flows) 2022-11-22 15.13.10.excalidraw]] We will have \\( \\(DG(Z_{0}) = \\begin{pmatrix}0 & 1 \\\\ 1 & 0\\end{pmatrix}\\) \\) with eigenvalues \\(\\lambda_{1}= 1\\) and \\(\\lambda_{2}= -1\\) and \\( \\(P=\\begin{pmatrix}1 & 1 \\\\ 1 & -1\\end{pmatrix}\\) \\) Remember to do the final step!! \\(W^{or var}(s) = Z_{0}+ PW(s)\\) Part 2 : Consider the window \\([0, 2\\pi] \\times [-2.5, 2.5]\\) and plot the phase portrait and on top the \\(W^{s}\\) ![[2.2 Expansion of parametrizations of invariant manifolds (for flows) 2022-11-22 15.20.51.excalidraw]]","title":"Example 1 (by hand)"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/2.2%20Expansion%20of%20parametrizations%20of%20invariant%20manifolds%20%28for%20flows%29/#example-2-the-lorenz-manifold","text":"Goal : To ocmpute an expansion for the parametrization of the stable manifold of the origin using the normal form style \\( \\(\\begin{cases} x_{1}' &= \\sigma(x_{2}-x_{1}) \\\\ x_{2}' &= \\rho x_{1}-x_{2}-x_{1}x_3 \\\\ x_{3}' &= x_{1}x_{2}-\\beta x_{3} \\\\ \\end{cases}\\) \\) with \\(\\sigma=10, \\rho=28, \\beta= \\frac{8}{3}\\) . Eq points : \\( \\(\\begin{align*} P_{0}=(0, 0, 0)\\\\ P_{1,2} = (\\pm \\sqrt{\\beta(\\rho-1)}, \\pm \\sqrt{\\beta(\\rho-1)}, \\rho-1)\\\\ \\end{align*}\\) \\) The eigenvalues are \\(\\lambda_{1}=-22.828\\dots\\) , \\(\\lambda_{2}=-2.666\\dots\\) and \\(\\lambda_{3}=11.828\\dots\\) . This means we have a 1d ustable manifold \\(\\mathfrak{W}^{u}\\) and a 2d stable manifold \\(\\mathfrak{W}^{s}\\) , called the Lorenz Manifold . Order of the expansion? In order to know at a given degree \\(k\\) how good is the expansion, you can check with the invariance condition. Fix a \\(k\\) : \\(\\mathfrak{W}^{s}\\) is a 2d manifold. Consider \\(s_{\\delta,\\theta} = (\\delta \\cos{\\theta}, \\delta \\sin{\\theta})\\) with \\(\\delta\\) fixed and \\(\\theta \\in [0, 2\\pi)\\) (a circle of initial conditions). Since we are using the normal form style, \\( \\(\\begin{cases} s_{1}' &= \\lambda_{1}s_{1}\\qquad \\Rightarrow \\qquad s_{1}(t) = s_{10}e^{\\lambda_{1}t} \\\\ s_{2}' &= \\lambda_{2}s_{2}\\qquad \\Rightarrow \\qquad s_{2}(t) = s_{20}e^{\\lambda_{2}t} \\\\ \\end{cases}\\) \\) and \\(\\phi_{t}(s_{10}e^{\\lambda_{1}t}, s_{20}e^{\\lambda_{2}t})\\) Take an \\(s=s_{0}=s_{\\delta, \\theta}\\) for \\(\\theta\\) given. Compute \\(\\phi_t(s)\\) for some \\(t\\) . \\(W(s) = W(s_{0})\\) and compute \\(\\Phi_{t}(W(s))\\) for some \\(t\\) We call \\(e_{0}(t, s_{\\delta, \\theta}) = ||W(s(t)) - Z(t)||\\) You compute the error for several \\(\\theta\\) and you compute \\(\\epsilon(\\delta) = \\max_{\\theta \\in [0, 2\\pi]}{e_{0}(t, s_{\\delta, \\theta})}\\) For a given \\(k\\) , the smaller \\(\\delta\\) is, the smaller the error is. Error depends on \\(t\\) . Here they take \\(t=0.3\\) . The idea is that from \\(s_{0}\\) you go to approx \\(1000s_0\\)","title":"Example 2: The Lorenz manifold"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/20220928%20NMDS%20Assignment%201/","text":"Assignment 1 \u00b6 \\( \\(F: \\begin{pmatrix}x \\\\ y\\end{pmatrix} \\longrightarrow \\begin{pmatrix}x + a\\sin(x+y) \\\\ x+y\\end{pmatrix}\\) \\) with \\(a = 0.7\\) . In principle, with the plot of Sol_assig1, we can find an approximation of the 3 periodic point if we zoom in the 3 islands area. Same if we zoom in in the 4 islands. Steps \u00b6 Find a good approximation of a 3-periodic point -> it's not just zooming in. Merc\u00e8 wants an explanation like we were explaining it to a friend. Comment on step 3: We have $$F^{3}\\begin{pmatrix}x \\ y\\end{pmatrix} = \\begin{pmatrix}x \\ y\\end{pmatrix} \\Leftrightarrow G(\\vec x) = H(\\vec x) - \\vec x = 0 $$where \\(F^{3}= H\\) . Newton Start with an approximation from the plot (zooming in), then solve a code that solves \\(D G(\\vec x_{n})\\Delta \\vec x_{n} = -G(\\vec x)\\) , so that we have a new approximation and we check again. \\(\\vec x_{n+1}= \\vec x_{n}+ \\Delta \\vec x_{n}\\) , where \\(D G(\\vec x_{n})\\Delta \\vec x_{n} = -G(\\vec x)\\) \\(\\vec x_{n}\\) approx Goal \\(\\Delta \\vec x_{n}\\) s.t. \\(\\begin{pmatrix}0 \\\\ 0\\end{pmatrix} = G \\begin{pmatrix}x_{n}+\\Delta x_{n} \\\\ y_{n} + \\Delta y_{n}\\end{pmatrix} = G \\begin{pmatrix}x_{n} \\\\ y_{n}\\end{pmatrix} + DG \\begin{pmatrix}x_{n} \\\\ y_{n}\\end{pmatrix}\\begin{pmatrix}\\Delta x_{n} \\\\ \\Delta y_{n}\\end{pmatrix}\\) How to compute \\(DG\\) ? Either the explicit expression of \\(F^3\\) and take the derivative or appy the chain rule \\(DF^{3}(\\vec x) = DF(F^{2}(\\vec x)) DF(F(\\vec x)) DF(\\vec x)\\) you only need a routine to compute \\(DF\\) and one to compute \\(F\\) The code needs to be part of the pdf. By next Wednesday \\[DF\\begin{pmatrix}x \\\\ y\\end{pmatrix} = \\begin{pmatrix}1 + a\\cos(x+y) & a\\cos(x+y) \\\\ 1 & 1\\end{pmatrix}\\]","title":"Assignment 1"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/20220928%20NMDS%20Assignment%201/#assignment-1","text":"\\( \\(F: \\begin{pmatrix}x \\\\ y\\end{pmatrix} \\longrightarrow \\begin{pmatrix}x + a\\sin(x+y) \\\\ x+y\\end{pmatrix}\\) \\) with \\(a = 0.7\\) . In principle, with the plot of Sol_assig1, we can find an approximation of the 3 periodic point if we zoom in the 3 islands area. Same if we zoom in in the 4 islands.","title":"Assignment 1"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/20220928%20NMDS%20Assignment%201/#steps","text":"Find a good approximation of a 3-periodic point -> it's not just zooming in. Merc\u00e8 wants an explanation like we were explaining it to a friend. Comment on step 3: We have $$F^{3}\\begin{pmatrix}x \\ y\\end{pmatrix} = \\begin{pmatrix}x \\ y\\end{pmatrix} \\Leftrightarrow G(\\vec x) = H(\\vec x) - \\vec x = 0 $$where \\(F^{3}= H\\) . Newton Start with an approximation from the plot (zooming in), then solve a code that solves \\(D G(\\vec x_{n})\\Delta \\vec x_{n} = -G(\\vec x)\\) , so that we have a new approximation and we check again. \\(\\vec x_{n+1}= \\vec x_{n}+ \\Delta \\vec x_{n}\\) , where \\(D G(\\vec x_{n})\\Delta \\vec x_{n} = -G(\\vec x)\\) \\(\\vec x_{n}\\) approx Goal \\(\\Delta \\vec x_{n}\\) s.t. \\(\\begin{pmatrix}0 \\\\ 0\\end{pmatrix} = G \\begin{pmatrix}x_{n}+\\Delta x_{n} \\\\ y_{n} + \\Delta y_{n}\\end{pmatrix} = G \\begin{pmatrix}x_{n} \\\\ y_{n}\\end{pmatrix} + DG \\begin{pmatrix}x_{n} \\\\ y_{n}\\end{pmatrix}\\begin{pmatrix}\\Delta x_{n} \\\\ \\Delta y_{n}\\end{pmatrix}\\) How to compute \\(DG\\) ? Either the explicit expression of \\(F^3\\) and take the derivative or appy the chain rule \\(DF^{3}(\\vec x) = DF(F^{2}(\\vec x)) DF(F(\\vec x)) DF(\\vec x)\\) you only need a routine to compute \\(DF\\) and one to compute \\(F\\) The code needs to be part of the pdf. By next Wednesday \\[DF\\begin{pmatrix}x \\\\ y\\end{pmatrix} = \\begin{pmatrix}1 + a\\cos(x+y) & a\\cos(x+y) \\\\ 1 & 1\\end{pmatrix}\\]","title":"Steps"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/20221005%20NMDS%20Assignment%202/","text":"Assignment 2 \u00b6 Idea : Check that our software to integrate ODEs work Harmonic oscillator \u00b6 System : A harmonic oscillator \\( \\(\\begin{cases}\\frac{dx}{dt}= \\; y \\\\ \\frac{dy}{dt}= -x\\end{cases}\\) \\) Exercise : The first integral is \\( \\(H(x, y) = \\frac{1}{2}(x^{2}+y^{2})\\) \\) We know that the orbits are circles around the center, since \\(x^{2}+y*2 = cnt > 0\\) . To find the tangent vector for a point (the sense of the orbits), we simply check a point in the \\(x\\) -axis: \\( \\(P = \\begin{pmatrix}x>0 \\\\ 0 \\end{pmatrix}, \\quad \\begin{pmatrix}x' \\\\ y' \\end{pmatrix}_{P}= \\begin{pmatrix} 0 \\\\ -x \\end{pmatrix}.\\) \\) First part \u00b6 In condition \\(\\vec x = (1, 0)\\) , \\(t_{0}= 0\\) , \\(t_{max}=2\\pi\\) (16 digits of precision) -> check the final piont Take \\(np = 50\\) intermediate points and check them. All this forward in time (idir=1) Repeat with idir=-1 (backward in time): From \\(t_{0} = 0\\) to \\(t_{max}= -2\\pi\\) . Play with initial conditions and plot the output Add in the code \"Check of the first integral\": Take an initial condition \\((x_{0}, y_{0})\\) , compute \\(H(x_{0}, y_{0})\\) Along each point of the orbit, we should check that \\(H(x(t), y(t)) = H(x_{0}, y_{0})\\) Second part \u00b6 Integrate the harmonic oscillator + variational equations (6 ODE) Check : Take initial condition. Integrate one period ( \\(x_{1}, x_{2}, x_{3}, x_{4}, x_{5}, x_{6}\\) ): the first two ocmponents were already checked in the previous part. To check the others \\[ \\begin{pmatrix}x_{3} & x_{4} \\\\ x_{5} & x_{6} \\end{pmatrix}= -1\\]","title":"Assignment 2"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/20221005%20NMDS%20Assignment%202/#assignment-2","text":"Idea : Check that our software to integrate ODEs work","title":"Assignment 2"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/20221005%20NMDS%20Assignment%202/#harmonic-oscillator","text":"System : A harmonic oscillator \\( \\(\\begin{cases}\\frac{dx}{dt}= \\; y \\\\ \\frac{dy}{dt}= -x\\end{cases}\\) \\) Exercise : The first integral is \\( \\(H(x, y) = \\frac{1}{2}(x^{2}+y^{2})\\) \\) We know that the orbits are circles around the center, since \\(x^{2}+y*2 = cnt > 0\\) . To find the tangent vector for a point (the sense of the orbits), we simply check a point in the \\(x\\) -axis: \\( \\(P = \\begin{pmatrix}x>0 \\\\ 0 \\end{pmatrix}, \\quad \\begin{pmatrix}x' \\\\ y' \\end{pmatrix}_{P}= \\begin{pmatrix} 0 \\\\ -x \\end{pmatrix}.\\) \\)","title":"Harmonic oscillator"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/20221005%20NMDS%20Assignment%202/#first-part","text":"In condition \\(\\vec x = (1, 0)\\) , \\(t_{0}= 0\\) , \\(t_{max}=2\\pi\\) (16 digits of precision) -> check the final piont Take \\(np = 50\\) intermediate points and check them. All this forward in time (idir=1) Repeat with idir=-1 (backward in time): From \\(t_{0} = 0\\) to \\(t_{max}= -2\\pi\\) . Play with initial conditions and plot the output Add in the code \"Check of the first integral\": Take an initial condition \\((x_{0}, y_{0})\\) , compute \\(H(x_{0}, y_{0})\\) Along each point of the orbit, we should check that \\(H(x(t), y(t)) = H(x_{0}, y_{0})\\)","title":"First part"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/20221005%20NMDS%20Assignment%202/#second-part","text":"Integrate the harmonic oscillator + variational equations (6 ODE) Check : Take initial condition. Integrate one period ( \\(x_{1}, x_{2}, x_{3}, x_{4}, x_{5}, x_{6}\\) ): the first two ocmponents were already checked in the previous part. To check the others \\[ \\begin{pmatrix}x_{3} & x_{4} \\\\ x_{5} & x_{6} \\end{pmatrix}= -1\\]","title":"Second part"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/20221011%20NMDS%20Assignment%203/","text":"Assignment 3 \u00b6 Part B \u00b6 We consider a linear system of dimension 2: \\( \\(\\begin{pmatrix}x' \\\\ y'\\end{pmatrix} = \\begin{pmatrix}a & b \\\\ c & d\\end{pmatrix}\\begin{pmatrix}x \\\\ y\\end{pmatrix}\\) \\) Deadline : Sunday 23rd Oct","title":"Assignment 3"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/20221011%20NMDS%20Assignment%203/#assignment-3","text":"","title":"Assignment 3"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/20221011%20NMDS%20Assignment%203/#part-b","text":"We consider a linear system of dimension 2: \\( \\(\\begin{pmatrix}x' \\\\ y'\\end{pmatrix} = \\begin{pmatrix}a & b \\\\ c & d\\end{pmatrix}\\begin{pmatrix}x \\\\ y\\end{pmatrix}\\) \\) Deadline : Sunday 23rd Oct","title":"Part B"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/Algebraic%20group/","text":"Algebraic group \u00b6 Conditions ...","title":"Algebraic group"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/Algebraic%20group/#algebraic-group","text":"Conditions ...","title":"Algebraic group"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/Assignment%204/","text":"For each vlaue of \\(\\mu\\) we do a plot. We will have 3 curves. \\(C(L_{i}) = 2\\Omega(x,y) - (x'^{2}+y'^{2})\\) (first integral) Again, for each value of \\(\\mu\\) we will have \\(C(L_{1}), C(L_{2}), C(L_{3})\\) . We plot them again and we will have three curves. Part 4 to be ignored for now.","title":"Assignment 4"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/First%20integral/","text":"First integral \u00b6 A first integral of the system is a (non-constant) continuously-differentiable function \u03a8:R\u00d7Rn\u2192R\u03a8:R\u00d7Rn\u2192R which is locally constant on any solution of (1) (1), namely such that ddt\u03a8(t,x(t))=0ddt\u03a8(t,x(t))=0 for any x:J\u2192Rnx:J\u2192Rn solving (1) (1) That is \\(H(\\theta(t), \\omega(t)) = constant\\) . Check relationship with Noether theorem https://researchspace.ukzn.ac.za/xmlui/bitstream/handle/10413/5061/Moyo_Sibusiso_1997.pdf?sequence=1&isAllowed=y","title":"First integral"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/First%20integral/#first-integral","text":"A first integral of the system is a (non-constant) continuously-differentiable function \u03a8:R\u00d7Rn\u2192R\u03a8:R\u00d7Rn\u2192R which is locally constant on any solution of (1) (1), namely such that ddt\u03a8(t,x(t))=0ddt\u03a8(t,x(t))=0 for any x:J\u2192Rnx:J\u2192Rn solving (1) (1) That is \\(H(\\theta(t), \\omega(t)) = constant\\) . Check relationship with Noether theorem https://researchspace.ukzn.ac.za/xmlui/bitstream/handle/10413/5061/Moyo_Sibusiso_1997.pdf?sequence=1&isAllowed=y","title":"First integral"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/Jordan%20matrix/","text":"Jordan matrix \u00b6","title":"Jordan matrix"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/Jordan%20matrix/#jordan-matrix","text":"","title":"Jordan matrix"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/Numerical%20Methods%20for%20Dynamical%20Systems/","text":"Numerical Methods for Dynamical Systems \u00b6 Index \u00b6 Chapter 1: Preliminaries 2. Equilibrium points and stability","title":"Numerical Methods for Dynamical Systems"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/Numerical%20Methods%20for%20Dynamical%20Systems/#numerical-methods-for-dynamical-systems","text":"","title":"Numerical Methods for Dynamical Systems"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/Numerical%20Methods%20for%20Dynamical%20Systems/#index","text":"Chapter 1: Preliminaries 2. Equilibrium points and stability","title":"Index"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/Outline%20-%20Numerical%20Methods%20for%20Dynamical%20Systems/","text":"Introduction to the class Outline \u00b6 Preliminaries Continuous dynamical systems Discrete dynamical systems Variational equations Poincar\u00e9 map Equilibrium points and stability The 3 body problem Expansion of parametrizations of invariant manifolds","title":"Outline   Numerical Methods for Dynamical Systems"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/Outline%20-%20Numerical%20Methods%20for%20Dynamical%20Systems/#outline","text":"Preliminaries Continuous dynamical systems Discrete dynamical systems Variational equations Poincar\u00e9 map Equilibrium points and stability The 3 body problem Expansion of parametrizations of invariant manifolds","title":"Outline"},{"location":"Numerical%20Methods%20for%20Dynamical%20Systems/Separatrix/","text":"","title":"Separatrix"}]}